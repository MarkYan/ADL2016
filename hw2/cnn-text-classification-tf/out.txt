
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NEGATIVE_DATA_FILE=../data/dataset_cnn_eng/training_data.neg
NUM_EPOCHS=200
NUM_FILTERS=128
POSITIVE_DATA_FILE=../data/dataset_cnn_eng/training_data.pos

Loading data...
Vocabulary Size: 8813
Train/Dev split: 4500/500
Writing to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814

2016-11-12T19:06:55.121666: step 1, loss 1.82122, acc 0.59375
2016-11-12T19:06:55.178424: step 2, loss 2.19589, acc 0.5625
2016-11-12T19:06:55.234305: step 3, loss 2.52719, acc 0.453125
2016-11-12T19:06:55.291941: step 4, loss 1.63965, acc 0.5625
2016-11-12T19:06:55.349603: step 5, loss 2.17541, acc 0.5
2016-11-12T19:06:55.407213: step 6, loss 2.79868, acc 0.515625
2016-11-12T19:06:55.462698: step 7, loss 2.7659, acc 0.515625
2016-11-12T19:06:55.520200: step 8, loss 2.37526, acc 0.53125
2016-11-12T19:06:55.576932: step 9, loss 2.79547, acc 0.453125
2016-11-12T19:06:55.633536: step 10, loss 1.80139, acc 0.46875
2016-11-12T19:06:55.692053: step 11, loss 1.66722, acc 0.5
2016-11-12T19:06:55.749571: step 12, loss 1.57555, acc 0.59375
2016-11-12T19:06:55.808170: step 13, loss 1.36605, acc 0.578125
2016-11-12T19:06:55.865841: step 14, loss 2.11588, acc 0.484375
2016-11-12T19:06:55.923044: step 15, loss 1.83185, acc 0.46875
2016-11-12T19:06:55.979717: step 16, loss 2.10381, acc 0.484375
2016-11-12T19:06:56.035886: step 17, loss 1.94742, acc 0.484375
2016-11-12T19:06:56.092363: step 18, loss 1.80531, acc 0.5625
2016-11-12T19:06:56.150331: step 19, loss 1.56273, acc 0.5625
2016-11-12T19:06:56.206899: step 20, loss 1.72182, acc 0.5
2016-11-12T19:06:56.263589: step 21, loss 1.77039, acc 0.5
2016-11-12T19:06:56.320659: step 22, loss 1.77301, acc 0.53125
2016-11-12T19:06:56.376747: step 23, loss 2.04055, acc 0.453125
2016-11-12T19:06:56.432645: step 24, loss 1.5904, acc 0.515625
2016-11-12T19:06:56.489372: step 25, loss 1.92929, acc 0.4375
2016-11-12T19:06:56.545516: step 26, loss 2.02285, acc 0.5
2016-11-12T19:06:56.605448: step 27, loss 1.62361, acc 0.53125
2016-11-12T19:06:56.662047: step 28, loss 1.63989, acc 0.546875
2016-11-12T19:06:56.719230: step 29, loss 2.53404, acc 0.34375
2016-11-12T19:06:56.776402: step 30, loss 1.55476, acc 0.59375
2016-11-12T19:06:56.833469: step 31, loss 1.90545, acc 0.5
2016-11-12T19:06:56.891952: step 32, loss 1.78854, acc 0.5
2016-11-12T19:06:56.947834: step 33, loss 1.37108, acc 0.515625
2016-11-12T19:06:57.004727: step 34, loss 1.94601, acc 0.4375
2016-11-12T19:06:57.061187: step 35, loss 1.48263, acc 0.484375
2016-11-12T19:06:57.119315: step 36, loss 1.7402, acc 0.53125
2016-11-12T19:06:57.177830: step 37, loss 2.20131, acc 0.453125
2016-11-12T19:06:57.235284: step 38, loss 1.54485, acc 0.53125
2016-11-12T19:06:57.292434: step 39, loss 1.56627, acc 0.5
2016-11-12T19:06:57.349283: step 40, loss 1.56366, acc 0.5
2016-11-12T19:06:57.407746: step 41, loss 1.65375, acc 0.546875
2016-11-12T19:06:57.464271: step 42, loss 1.41531, acc 0.5625
2016-11-12T19:06:57.523989: step 43, loss 1.93045, acc 0.421875
2016-11-12T19:06:57.580131: step 44, loss 1.6152, acc 0.46875
2016-11-12T19:06:57.637149: step 45, loss 1.73807, acc 0.484375
2016-11-12T19:06:57.693734: step 46, loss 1.343, acc 0.515625
2016-11-12T19:06:57.749332: step 47, loss 1.68016, acc 0.53125
2016-11-12T19:06:57.804872: step 48, loss 1.12536, acc 0.59375
2016-11-12T19:06:57.861070: step 49, loss 1.31962, acc 0.5625
2016-11-12T19:06:57.917384: step 50, loss 1.80383, acc 0.5
2016-11-12T19:06:57.973099: step 51, loss 1.36213, acc 0.5
2016-11-12T19:06:58.029451: step 52, loss 1.89932, acc 0.53125
2016-11-12T19:06:58.086036: step 53, loss 1.41095, acc 0.59375
2016-11-12T19:06:58.143030: step 54, loss 1.44729, acc 0.578125
2016-11-12T19:06:58.199281: step 55, loss 2.18709, acc 0.421875
2016-11-12T19:06:58.258192: step 56, loss 1.06522, acc 0.6875
2016-11-12T19:06:58.317001: step 57, loss 1.18158, acc 0.609375
2016-11-12T19:06:58.373398: step 58, loss 1.56821, acc 0.5
2016-11-12T19:06:58.432363: step 59, loss 1.91482, acc 0.515625
2016-11-12T19:06:58.488863: step 60, loss 1.6876, acc 0.46875
2016-11-12T19:06:58.547424: step 61, loss 1.24025, acc 0.609375
2016-11-12T19:06:58.604186: step 62, loss 1.58713, acc 0.515625
2016-11-12T19:06:58.661252: step 63, loss 1.87053, acc 0.375
2016-11-12T19:06:58.717169: step 64, loss 1.70415, acc 0.484375
2016-11-12T19:06:58.775000: step 65, loss 1.79857, acc 0.4375
2016-11-12T19:06:58.831132: step 66, loss 1.56765, acc 0.5
2016-11-12T19:06:58.887634: step 67, loss 1.39081, acc 0.546875
2016-11-12T19:06:58.942852: step 68, loss 1.27687, acc 0.578125
2016-11-12T19:06:59.000663: step 69, loss 1.38237, acc 0.5
2016-11-12T19:06:59.058241: step 70, loss 1.42775, acc 0.5
2016-11-12T19:06:59.098057: step 71, loss 1.65012, acc 0.55
2016-11-12T19:06:59.155809: step 72, loss 1.32201, acc 0.5
2016-11-12T19:06:59.211761: step 73, loss 1.40373, acc 0.46875
2016-11-12T19:06:59.268439: step 74, loss 1.0342, acc 0.640625
2016-11-12T19:06:59.326863: step 75, loss 0.916682, acc 0.625
2016-11-12T19:06:59.382320: step 76, loss 1.23218, acc 0.53125
2016-11-12T19:06:59.438821: step 77, loss 1.27166, acc 0.5625
2016-11-12T19:06:59.495402: step 78, loss 1.22491, acc 0.5625
2016-11-12T19:06:59.552794: step 79, loss 1.50808, acc 0.578125
2016-11-12T19:06:59.612505: step 80, loss 1.13252, acc 0.5625
2016-11-12T19:06:59.668361: step 81, loss 1.24621, acc 0.609375
2016-11-12T19:06:59.725950: step 82, loss 1.21085, acc 0.53125
2016-11-12T19:06:59.783213: step 83, loss 1.31929, acc 0.484375
2016-11-12T19:06:59.840395: step 84, loss 1.43084, acc 0.5625
2016-11-12T19:06:59.897536: step 85, loss 1.23813, acc 0.546875
2016-11-12T19:06:59.954326: step 86, loss 1.11827, acc 0.625
2016-11-12T19:07:00.014052: step 87, loss 1.42397, acc 0.484375
2016-11-12T19:07:00.071496: step 88, loss 1.21409, acc 0.546875
2016-11-12T19:07:00.128411: step 89, loss 1.10422, acc 0.59375
2016-11-12T19:07:00.186214: step 90, loss 1.3764, acc 0.515625
2016-11-12T19:07:00.244293: step 91, loss 1.4921, acc 0.53125
2016-11-12T19:07:00.300735: step 92, loss 1.23189, acc 0.484375
2016-11-12T19:07:00.360176: step 93, loss 1.08749, acc 0.578125
2016-11-12T19:07:00.417632: step 94, loss 0.9834, acc 0.65625
2016-11-12T19:07:00.473807: step 95, loss 1.24408, acc 0.578125
2016-11-12T19:07:00.532951: step 96, loss 1.22629, acc 0.640625
2016-11-12T19:07:00.589478: step 97, loss 1.14891, acc 0.609375
2016-11-12T19:07:00.646644: step 98, loss 1.23582, acc 0.5625
2016-11-12T19:07:00.704256: step 99, loss 1.11161, acc 0.578125
2016-11-12T19:07:00.762089: step 100, loss 1.37884, acc 0.578125

Evaluation:
2016-11-12T19:07:00.839328: step 100, loss 0.816751, acc 0.526

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-100

2016-11-12T19:07:01.346603: step 101, loss 0.948144, acc 0.640625
2016-11-12T19:07:01.404792: step 102, loss 0.991361, acc 0.546875
2016-11-12T19:07:01.460981: step 103, loss 1.34404, acc 0.5
2016-11-12T19:07:01.518434: step 104, loss 1.01672, acc 0.640625
2016-11-12T19:07:01.573576: step 105, loss 1.33192, acc 0.578125
2016-11-12T19:07:01.629773: step 106, loss 1.29571, acc 0.53125
2016-11-12T19:07:01.684522: step 107, loss 1.26794, acc 0.625
2016-11-12T19:07:01.740844: step 108, loss 1.02475, acc 0.703125
2016-11-12T19:07:01.796206: step 109, loss 1.00224, acc 0.640625
2016-11-12T19:07:01.852815: step 110, loss 1.24944, acc 0.5625
2016-11-12T19:07:01.909464: step 111, loss 1.17991, acc 0.546875
2016-11-12T19:07:01.965129: step 112, loss 1.11475, acc 0.640625
2016-11-12T19:07:02.023343: step 113, loss 1.04339, acc 0.59375
2016-11-12T19:07:02.081613: step 114, loss 1.13349, acc 0.59375
2016-11-12T19:07:02.137128: step 115, loss 0.988034, acc 0.65625
2016-11-12T19:07:02.196898: step 116, loss 1.04396, acc 0.640625
2016-11-12T19:07:02.258644: step 117, loss 1.20359, acc 0.5625
2016-11-12T19:07:02.314713: step 118, loss 1.06752, acc 0.578125
2016-11-12T19:07:02.371444: step 119, loss 0.706967, acc 0.703125
2016-11-12T19:07:02.427742: step 120, loss 1.01704, acc 0.5625
2016-11-12T19:07:02.484054: step 121, loss 1.18383, acc 0.609375
2016-11-12T19:07:02.540597: step 122, loss 1.81319, acc 0.4375
2016-11-12T19:07:02.596422: step 123, loss 1.22711, acc 0.578125
2016-11-12T19:07:02.653583: step 124, loss 1.18071, acc 0.546875
2016-11-12T19:07:02.709177: step 125, loss 1.111, acc 0.59375
2016-11-12T19:07:02.765604: step 126, loss 1.25453, acc 0.515625
2016-11-12T19:07:02.823097: step 127, loss 1.00541, acc 0.609375
2016-11-12T19:07:02.880884: step 128, loss 1.26541, acc 0.484375
2016-11-12T19:07:02.938005: step 129, loss 0.841166, acc 0.65625
2016-11-12T19:07:02.996831: step 130, loss 0.729457, acc 0.65625
2016-11-12T19:07:03.053607: step 131, loss 1.47937, acc 0.53125
2016-11-12T19:07:03.110961: step 132, loss 1.56028, acc 0.546875
2016-11-12T19:07:03.172150: step 133, loss 1.52861, acc 0.484375
2016-11-12T19:07:03.229262: step 134, loss 1.34865, acc 0.484375
2016-11-12T19:07:03.284882: step 135, loss 1.07621, acc 0.515625
2016-11-12T19:07:03.340521: step 136, loss 1.0737, acc 0.59375
2016-11-12T19:07:03.396610: step 137, loss 1.04166, acc 0.625
2016-11-12T19:07:03.452066: step 138, loss 1.16262, acc 0.578125
2016-11-12T19:07:03.509468: step 139, loss 1.32608, acc 0.546875
2016-11-12T19:07:03.565719: step 140, loss 1.02325, acc 0.59375
2016-11-12T19:07:03.621651: step 141, loss 1.26781, acc 0.453125
2016-11-12T19:07:03.658804: step 142, loss 1.90177, acc 0.45
2016-11-12T19:07:03.716449: step 143, loss 1.15612, acc 0.5625
2016-11-12T19:07:03.774004: step 144, loss 1.14239, acc 0.59375
2016-11-12T19:07:03.831528: step 145, loss 0.884959, acc 0.609375
2016-11-12T19:07:03.889007: step 146, loss 0.984279, acc 0.609375
2016-11-12T19:07:03.946096: step 147, loss 0.806498, acc 0.65625
2016-11-12T19:07:04.002158: step 148, loss 0.841947, acc 0.765625
2016-11-12T19:07:04.058348: step 149, loss 1.01268, acc 0.53125
2016-11-12T19:07:04.117276: step 150, loss 0.683979, acc 0.734375
2016-11-12T19:07:04.176945: step 151, loss 0.971333, acc 0.625
2016-11-12T19:07:04.233379: step 152, loss 0.909975, acc 0.609375
2016-11-12T19:07:04.290000: step 153, loss 0.971441, acc 0.6875
2016-11-12T19:07:04.345213: step 154, loss 0.672515, acc 0.765625
2016-11-12T19:07:04.401858: step 155, loss 0.859472, acc 0.71875
2016-11-12T19:07:04.461011: step 156, loss 0.756546, acc 0.765625
2016-11-12T19:07:04.517157: step 157, loss 0.962527, acc 0.609375
2016-11-12T19:07:04.573753: step 158, loss 1.00428, acc 0.609375
2016-11-12T19:07:04.629357: step 159, loss 0.71577, acc 0.65625
2016-11-12T19:07:04.685361: step 160, loss 1.2548, acc 0.53125
2016-11-12T19:07:04.744415: step 161, loss 0.877736, acc 0.65625
2016-11-12T19:07:04.800448: step 162, loss 1.0084, acc 0.625
2016-11-12T19:07:04.858086: step 163, loss 0.876398, acc 0.59375
2016-11-12T19:07:04.913936: step 164, loss 1.07827, acc 0.59375
2016-11-12T19:07:04.970496: step 165, loss 0.804464, acc 0.609375
2016-11-12T19:07:05.028805: step 166, loss 0.990916, acc 0.578125
2016-11-12T19:07:05.084606: step 167, loss 0.76171, acc 0.671875
2016-11-12T19:07:05.141411: step 168, loss 0.771346, acc 0.65625
2016-11-12T19:07:05.197643: step 169, loss 0.785271, acc 0.703125
2016-11-12T19:07:05.253203: step 170, loss 0.920701, acc 0.625
2016-11-12T19:07:05.309065: step 171, loss 0.766889, acc 0.671875
2016-11-12T19:07:05.365258: step 172, loss 0.797571, acc 0.703125
2016-11-12T19:07:05.421305: step 173, loss 0.639286, acc 0.65625
2016-11-12T19:07:05.478425: step 174, loss 0.895288, acc 0.640625
2016-11-12T19:07:05.537249: step 175, loss 1.13687, acc 0.484375
2016-11-12T19:07:05.593784: step 176, loss 0.926145, acc 0.625
2016-11-12T19:07:05.649674: step 177, loss 1.08157, acc 0.59375
2016-11-12T19:07:05.706319: step 178, loss 0.963642, acc 0.59375
2016-11-12T19:07:05.762337: step 179, loss 1.01215, acc 0.625
2016-11-12T19:07:05.820143: step 180, loss 0.671508, acc 0.65625
2016-11-12T19:07:05.877865: step 181, loss 0.839891, acc 0.625
2016-11-12T19:07:05.934808: step 182, loss 0.800666, acc 0.65625
2016-11-12T19:07:05.993066: step 183, loss 0.581499, acc 0.703125
2016-11-12T19:07:06.048980: step 184, loss 0.713072, acc 0.671875
2016-11-12T19:07:06.105034: step 185, loss 0.764063, acc 0.6875
2016-11-12T19:07:06.161011: step 186, loss 1.15469, acc 0.546875
2016-11-12T19:07:06.217078: step 187, loss 0.649204, acc 0.71875
2016-11-12T19:07:06.274973: step 188, loss 1.16306, acc 0.53125
2016-11-12T19:07:06.331953: step 189, loss 0.950972, acc 0.625
2016-11-12T19:07:06.387775: step 190, loss 0.836916, acc 0.671875
2016-11-12T19:07:06.445273: step 191, loss 0.758169, acc 0.703125
2016-11-12T19:07:06.501137: step 192, loss 1.00759, acc 0.609375
2016-11-12T19:07:06.559339: step 193, loss 1.12092, acc 0.6875
2016-11-12T19:07:06.619410: step 194, loss 0.702676, acc 0.65625
2016-11-12T19:07:06.677086: step 195, loss 0.706213, acc 0.703125
2016-11-12T19:07:06.734751: step 196, loss 0.822687, acc 0.609375
2016-11-12T19:07:06.793423: step 197, loss 0.78947, acc 0.671875
2016-11-12T19:07:06.850796: step 198, loss 1.4077, acc 0.53125
2016-11-12T19:07:06.908710: step 199, loss 0.76706, acc 0.734375
2016-11-12T19:07:06.966087: step 200, loss 1.00339, acc 0.53125

Evaluation:
2016-11-12T19:07:07.038336: step 200, loss 0.758734, acc 0.564

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-200

2016-11-12T19:07:07.539091: step 201, loss 0.766827, acc 0.65625
2016-11-12T19:07:07.596710: step 202, loss 0.712648, acc 0.65625
2016-11-12T19:07:07.653547: step 203, loss 0.459106, acc 0.796875
2016-11-12T19:07:07.713211: step 204, loss 1.06072, acc 0.5625
2016-11-12T19:07:07.769553: step 205, loss 0.618102, acc 0.734375
2016-11-12T19:07:07.825964: step 206, loss 0.510765, acc 0.703125
2016-11-12T19:07:07.885126: step 207, loss 0.63767, acc 0.640625
2016-11-12T19:07:07.942059: step 208, loss 0.802143, acc 0.65625
2016-11-12T19:07:07.997989: step 209, loss 1.04873, acc 0.625
2016-11-12T19:07:08.053722: step 210, loss 0.727273, acc 0.640625
2016-11-12T19:07:08.110022: step 211, loss 0.948907, acc 0.640625
2016-11-12T19:07:08.167474: step 212, loss 0.713398, acc 0.6875
2016-11-12T19:07:08.204765: step 213, loss 0.410834, acc 0.8
2016-11-12T19:07:08.261248: step 214, loss 0.727947, acc 0.71875
2016-11-12T19:07:08.317373: step 215, loss 0.646668, acc 0.6875
2016-11-12T19:07:08.375223: step 216, loss 0.784187, acc 0.734375
2016-11-12T19:07:08.432571: step 217, loss 0.685125, acc 0.6875
2016-11-12T19:07:08.489673: step 218, loss 0.586842, acc 0.765625
2016-11-12T19:07:08.548502: step 219, loss 0.652349, acc 0.6875
2016-11-12T19:07:08.604369: step 220, loss 0.813796, acc 0.734375
2016-11-12T19:07:08.660361: step 221, loss 0.814685, acc 0.65625
2016-11-12T19:07:08.719263: step 222, loss 0.724568, acc 0.671875
2016-11-12T19:07:08.775273: step 223, loss 0.618763, acc 0.71875
2016-11-12T19:07:08.833370: step 224, loss 0.895712, acc 0.59375
2016-11-12T19:07:08.892459: step 225, loss 0.764, acc 0.671875
2016-11-12T19:07:08.949308: step 226, loss 0.696395, acc 0.65625
2016-11-12T19:07:09.004920: step 227, loss 0.683945, acc 0.703125
2016-11-12T19:07:09.062194: step 228, loss 0.860979, acc 0.703125
2016-11-12T19:07:09.117604: step 229, loss 0.54432, acc 0.734375
2016-11-12T19:07:09.173135: step 230, loss 0.68114, acc 0.734375
2016-11-12T19:07:09.229332: step 231, loss 0.747031, acc 0.6875
2016-11-12T19:07:09.285585: step 232, loss 0.650385, acc 0.6875
2016-11-12T19:07:09.341126: step 233, loss 0.731412, acc 0.6875
2016-11-12T19:07:09.397223: step 234, loss 0.685492, acc 0.671875
2016-11-12T19:07:09.453965: step 235, loss 0.629298, acc 0.71875
2016-11-12T19:07:09.512967: step 236, loss 0.718085, acc 0.65625
2016-11-12T19:07:09.573490: step 237, loss 0.833946, acc 0.640625
2016-11-12T19:07:09.630937: step 238, loss 0.621055, acc 0.75
2016-11-12T19:07:09.687927: step 239, loss 0.887387, acc 0.75
2016-11-12T19:07:09.744306: step 240, loss 0.656572, acc 0.671875
2016-11-12T19:07:09.799689: step 241, loss 0.522902, acc 0.78125
2016-11-12T19:07:09.855620: step 242, loss 0.758719, acc 0.65625
2016-11-12T19:07:09.912265: step 243, loss 0.678269, acc 0.6875
2016-11-12T19:07:09.969198: step 244, loss 0.615588, acc 0.703125
2016-11-12T19:07:10.028814: step 245, loss 0.620453, acc 0.640625
2016-11-12T19:07:10.084648: step 246, loss 0.779625, acc 0.609375
2016-11-12T19:07:10.140806: step 247, loss 0.644981, acc 0.6875
2016-11-12T19:07:10.197729: step 248, loss 0.643971, acc 0.6875
2016-11-12T19:07:10.255662: step 249, loss 0.57655, acc 0.75
2016-11-12T19:07:10.311624: step 250, loss 0.579725, acc 0.734375
2016-11-12T19:07:10.367280: step 251, loss 0.604275, acc 0.6875
2016-11-12T19:07:10.425306: step 252, loss 0.693807, acc 0.640625
2016-11-12T19:07:10.481057: step 253, loss 0.847375, acc 0.59375
2016-11-12T19:07:10.537388: step 254, loss 0.710213, acc 0.625
2016-11-12T19:07:10.593159: step 255, loss 0.534754, acc 0.703125
2016-11-12T19:07:10.650047: step 256, loss 0.565292, acc 0.734375
2016-11-12T19:07:10.705573: step 257, loss 0.798473, acc 0.671875
2016-11-12T19:07:10.764167: step 258, loss 0.702267, acc 0.640625
2016-11-12T19:07:10.820856: step 259, loss 0.634752, acc 0.640625
2016-11-12T19:07:10.876884: step 260, loss 0.537045, acc 0.75
2016-11-12T19:07:10.932655: step 261, loss 0.662356, acc 0.703125
2016-11-12T19:07:10.988174: step 262, loss 0.626078, acc 0.703125
2016-11-12T19:07:11.045266: step 263, loss 0.68339, acc 0.703125
2016-11-12T19:07:11.101644: step 264, loss 0.605064, acc 0.71875
2016-11-12T19:07:11.156520: step 265, loss 0.512994, acc 0.765625
2016-11-12T19:07:11.213152: step 266, loss 0.726599, acc 0.6875
2016-11-12T19:07:11.269126: step 267, loss 0.426878, acc 0.8125
2016-11-12T19:07:11.325303: step 268, loss 0.853877, acc 0.671875
2016-11-12T19:07:11.380938: step 269, loss 0.533409, acc 0.78125
2016-11-12T19:07:11.436551: step 270, loss 0.724386, acc 0.640625
2016-11-12T19:07:11.493314: step 271, loss 0.792006, acc 0.640625
2016-11-12T19:07:11.549073: step 272, loss 0.906648, acc 0.609375
2016-11-12T19:07:11.605097: step 273, loss 0.537372, acc 0.75
2016-11-12T19:07:11.661799: step 274, loss 0.833805, acc 0.671875
2016-11-12T19:07:11.721459: step 275, loss 0.826269, acc 0.65625
2016-11-12T19:07:11.778792: step 276, loss 0.775522, acc 0.6875
2016-11-12T19:07:11.836073: step 277, loss 0.750053, acc 0.671875
2016-11-12T19:07:11.892649: step 278, loss 0.803894, acc 0.53125
2016-11-12T19:07:11.948040: step 279, loss 0.575798, acc 0.703125
2016-11-12T19:07:12.004228: step 280, loss 0.66212, acc 0.640625
2016-11-12T19:07:12.060784: step 281, loss 0.86635, acc 0.640625
2016-11-12T19:07:12.118108: step 282, loss 0.677597, acc 0.6875
2016-11-12T19:07:12.174728: step 283, loss 0.654879, acc 0.671875
2016-11-12T19:07:12.212872: step 284, loss 0.465617, acc 0.7
2016-11-12T19:07:12.270167: step 285, loss 0.585012, acc 0.71875
2016-11-12T19:07:12.325901: step 286, loss 0.573879, acc 0.796875
2016-11-12T19:07:12.381267: step 287, loss 0.447875, acc 0.78125
2016-11-12T19:07:12.436186: step 288, loss 0.490682, acc 0.765625
2016-11-12T19:07:12.491818: step 289, loss 0.687208, acc 0.6875
2016-11-12T19:07:12.549467: step 290, loss 0.675439, acc 0.671875
2016-11-12T19:07:12.605292: step 291, loss 0.727082, acc 0.59375
2016-11-12T19:07:12.660313: step 292, loss 0.581775, acc 0.703125
2016-11-12T19:07:12.716554: step 293, loss 0.47591, acc 0.78125
2016-11-12T19:07:12.772553: step 294, loss 0.716505, acc 0.671875
2016-11-12T19:07:12.829611: step 295, loss 0.69545, acc 0.734375
2016-11-12T19:07:12.884623: step 296, loss 0.818201, acc 0.640625
2016-11-12T19:07:12.939751: step 297, loss 0.621447, acc 0.703125
2016-11-12T19:07:12.997368: step 298, loss 0.466961, acc 0.75
2016-11-12T19:07:13.052424: step 299, loss 0.574319, acc 0.734375
2016-11-12T19:07:13.107725: step 300, loss 0.629293, acc 0.671875

Evaluation:
2016-11-12T19:07:13.179213: step 300, loss 0.748723, acc 0.54

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-300

2016-11-12T19:07:13.680489: step 301, loss 0.449395, acc 0.71875
2016-11-12T19:07:13.739738: step 302, loss 0.766656, acc 0.625
2016-11-12T19:07:13.794755: step 303, loss 0.516575, acc 0.75
2016-11-12T19:07:13.850117: step 304, loss 0.547186, acc 0.78125
2016-11-12T19:07:13.904853: step 305, loss 0.727744, acc 0.609375
2016-11-12T19:07:13.961729: step 306, loss 0.546153, acc 0.75
2016-11-12T19:07:14.017056: step 307, loss 0.759983, acc 0.6875
2016-11-12T19:07:14.072163: step 308, loss 0.647057, acc 0.671875
2016-11-12T19:07:14.127934: step 309, loss 0.594479, acc 0.703125
2016-11-12T19:07:14.185021: step 310, loss 0.637029, acc 0.703125
2016-11-12T19:07:14.241107: step 311, loss 0.7151, acc 0.71875
2016-11-12T19:07:14.300473: step 312, loss 0.719223, acc 0.640625
2016-11-12T19:07:14.356456: step 313, loss 0.631643, acc 0.734375
2016-11-12T19:07:14.412981: step 314, loss 0.760798, acc 0.671875
2016-11-12T19:07:14.468531: step 315, loss 0.490499, acc 0.796875
2016-11-12T19:07:14.529359: step 316, loss 0.470426, acc 0.78125
2016-11-12T19:07:14.585235: step 317, loss 0.527629, acc 0.765625
2016-11-12T19:07:14.641399: step 318, loss 0.592593, acc 0.703125
2016-11-12T19:07:14.697575: step 319, loss 0.515522, acc 0.75
2016-11-12T19:07:14.753391: step 320, loss 0.582447, acc 0.71875
2016-11-12T19:07:14.810918: step 321, loss 0.44664, acc 0.765625
2016-11-12T19:07:14.868216: step 322, loss 0.75732, acc 0.65625
2016-11-12T19:07:14.925173: step 323, loss 0.603898, acc 0.734375
2016-11-12T19:07:14.981345: step 324, loss 0.51995, acc 0.78125
2016-11-12T19:07:15.037612: step 325, loss 0.709421, acc 0.65625
2016-11-12T19:07:15.093771: step 326, loss 0.529474, acc 0.796875
2016-11-12T19:07:15.152489: step 327, loss 0.63047, acc 0.6875
2016-11-12T19:07:15.208194: step 328, loss 0.740694, acc 0.609375
2016-11-12T19:07:15.263573: step 329, loss 0.731461, acc 0.671875
2016-11-12T19:07:15.318876: step 330, loss 0.541623, acc 0.765625
2016-11-12T19:07:15.375817: step 331, loss 0.602942, acc 0.671875
2016-11-12T19:07:15.431368: step 332, loss 0.743492, acc 0.6875
2016-11-12T19:07:15.489269: step 333, loss 0.723164, acc 0.671875
2016-11-12T19:07:15.544466: step 334, loss 0.751839, acc 0.640625
2016-11-12T19:07:15.600969: step 335, loss 0.615982, acc 0.71875
2016-11-12T19:07:15.656098: step 336, loss 0.746197, acc 0.71875
2016-11-12T19:07:15.711258: step 337, loss 0.690388, acc 0.625
2016-11-12T19:07:15.768127: step 338, loss 0.667407, acc 0.6875
2016-11-12T19:07:15.825014: step 339, loss 0.810685, acc 0.671875
2016-11-12T19:07:15.881370: step 340, loss 0.665237, acc 0.671875
2016-11-12T19:07:15.936959: step 341, loss 0.664089, acc 0.71875
2016-11-12T19:07:15.992247: step 342, loss 0.886012, acc 0.5625
2016-11-12T19:07:16.048278: step 343, loss 0.624735, acc 0.671875
2016-11-12T19:07:16.104713: step 344, loss 0.488525, acc 0.75
2016-11-12T19:07:16.160389: step 345, loss 0.64452, acc 0.6875
2016-11-12T19:07:16.216177: step 346, loss 0.686531, acc 0.65625
2016-11-12T19:07:16.272192: step 347, loss 0.630165, acc 0.734375
2016-11-12T19:07:16.329135: step 348, loss 0.540102, acc 0.796875
2016-11-12T19:07:16.385382: step 349, loss 0.585004, acc 0.734375
2016-11-12T19:07:16.442382: step 350, loss 0.581227, acc 0.71875
2016-11-12T19:07:16.497761: step 351, loss 0.741264, acc 0.671875
2016-11-12T19:07:16.554432: step 352, loss 0.470683, acc 0.78125
2016-11-12T19:07:16.611859: step 353, loss 0.641216, acc 0.71875
2016-11-12T19:07:16.670008: step 354, loss 0.697631, acc 0.6875
2016-11-12T19:07:16.708184: step 355, loss 0.517488, acc 0.8
2016-11-12T19:07:16.764931: step 356, loss 0.539193, acc 0.75
2016-11-12T19:07:16.821487: step 357, loss 0.523675, acc 0.75
2016-11-12T19:07:16.877829: step 358, loss 0.580109, acc 0.6875
2016-11-12T19:07:16.933272: step 359, loss 0.521184, acc 0.734375
2016-11-12T19:07:16.989005: step 360, loss 0.50039, acc 0.78125
2016-11-12T19:07:17.045051: step 361, loss 0.504109, acc 0.78125
2016-11-12T19:07:17.101275: step 362, loss 0.551718, acc 0.6875
2016-11-12T19:07:17.160078: step 363, loss 0.516097, acc 0.765625
2016-11-12T19:07:17.218116: step 364, loss 0.518927, acc 0.71875
2016-11-12T19:07:17.273483: step 365, loss 0.642421, acc 0.703125
2016-11-12T19:07:17.328711: step 366, loss 0.509944, acc 0.765625
2016-11-12T19:07:17.385338: step 367, loss 0.66099, acc 0.765625
2016-11-12T19:07:17.441406: step 368, loss 0.566559, acc 0.75
2016-11-12T19:07:17.498949: step 369, loss 0.671462, acc 0.65625
2016-11-12T19:07:17.555383: step 370, loss 0.633005, acc 0.6875
2016-11-12T19:07:17.611094: step 371, loss 0.538067, acc 0.796875
2016-11-12T19:07:17.667012: step 372, loss 0.385702, acc 0.78125
2016-11-12T19:07:17.722920: step 373, loss 0.69355, acc 0.65625
2016-11-12T19:07:17.780904: step 374, loss 0.530643, acc 0.71875
2016-11-12T19:07:17.840000: step 375, loss 0.480641, acc 0.765625
2016-11-12T19:07:17.896712: step 376, loss 0.492222, acc 0.84375
2016-11-12T19:07:17.955078: step 377, loss 0.660954, acc 0.734375
2016-11-12T19:07:18.013632: step 378, loss 0.461978, acc 0.796875
2016-11-12T19:07:18.069584: step 379, loss 0.603138, acc 0.734375
2016-11-12T19:07:18.126318: step 380, loss 0.569971, acc 0.734375
2016-11-12T19:07:18.181905: step 381, loss 0.573383, acc 0.71875
2016-11-12T19:07:18.237670: step 382, loss 0.625616, acc 0.6875
2016-11-12T19:07:18.293926: step 383, loss 0.462672, acc 0.75
2016-11-12T19:07:18.349219: step 384, loss 0.647167, acc 0.734375
2016-11-12T19:07:18.404609: step 385, loss 0.471199, acc 0.71875
2016-11-12T19:07:18.461362: step 386, loss 0.622441, acc 0.71875
2016-11-12T19:07:18.516044: step 387, loss 0.575092, acc 0.75
2016-11-12T19:07:18.571617: step 388, loss 0.745561, acc 0.703125
2016-11-12T19:07:18.628279: step 389, loss 0.49031, acc 0.6875
2016-11-12T19:07:18.684055: step 390, loss 0.557057, acc 0.703125
2016-11-12T19:07:18.740695: step 391, loss 0.447888, acc 0.828125
2016-11-12T19:07:18.797365: step 392, loss 0.59383, acc 0.71875
2016-11-12T19:07:18.853069: step 393, loss 0.470672, acc 0.75
2016-11-12T19:07:18.908866: step 394, loss 0.439769, acc 0.796875
2016-11-12T19:07:18.965546: step 395, loss 0.572827, acc 0.734375
2016-11-12T19:07:19.021844: step 396, loss 0.575766, acc 0.75
2016-11-12T19:07:19.078704: step 397, loss 0.559378, acc 0.71875
2016-11-12T19:07:19.136507: step 398, loss 0.677947, acc 0.671875
2016-11-12T19:07:19.192634: step 399, loss 0.533622, acc 0.703125
2016-11-12T19:07:19.251147: step 400, loss 0.676819, acc 0.640625

Evaluation:
2016-11-12T19:07:19.321710: step 400, loss 0.737349, acc 0.554

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-400

2016-11-12T19:07:19.824636: step 401, loss 0.615761, acc 0.71875
2016-11-12T19:07:19.881234: step 402, loss 0.408424, acc 0.859375
2016-11-12T19:07:19.940331: step 403, loss 0.503896, acc 0.765625
2016-11-12T19:07:19.996192: step 404, loss 0.490022, acc 0.78125
2016-11-12T19:07:20.056218: step 405, loss 0.583266, acc 0.734375
2016-11-12T19:07:20.113892: step 406, loss 0.536576, acc 0.71875
2016-11-12T19:07:20.173103: step 407, loss 0.41532, acc 0.84375
2016-11-12T19:07:20.231946: step 408, loss 0.639965, acc 0.734375
2016-11-12T19:07:20.290313: step 409, loss 0.448889, acc 0.75
2016-11-12T19:07:20.346564: step 410, loss 0.493445, acc 0.75
2016-11-12T19:07:20.402599: step 411, loss 0.657803, acc 0.6875
2016-11-12T19:07:20.457925: step 412, loss 0.639811, acc 0.6875
2016-11-12T19:07:20.513463: step 413, loss 0.444496, acc 0.8125
2016-11-12T19:07:20.569178: step 414, loss 0.62137, acc 0.65625
2016-11-12T19:07:20.625055: step 415, loss 0.450954, acc 0.78125
2016-11-12T19:07:20.681076: step 416, loss 0.477652, acc 0.8125
2016-11-12T19:07:20.736954: step 417, loss 0.704016, acc 0.734375
2016-11-12T19:07:20.791515: step 418, loss 0.41848, acc 0.78125
2016-11-12T19:07:20.846926: step 419, loss 0.499534, acc 0.8125
2016-11-12T19:07:20.902859: step 420, loss 0.364543, acc 0.875
2016-11-12T19:07:20.958867: step 421, loss 0.450804, acc 0.828125
2016-11-12T19:07:21.015104: step 422, loss 0.473569, acc 0.75
2016-11-12T19:07:21.072510: step 423, loss 0.485539, acc 0.734375
2016-11-12T19:07:21.129068: step 424, loss 0.620362, acc 0.75
2016-11-12T19:07:21.185193: step 425, loss 0.417231, acc 0.828125
2016-11-12T19:07:21.221601: step 426, loss 0.688027, acc 0.75
2016-11-12T19:07:21.281240: step 427, loss 0.617844, acc 0.640625
2016-11-12T19:07:21.337173: step 428, loss 0.449688, acc 0.796875
2016-11-12T19:07:21.393358: step 429, loss 0.424005, acc 0.8125
2016-11-12T19:07:21.449403: step 430, loss 0.419347, acc 0.84375
2016-11-12T19:07:21.505483: step 431, loss 0.455416, acc 0.75
2016-11-12T19:07:21.560950: step 432, loss 0.375474, acc 0.828125
2016-11-12T19:07:21.616870: step 433, loss 0.498941, acc 0.796875
2016-11-12T19:07:21.673268: step 434, loss 0.511009, acc 0.8125
2016-11-12T19:07:21.730038: step 435, loss 0.435814, acc 0.796875
2016-11-12T19:07:21.786705: step 436, loss 0.632694, acc 0.671875
2016-11-12T19:07:21.844322: step 437, loss 0.461235, acc 0.765625
2016-11-12T19:07:21.901560: step 438, loss 0.564025, acc 0.75
2016-11-12T19:07:21.956698: step 439, loss 0.4776, acc 0.765625
2016-11-12T19:07:22.011821: step 440, loss 0.649005, acc 0.75
2016-11-12T19:07:22.066841: step 441, loss 0.435086, acc 0.78125
2016-11-12T19:07:22.124893: step 442, loss 0.497967, acc 0.765625
2016-11-12T19:07:22.180609: step 443, loss 0.45017, acc 0.828125
2016-11-12T19:07:22.236155: step 444, loss 0.406056, acc 0.828125
2016-11-12T19:07:22.292298: step 445, loss 0.441642, acc 0.765625
2016-11-12T19:07:22.347295: step 446, loss 0.463315, acc 0.765625
2016-11-12T19:07:22.405418: step 447, loss 0.478761, acc 0.765625
2016-11-12T19:07:22.461379: step 448, loss 0.532108, acc 0.8125
2016-11-12T19:07:22.518991: step 449, loss 0.580215, acc 0.734375
2016-11-12T19:07:22.575104: step 450, loss 0.48908, acc 0.734375
2016-11-12T19:07:22.630044: step 451, loss 0.370134, acc 0.828125
2016-11-12T19:07:22.684879: step 452, loss 0.393944, acc 0.8125
2016-11-12T19:07:22.740582: step 453, loss 0.473032, acc 0.796875
2016-11-12T19:07:22.797254: step 454, loss 0.415713, acc 0.828125
2016-11-12T19:07:22.854350: step 455, loss 0.417141, acc 0.8125
2016-11-12T19:07:22.912374: step 456, loss 0.610288, acc 0.65625
2016-11-12T19:07:22.969132: step 457, loss 0.337018, acc 0.859375
2016-11-12T19:07:23.024188: step 458, loss 0.666073, acc 0.671875
2016-11-12T19:07:23.080494: step 459, loss 0.512225, acc 0.734375
2016-11-12T19:07:23.136080: step 460, loss 0.410163, acc 0.828125
2016-11-12T19:07:23.195143: step 461, loss 0.474359, acc 0.8125
2016-11-12T19:07:23.253327: step 462, loss 0.574741, acc 0.703125
2016-11-12T19:07:23.309238: step 463, loss 0.466026, acc 0.75
2016-11-12T19:07:23.364200: step 464, loss 0.45693, acc 0.828125
2016-11-12T19:07:23.419340: step 465, loss 0.42794, acc 0.84375
2016-11-12T19:07:23.476169: step 466, loss 0.582109, acc 0.75
2016-11-12T19:07:23.532083: step 467, loss 0.60569, acc 0.671875
2016-11-12T19:07:23.587752: step 468, loss 0.611503, acc 0.703125
2016-11-12T19:07:23.642293: step 469, loss 0.498916, acc 0.703125
2016-11-12T19:07:23.698995: step 470, loss 0.472853, acc 0.796875
2016-11-12T19:07:23.756024: step 471, loss 0.62011, acc 0.765625
2016-11-12T19:07:23.813578: step 472, loss 0.507199, acc 0.75
2016-11-12T19:07:23.871118: step 473, loss 0.44986, acc 0.75
2016-11-12T19:07:23.925850: step 474, loss 0.430218, acc 0.8125
2016-11-12T19:07:23.981558: step 475, loss 0.594017, acc 0.765625
2016-11-12T19:07:24.038348: step 476, loss 0.541098, acc 0.796875
2016-11-12T19:07:24.093825: step 477, loss 0.382963, acc 0.78125
2016-11-12T19:07:24.149367: step 478, loss 0.458771, acc 0.796875
2016-11-12T19:07:24.204472: step 479, loss 0.362252, acc 0.859375
2016-11-12T19:07:24.260211: step 480, loss 0.365946, acc 0.828125
2016-11-12T19:07:24.317189: step 481, loss 0.48085, acc 0.765625
2016-11-12T19:07:24.375501: step 482, loss 0.433967, acc 0.765625
2016-11-12T19:07:24.433950: step 483, loss 0.447982, acc 0.828125
2016-11-12T19:07:24.490975: step 484, loss 0.479698, acc 0.75
2016-11-12T19:07:24.546135: step 485, loss 0.55772, acc 0.78125
2016-11-12T19:07:24.600978: step 486, loss 0.465078, acc 0.78125
2016-11-12T19:07:24.657273: step 487, loss 0.473317, acc 0.75
2016-11-12T19:07:24.713881: step 488, loss 0.483844, acc 0.734375
2016-11-12T19:07:24.769465: step 489, loss 0.321708, acc 0.921875
2016-11-12T19:07:24.825899: step 490, loss 0.533438, acc 0.75
2016-11-12T19:07:24.881854: step 491, loss 0.512142, acc 0.78125
2016-11-12T19:07:24.938000: step 492, loss 0.335058, acc 0.890625
2016-11-12T19:07:24.996061: step 493, loss 0.40323, acc 0.765625
2016-11-12T19:07:25.051533: step 494, loss 0.438539, acc 0.859375
2016-11-12T19:07:25.109334: step 495, loss 0.561132, acc 0.734375
2016-11-12T19:07:25.168431: step 496, loss 0.697704, acc 0.609375
2016-11-12T19:07:25.205298: step 497, loss 0.508485, acc 0.75
2016-11-12T19:07:25.261584: step 498, loss 0.36971, acc 0.796875
2016-11-12T19:07:25.317315: step 499, loss 0.356447, acc 0.84375
2016-11-12T19:07:25.373018: step 500, loss 0.480901, acc 0.796875

Evaluation:
2016-11-12T19:07:25.443388: step 500, loss 0.756373, acc 0.556

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-500

2016-11-12T19:07:25.944977: step 501, loss 0.45456, acc 0.796875
2016-11-12T19:07:26.001080: step 502, loss 0.379956, acc 0.8125
2016-11-12T19:07:26.057461: step 503, loss 0.401941, acc 0.796875
2016-11-12T19:07:26.113224: step 504, loss 0.539957, acc 0.71875
2016-11-12T19:07:26.169105: step 505, loss 0.348973, acc 0.875
2016-11-12T19:07:26.224601: step 506, loss 0.306091, acc 0.84375
2016-11-12T19:07:26.280361: step 507, loss 0.405281, acc 0.796875
2016-11-12T19:07:26.337212: step 508, loss 0.427597, acc 0.78125
2016-11-12T19:07:26.392706: step 509, loss 0.347553, acc 0.90625
2016-11-12T19:07:26.449158: step 510, loss 0.547643, acc 0.765625
2016-11-12T19:07:26.504796: step 511, loss 0.370279, acc 0.796875
2016-11-12T19:07:26.559861: step 512, loss 0.494592, acc 0.765625
2016-11-12T19:07:26.617285: step 513, loss 0.424476, acc 0.84375
2016-11-12T19:07:26.673311: step 514, loss 0.358931, acc 0.8125
2016-11-12T19:07:26.729484: step 515, loss 0.327363, acc 0.859375
2016-11-12T19:07:26.788478: step 516, loss 0.490045, acc 0.703125
2016-11-12T19:07:26.844361: step 517, loss 0.41269, acc 0.828125
2016-11-12T19:07:26.900045: step 518, loss 0.410187, acc 0.796875
2016-11-12T19:07:26.957454: step 519, loss 0.459283, acc 0.765625
2016-11-12T19:07:27.014381: step 520, loss 0.54471, acc 0.75
2016-11-12T19:07:27.071146: step 521, loss 0.451009, acc 0.828125
2016-11-12T19:07:27.128770: step 522, loss 0.50608, acc 0.703125
2016-11-12T19:07:27.183663: step 523, loss 0.396925, acc 0.84375
2016-11-12T19:07:27.240975: step 524, loss 0.46788, acc 0.78125
2016-11-12T19:07:27.299279: step 525, loss 0.519456, acc 0.796875
2016-11-12T19:07:27.354530: step 526, loss 0.430753, acc 0.765625
2016-11-12T19:07:27.410729: step 527, loss 0.500215, acc 0.765625
2016-11-12T19:07:27.466483: step 528, loss 0.350894, acc 0.84375
2016-11-12T19:07:27.525031: step 529, loss 0.448934, acc 0.78125
2016-11-12T19:07:27.580267: step 530, loss 0.556798, acc 0.6875
2016-11-12T19:07:27.635864: step 531, loss 0.445688, acc 0.78125
2016-11-12T19:07:27.691793: step 532, loss 0.39373, acc 0.84375
2016-11-12T19:07:27.747286: step 533, loss 0.409002, acc 0.84375
2016-11-12T19:07:27.803025: step 534, loss 0.544355, acc 0.765625
2016-11-12T19:07:27.859031: step 535, loss 0.408003, acc 0.859375
2016-11-12T19:07:27.914796: step 536, loss 0.316114, acc 0.859375
2016-11-12T19:07:27.970012: step 537, loss 0.377302, acc 0.796875
2016-11-12T19:07:28.025174: step 538, loss 0.403379, acc 0.796875
2016-11-12T19:07:28.081795: step 539, loss 0.361064, acc 0.828125
2016-11-12T19:07:28.138565: step 540, loss 0.396668, acc 0.78125
2016-11-12T19:07:28.193642: step 541, loss 0.354169, acc 0.8125
2016-11-12T19:07:28.249207: step 542, loss 0.467716, acc 0.8125
2016-11-12T19:07:28.304600: step 543, loss 0.453253, acc 0.84375
2016-11-12T19:07:28.360185: step 544, loss 0.29078, acc 0.890625
2016-11-12T19:07:28.416089: step 545, loss 0.42667, acc 0.796875
2016-11-12T19:07:28.470732: step 546, loss 0.491021, acc 0.78125
2016-11-12T19:07:28.526329: step 547, loss 0.344303, acc 0.84375
2016-11-12T19:07:28.581051: step 548, loss 0.336682, acc 0.875
2016-11-12T19:07:28.637342: step 549, loss 0.447998, acc 0.8125
2016-11-12T19:07:28.692849: step 550, loss 0.470576, acc 0.78125
2016-11-12T19:07:28.749404: step 551, loss 0.414103, acc 0.8125
2016-11-12T19:07:28.806796: step 552, loss 0.362041, acc 0.828125
2016-11-12T19:07:28.862866: step 553, loss 0.460379, acc 0.796875
2016-11-12T19:07:28.920848: step 554, loss 0.483192, acc 0.765625
2016-11-12T19:07:28.975706: step 555, loss 0.464254, acc 0.734375
2016-11-12T19:07:29.033321: step 556, loss 0.443611, acc 0.8125
2016-11-12T19:07:29.089031: step 557, loss 0.282299, acc 0.875
2016-11-12T19:07:29.144565: step 558, loss 0.435987, acc 0.828125
2016-11-12T19:07:29.200574: step 559, loss 0.509858, acc 0.671875
2016-11-12T19:07:29.256621: step 560, loss 0.433286, acc 0.765625
2016-11-12T19:07:29.313220: step 561, loss 0.315923, acc 0.859375
2016-11-12T19:07:29.369605: step 562, loss 0.481459, acc 0.796875
2016-11-12T19:07:29.427350: step 563, loss 0.304841, acc 0.90625
2016-11-12T19:07:29.485692: step 564, loss 0.42275, acc 0.78125
2016-11-12T19:07:29.542371: step 565, loss 0.323188, acc 0.84375
2016-11-12T19:07:29.601139: step 566, loss 0.434587, acc 0.78125
2016-11-12T19:07:29.657335: step 567, loss 0.553685, acc 0.734375
2016-11-12T19:07:29.696365: step 568, loss 0.356453, acc 0.85
2016-11-12T19:07:29.753241: step 569, loss 0.409953, acc 0.8125
2016-11-12T19:07:29.809464: step 570, loss 0.348789, acc 0.9375
2016-11-12T19:07:29.864819: step 571, loss 0.475869, acc 0.78125
2016-11-12T19:07:29.920231: step 572, loss 0.392527, acc 0.765625
2016-11-12T19:07:29.976457: step 573, loss 0.378366, acc 0.875
2016-11-12T19:07:30.031872: step 574, loss 0.385377, acc 0.84375
2016-11-12T19:07:30.086996: step 575, loss 0.395544, acc 0.859375
2016-11-12T19:07:30.145559: step 576, loss 0.487576, acc 0.75
2016-11-12T19:07:30.202242: step 577, loss 0.395325, acc 0.8125
2016-11-12T19:07:30.259733: step 578, loss 0.432798, acc 0.828125
2016-11-12T19:07:30.315248: step 579, loss 0.352722, acc 0.84375
2016-11-12T19:07:30.370167: step 580, loss 0.350799, acc 0.8125
2016-11-12T19:07:30.425531: step 581, loss 0.309527, acc 0.84375
2016-11-12T19:07:30.482490: step 582, loss 0.383878, acc 0.890625
2016-11-12T19:07:30.539315: step 583, loss 0.282184, acc 0.890625
2016-11-12T19:07:30.594223: step 584, loss 0.309986, acc 0.890625
2016-11-12T19:07:30.648682: step 585, loss 0.303249, acc 0.84375
2016-11-12T19:07:30.708044: step 586, loss 0.352892, acc 0.765625
2016-11-12T19:07:30.765870: step 587, loss 0.269999, acc 0.90625
2016-11-12T19:07:30.821844: step 588, loss 0.45511, acc 0.75
2016-11-12T19:07:30.876995: step 589, loss 0.419209, acc 0.8125
2016-11-12T19:07:30.934012: step 590, loss 0.363254, acc 0.859375
2016-11-12T19:07:30.988878: step 591, loss 0.448037, acc 0.828125
2016-11-12T19:07:31.043881: step 592, loss 0.461399, acc 0.78125
2016-11-12T19:07:31.098842: step 593, loss 0.55041, acc 0.71875
2016-11-12T19:07:31.156082: step 594, loss 0.482182, acc 0.796875
2016-11-12T19:07:31.210808: step 595, loss 0.37049, acc 0.84375
2016-11-12T19:07:31.268437: step 596, loss 0.510339, acc 0.78125
2016-11-12T19:07:31.324798: step 597, loss 0.446048, acc 0.78125
2016-11-12T19:07:31.381364: step 598, loss 0.519029, acc 0.734375
2016-11-12T19:07:31.436218: step 599, loss 0.415158, acc 0.84375
2016-11-12T19:07:31.493354: step 600, loss 0.313339, acc 0.90625

Evaluation:
2016-11-12T19:07:31.563568: step 600, loss 0.812921, acc 0.554

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-600

2016-11-12T19:07:32.064406: step 601, loss 0.428136, acc 0.828125
2016-11-12T19:07:32.124886: step 602, loss 0.437293, acc 0.828125
2016-11-12T19:07:32.179912: step 603, loss 0.382157, acc 0.859375
2016-11-12T19:07:32.234896: step 604, loss 0.51728, acc 0.765625
2016-11-12T19:07:32.289855: step 605, loss 0.378716, acc 0.8125
2016-11-12T19:07:32.346465: step 606, loss 0.295595, acc 0.875
2016-11-12T19:07:32.402623: step 607, loss 0.438045, acc 0.84375
2016-11-12T19:07:32.459350: step 608, loss 0.42832, acc 0.796875
2016-11-12T19:07:32.513989: step 609, loss 0.319701, acc 0.875
2016-11-12T19:07:32.570493: step 610, loss 0.425065, acc 0.828125
2016-11-12T19:07:32.625665: step 611, loss 0.319568, acc 0.921875
2016-11-12T19:07:32.682239: step 612, loss 0.316447, acc 0.828125
2016-11-12T19:07:32.737378: step 613, loss 0.435788, acc 0.78125
2016-11-12T19:07:32.794819: step 614, loss 0.316967, acc 0.859375
2016-11-12T19:07:32.853981: step 615, loss 0.322874, acc 0.859375
2016-11-12T19:07:32.910840: step 616, loss 0.444034, acc 0.796875
2016-11-12T19:07:32.966604: step 617, loss 0.319273, acc 0.859375
2016-11-12T19:07:33.021316: step 618, loss 0.376253, acc 0.859375
2016-11-12T19:07:33.076404: step 619, loss 0.265386, acc 0.90625
2016-11-12T19:07:33.133376: step 620, loss 0.279446, acc 0.890625
2016-11-12T19:07:33.188987: step 621, loss 0.508813, acc 0.734375
2016-11-12T19:07:33.244784: step 622, loss 0.254875, acc 0.90625
2016-11-12T19:07:33.299922: step 623, loss 0.387739, acc 0.828125
2016-11-12T19:07:33.355329: step 624, loss 0.426788, acc 0.8125
2016-11-12T19:07:33.411910: step 625, loss 0.375057, acc 0.828125
2016-11-12T19:07:33.469200: step 626, loss 0.396705, acc 0.796875
2016-11-12T19:07:33.524561: step 627, loss 0.419747, acc 0.765625
2016-11-12T19:07:33.581294: step 628, loss 0.509478, acc 0.765625
2016-11-12T19:07:33.637114: step 629, loss 0.506472, acc 0.703125
2016-11-12T19:07:33.694162: step 630, loss 0.322572, acc 0.90625
2016-11-12T19:07:33.749251: step 631, loss 0.434838, acc 0.8125
2016-11-12T19:07:33.806563: step 632, loss 0.460359, acc 0.796875
2016-11-12T19:07:33.861525: step 633, loss 0.44863, acc 0.796875
2016-11-12T19:07:33.916806: step 634, loss 0.386715, acc 0.90625
2016-11-12T19:07:33.973962: step 635, loss 0.318724, acc 0.890625
2016-11-12T19:07:34.030214: step 636, loss 0.423576, acc 0.828125
2016-11-12T19:07:34.085983: step 637, loss 0.388226, acc 0.828125
2016-11-12T19:07:34.141317: step 638, loss 0.390427, acc 0.796875
2016-11-12T19:07:34.178554: step 639, loss 0.442763, acc 0.75
2016-11-12T19:07:34.235085: step 640, loss 0.268388, acc 0.875
2016-11-12T19:07:34.289806: step 641, loss 0.317915, acc 0.828125
2016-11-12T19:07:34.345727: step 642, loss 0.332724, acc 0.921875
2016-11-12T19:07:34.401864: step 643, loss 0.393815, acc 0.78125
2016-11-12T19:07:34.457253: step 644, loss 0.349124, acc 0.84375
2016-11-12T19:07:34.513867: step 645, loss 0.370977, acc 0.828125
2016-11-12T19:07:34.572700: step 646, loss 0.263172, acc 0.890625
2016-11-12T19:07:34.628158: step 647, loss 0.20218, acc 0.921875
2016-11-12T19:07:34.683752: step 648, loss 0.33222, acc 0.828125
2016-11-12T19:07:34.739105: step 649, loss 0.335325, acc 0.828125
2016-11-12T19:07:34.797029: step 650, loss 0.289487, acc 0.875
2016-11-12T19:07:34.853854: step 651, loss 0.343213, acc 0.859375
2016-11-12T19:07:34.909272: step 652, loss 0.27535, acc 0.875
2016-11-12T19:07:34.966928: step 653, loss 0.314772, acc 0.859375
2016-11-12T19:07:35.022894: step 654, loss 0.409126, acc 0.828125
2016-11-12T19:07:35.078782: step 655, loss 0.331446, acc 0.859375
2016-11-12T19:07:35.134201: step 656, loss 0.318946, acc 0.84375
2016-11-12T19:07:35.193201: step 657, loss 0.40227, acc 0.828125
2016-11-12T19:07:35.248312: step 658, loss 0.322101, acc 0.859375
2016-11-12T19:07:35.306338: step 659, loss 0.305492, acc 0.859375
2016-11-12T19:07:35.363439: step 660, loss 0.430841, acc 0.828125
2016-11-12T19:07:35.421163: step 661, loss 0.385004, acc 0.84375
2016-11-12T19:07:35.476584: step 662, loss 0.403233, acc 0.828125
2016-11-12T19:07:35.534583: step 663, loss 0.363965, acc 0.8125
2016-11-12T19:07:35.592838: step 664, loss 0.294016, acc 0.890625
2016-11-12T19:07:35.649348: step 665, loss 0.380034, acc 0.84375
2016-11-12T19:07:35.705192: step 666, loss 0.345994, acc 0.78125
2016-11-12T19:07:35.761268: step 667, loss 0.350756, acc 0.796875
2016-11-12T19:07:35.817831: step 668, loss 0.2591, acc 0.953125
2016-11-12T19:07:35.873362: step 669, loss 0.332793, acc 0.859375
2016-11-12T19:07:35.929555: step 670, loss 0.347156, acc 0.8125
2016-11-12T19:07:35.985363: step 671, loss 0.281141, acc 0.859375
2016-11-12T19:07:36.042649: step 672, loss 0.259356, acc 0.875
2016-11-12T19:07:36.101293: step 673, loss 0.301593, acc 0.84375
2016-11-12T19:07:36.158885: step 674, loss 0.304449, acc 0.875
2016-11-12T19:07:36.214399: step 675, loss 0.264647, acc 0.90625
2016-11-12T19:07:36.269412: step 676, loss 0.405656, acc 0.84375
2016-11-12T19:07:36.325079: step 677, loss 0.340386, acc 0.875
2016-11-12T19:07:36.380111: step 678, loss 0.311573, acc 0.84375
2016-11-12T19:07:36.436853: step 679, loss 0.32521, acc 0.859375
2016-11-12T19:07:36.494024: step 680, loss 0.349211, acc 0.875
2016-11-12T19:07:36.549448: step 681, loss 0.300227, acc 0.84375
2016-11-12T19:07:36.604908: step 682, loss 0.310163, acc 0.859375
2016-11-12T19:07:36.660712: step 683, loss 0.253126, acc 0.953125
2016-11-12T19:07:36.717851: step 684, loss 0.347538, acc 0.796875
2016-11-12T19:07:36.775636: step 685, loss 0.334409, acc 0.875
2016-11-12T19:07:36.832374: step 686, loss 0.26416, acc 0.90625
2016-11-12T19:07:36.889000: step 687, loss 0.399663, acc 0.8125
2016-11-12T19:07:36.944860: step 688, loss 0.251821, acc 0.9375
2016-11-12T19:07:37.001196: step 689, loss 0.265919, acc 0.875
2016-11-12T19:07:37.060053: step 690, loss 0.331672, acc 0.84375
2016-11-12T19:07:37.117915: step 691, loss 0.186913, acc 0.9375
2016-11-12T19:07:37.173708: step 692, loss 0.329805, acc 0.859375
2016-11-12T19:07:37.228730: step 693, loss 0.351209, acc 0.84375
2016-11-12T19:07:37.284021: step 694, loss 0.256307, acc 0.875
2016-11-12T19:07:37.339519: step 695, loss 0.296567, acc 0.875
2016-11-12T19:07:37.397663: step 696, loss 0.414043, acc 0.84375
2016-11-12T19:07:37.452975: step 697, loss 0.416286, acc 0.828125
2016-11-12T19:07:37.510179: step 698, loss 0.384941, acc 0.859375
2016-11-12T19:07:37.565915: step 699, loss 0.301664, acc 0.90625
2016-11-12T19:07:37.623493: step 700, loss 0.290867, acc 0.875

Evaluation:
2016-11-12T19:07:37.695116: step 700, loss 0.797062, acc 0.542

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-700

2016-11-12T19:07:38.197855: step 701, loss 0.331366, acc 0.875
2016-11-12T19:07:38.253712: step 702, loss 0.309514, acc 0.84375
2016-11-12T19:07:38.310869: step 703, loss 0.379258, acc 0.828125
2016-11-12T19:07:38.366112: step 704, loss 0.408606, acc 0.859375
2016-11-12T19:07:38.421147: step 705, loss 0.378161, acc 0.78125
2016-11-12T19:07:38.475911: step 706, loss 0.33891, acc 0.890625
2016-11-12T19:07:38.533249: step 707, loss 0.36413, acc 0.828125
2016-11-12T19:07:38.588588: step 708, loss 0.228581, acc 0.9375
2016-11-12T19:07:38.645266: step 709, loss 0.225754, acc 0.921875
2016-11-12T19:07:38.684139: step 710, loss 0.389762, acc 0.75
2016-11-12T19:07:38.742198: step 711, loss 0.277964, acc 0.890625
2016-11-12T19:07:38.796663: step 712, loss 0.384022, acc 0.828125
2016-11-12T19:07:38.853328: step 713, loss 0.271615, acc 0.890625
2016-11-12T19:07:38.909317: step 714, loss 0.415466, acc 0.796875
2016-11-12T19:07:38.966360: step 715, loss 0.346285, acc 0.890625
2016-11-12T19:07:39.024222: step 716, loss 0.390585, acc 0.859375
2016-11-12T19:07:39.081172: step 717, loss 0.248203, acc 0.90625
2016-11-12T19:07:39.137707: step 718, loss 0.35984, acc 0.84375
2016-11-12T19:07:39.192490: step 719, loss 0.440034, acc 0.765625
2016-11-12T19:07:39.247723: step 720, loss 0.253754, acc 0.921875
2016-11-12T19:07:39.303279: step 721, loss 0.359672, acc 0.859375
2016-11-12T19:07:39.362117: step 722, loss 0.250239, acc 0.90625
2016-11-12T19:07:39.418420: step 723, loss 0.275664, acc 0.890625
2016-11-12T19:07:39.473750: step 724, loss 0.178437, acc 0.921875
2016-11-12T19:07:39.531276: step 725, loss 0.240695, acc 0.90625
2016-11-12T19:07:39.591106: step 726, loss 0.295517, acc 0.859375
2016-11-12T19:07:39.646150: step 727, loss 0.276634, acc 0.90625
2016-11-12T19:07:39.701185: step 728, loss 0.288993, acc 0.9375
2016-11-12T19:07:39.757474: step 729, loss 0.248701, acc 0.875
2016-11-12T19:07:39.813008: step 730, loss 0.331019, acc 0.859375
2016-11-12T19:07:39.867815: step 731, loss 0.257889, acc 0.90625
2016-11-12T19:07:39.924180: step 732, loss 0.388714, acc 0.796875
2016-11-12T19:07:39.981415: step 733, loss 0.289462, acc 0.859375
2016-11-12T19:07:40.041323: step 734, loss 0.337335, acc 0.828125
2016-11-12T19:07:40.097009: step 735, loss 0.32475, acc 0.859375
2016-11-12T19:07:40.153432: step 736, loss 0.265877, acc 0.90625
2016-11-12T19:07:40.209013: step 737, loss 0.211538, acc 0.953125
2016-11-12T19:07:40.265474: step 738, loss 0.426609, acc 0.84375
2016-11-12T19:07:40.320505: step 739, loss 0.277761, acc 0.890625
2016-11-12T19:07:40.375916: step 740, loss 0.416407, acc 0.734375
2016-11-12T19:07:40.430871: step 741, loss 0.318292, acc 0.859375
2016-11-12T19:07:40.485626: step 742, loss 0.363988, acc 0.90625
2016-11-12T19:07:40.541279: step 743, loss 0.245588, acc 0.921875
2016-11-12T19:07:40.596425: step 744, loss 0.337718, acc 0.84375
2016-11-12T19:07:40.652679: step 745, loss 0.314849, acc 0.84375
2016-11-12T19:07:40.709543: step 746, loss 0.205988, acc 0.953125
2016-11-12T19:07:40.764757: step 747, loss 0.297423, acc 0.859375
2016-11-12T19:07:40.820702: step 748, loss 0.262579, acc 0.875
2016-11-12T19:07:40.878817: step 749, loss 0.279198, acc 0.890625
2016-11-12T19:07:40.937470: step 750, loss 0.20115, acc 0.96875
2016-11-12T19:07:40.993715: step 751, loss 0.498833, acc 0.796875
2016-11-12T19:07:41.051970: step 752, loss 0.228802, acc 0.9375
2016-11-12T19:07:41.107862: step 753, loss 0.405303, acc 0.828125
2016-11-12T19:07:41.163048: step 754, loss 0.352769, acc 0.8125
2016-11-12T19:07:41.220213: step 755, loss 0.323871, acc 0.859375
2016-11-12T19:07:41.276763: step 756, loss 0.308291, acc 0.84375
2016-11-12T19:07:41.331739: step 757, loss 0.264162, acc 0.859375
2016-11-12T19:07:41.386944: step 758, loss 0.340441, acc 0.828125
2016-11-12T19:07:41.442282: step 759, loss 0.31124, acc 0.875
2016-11-12T19:07:41.498484: step 760, loss 0.241165, acc 0.875
2016-11-12T19:07:41.553319: step 761, loss 0.283741, acc 0.890625
2016-11-12T19:07:41.610492: step 762, loss 0.281235, acc 0.875
2016-11-12T19:07:41.668787: step 763, loss 0.285664, acc 0.90625
2016-11-12T19:07:41.723684: step 764, loss 0.259517, acc 0.890625
2016-11-12T19:07:41.779231: step 765, loss 0.31437, acc 0.875
2016-11-12T19:07:41.834321: step 766, loss 0.394695, acc 0.875
2016-11-12T19:07:41.889701: step 767, loss 0.22799, acc 0.890625
2016-11-12T19:07:41.945280: step 768, loss 0.191189, acc 0.921875
2016-11-12T19:07:42.004677: step 769, loss 0.322707, acc 0.875
2016-11-12T19:07:42.060221: step 770, loss 0.310892, acc 0.859375
2016-11-12T19:07:42.116097: step 771, loss 0.327773, acc 0.890625
2016-11-12T19:07:42.173672: step 772, loss 0.312183, acc 0.875
2016-11-12T19:07:42.229410: step 773, loss 0.309569, acc 0.859375
2016-11-12T19:07:42.284401: step 774, loss 0.295707, acc 0.859375
2016-11-12T19:07:42.341613: step 775, loss 0.302401, acc 0.859375
2016-11-12T19:07:42.397631: step 776, loss 0.29451, acc 0.875
2016-11-12T19:07:42.453325: step 777, loss 0.354107, acc 0.84375
2016-11-12T19:07:42.510408: step 778, loss 0.29939, acc 0.875
2016-11-12T19:07:42.566262: step 779, loss 0.274824, acc 0.84375
2016-11-12T19:07:42.622322: step 780, loss 0.272569, acc 0.890625
2016-11-12T19:07:42.659650: step 781, loss 0.699885, acc 0.8
2016-11-12T19:07:42.717861: step 782, loss 0.233731, acc 0.953125
2016-11-12T19:07:42.773462: step 783, loss 0.226141, acc 0.875
2016-11-12T19:07:42.828909: step 784, loss 0.274456, acc 0.875
2016-11-12T19:07:42.884772: step 785, loss 0.210882, acc 0.921875
2016-11-12T19:07:42.940338: step 786, loss 0.193845, acc 0.9375
2016-11-12T19:07:42.995769: step 787, loss 0.230891, acc 0.921875
2016-11-12T19:07:43.052754: step 788, loss 0.184683, acc 0.953125
2016-11-12T19:07:43.108332: step 789, loss 0.25379, acc 0.890625
2016-11-12T19:07:43.164872: step 790, loss 0.261231, acc 0.90625
2016-11-12T19:07:43.222116: step 791, loss 0.315242, acc 0.859375
2016-11-12T19:07:43.277121: step 792, loss 0.268588, acc 0.875
2016-11-12T19:07:43.332269: step 793, loss 0.216726, acc 0.921875
2016-11-12T19:07:43.389065: step 794, loss 0.191721, acc 0.953125
2016-11-12T19:07:43.445097: step 795, loss 0.198097, acc 0.9375
2016-11-12T19:07:43.501376: step 796, loss 0.136455, acc 0.984375
2016-11-12T19:07:43.557544: step 797, loss 0.282389, acc 0.859375
2016-11-12T19:07:43.613536: step 798, loss 0.264015, acc 0.921875
2016-11-12T19:07:43.670544: step 799, loss 0.162388, acc 0.9375
2016-11-12T19:07:43.728069: step 800, loss 0.224129, acc 0.90625

Evaluation:
2016-11-12T19:07:43.797826: step 800, loss 0.816179, acc 0.556

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-800

2016-11-12T19:07:44.296272: step 801, loss 0.303516, acc 0.859375
2016-11-12T19:07:44.351536: step 802, loss 0.199513, acc 0.9375
2016-11-12T19:07:44.407887: step 803, loss 0.299595, acc 0.859375
2016-11-12T19:07:44.465211: step 804, loss 0.200809, acc 0.9375
2016-11-12T19:07:44.521183: step 805, loss 0.170721, acc 0.921875
2016-11-12T19:07:44.576982: step 806, loss 0.279535, acc 0.90625
2016-11-12T19:07:44.633383: step 807, loss 0.336422, acc 0.84375
2016-11-12T19:07:44.689365: step 808, loss 0.241188, acc 0.90625
2016-11-12T19:07:44.746075: step 809, loss 0.27914, acc 0.890625
2016-11-12T19:07:44.806684: step 810, loss 0.181492, acc 0.96875
2016-11-12T19:07:44.863788: step 811, loss 0.231624, acc 0.890625
2016-11-12T19:07:44.919300: step 812, loss 0.267589, acc 0.875
2016-11-12T19:07:44.975264: step 813, loss 0.270663, acc 0.84375
2016-11-12T19:07:45.030381: step 814, loss 0.322656, acc 0.84375
2016-11-12T19:07:45.087064: step 815, loss 0.261661, acc 0.90625
2016-11-12T19:07:45.144867: step 816, loss 0.319337, acc 0.875
2016-11-12T19:07:45.200057: step 817, loss 0.244295, acc 0.875
2016-11-12T19:07:45.255512: step 818, loss 0.322856, acc 0.90625
2016-11-12T19:07:45.312741: step 819, loss 0.183789, acc 0.921875
2016-11-12T19:07:45.370228: step 820, loss 0.232613, acc 0.84375
2016-11-12T19:07:45.425817: step 821, loss 0.225467, acc 0.890625
2016-11-12T19:07:45.481118: step 822, loss 0.208502, acc 0.9375
2016-11-12T19:07:45.536457: step 823, loss 0.256632, acc 0.921875
2016-11-12T19:07:45.592261: step 824, loss 0.235879, acc 0.90625
2016-11-12T19:07:45.649817: step 825, loss 0.255905, acc 0.875
2016-11-12T19:07:45.705545: step 826, loss 0.29385, acc 0.890625
2016-11-12T19:07:45.761192: step 827, loss 0.311533, acc 0.84375
2016-11-12T19:07:45.817155: step 828, loss 0.328648, acc 0.875
2016-11-12T19:07:45.873279: step 829, loss 0.324698, acc 0.8125
2016-11-12T19:07:45.929249: step 830, loss 0.380618, acc 0.828125
2016-11-12T19:07:45.984988: step 831, loss 0.234087, acc 0.90625
2016-11-12T19:07:46.040233: step 832, loss 0.292816, acc 0.859375
2016-11-12T19:07:46.096068: step 833, loss 0.273332, acc 0.859375
2016-11-12T19:07:46.152748: step 834, loss 0.205826, acc 0.90625
2016-11-12T19:07:46.209905: step 835, loss 0.290313, acc 0.84375
2016-11-12T19:07:46.265134: step 836, loss 0.269571, acc 0.859375
2016-11-12T19:07:46.322297: step 837, loss 0.28479, acc 0.859375
2016-11-12T19:07:46.381079: step 838, loss 0.290919, acc 0.859375
2016-11-12T19:07:46.435833: step 839, loss 0.33428, acc 0.859375
2016-11-12T19:07:46.493552: step 840, loss 0.270997, acc 0.90625
2016-11-12T19:07:46.549273: step 841, loss 0.243989, acc 0.90625
2016-11-12T19:07:46.607582: step 842, loss 0.25311, acc 0.90625
2016-11-12T19:07:46.662853: step 843, loss 0.26722, acc 0.875
2016-11-12T19:07:46.720172: step 844, loss 0.280032, acc 0.90625
2016-11-12T19:07:46.780117: step 845, loss 0.387555, acc 0.796875
2016-11-12T19:07:46.836023: step 846, loss 0.244622, acc 0.90625
2016-11-12T19:07:46.891069: step 847, loss 0.287342, acc 0.890625
2016-11-12T19:07:46.948625: step 848, loss 0.23545, acc 0.875
2016-11-12T19:07:47.005284: step 849, loss 0.280153, acc 0.84375
2016-11-12T19:07:47.061554: step 850, loss 0.265935, acc 0.890625
2016-11-12T19:07:47.116657: step 851, loss 0.265788, acc 0.859375
2016-11-12T19:07:47.152065: step 852, loss 0.298103, acc 0.95
2016-11-12T19:07:47.208508: step 853, loss 0.205067, acc 0.921875
2016-11-12T19:07:47.266711: step 854, loss 0.205495, acc 0.9375
2016-11-12T19:07:47.322191: step 855, loss 0.233817, acc 0.921875
2016-11-12T19:07:47.379492: step 856, loss 0.289215, acc 0.875
2016-11-12T19:07:47.436472: step 857, loss 0.211374, acc 0.921875
2016-11-12T19:07:47.493631: step 858, loss 0.288319, acc 0.875
2016-11-12T19:07:47.549466: step 859, loss 0.147722, acc 0.953125
2016-11-12T19:07:47.605605: step 860, loss 0.214851, acc 0.921875
2016-11-12T19:07:47.660442: step 861, loss 0.268096, acc 0.90625
2016-11-12T19:07:47.715620: step 862, loss 0.249346, acc 0.90625
2016-11-12T19:07:47.771593: step 863, loss 0.188251, acc 0.921875
2016-11-12T19:07:47.827846: step 864, loss 0.224581, acc 0.890625
2016-11-12T19:07:47.885316: step 865, loss 0.275093, acc 0.90625
2016-11-12T19:07:47.939962: step 866, loss 0.354603, acc 0.84375
2016-11-12T19:07:47.995451: step 867, loss 0.319086, acc 0.90625
2016-11-12T19:07:48.051440: step 868, loss 0.273327, acc 0.890625
2016-11-12T19:07:48.108932: step 869, loss 0.230912, acc 0.90625
2016-11-12T19:07:48.166150: step 870, loss 0.213099, acc 0.921875
2016-11-12T19:07:48.221129: step 871, loss 0.193695, acc 0.96875
2016-11-12T19:07:48.276829: step 872, loss 0.219237, acc 0.921875
2016-11-12T19:07:48.332987: step 873, loss 0.25629, acc 0.9375
2016-11-12T19:07:48.390577: step 874, loss 0.31206, acc 0.84375
2016-11-12T19:07:48.445769: step 875, loss 0.500337, acc 0.765625
2016-11-12T19:07:48.500602: step 876, loss 0.297639, acc 0.859375
2016-11-12T19:07:48.555751: step 877, loss 0.186551, acc 0.9375
2016-11-12T19:07:48.611340: step 878, loss 0.247372, acc 0.90625
2016-11-12T19:07:48.667811: step 879, loss 0.203445, acc 0.96875
2016-11-12T19:07:48.723179: step 880, loss 0.289421, acc 0.859375
2016-11-12T19:07:48.781378: step 881, loss 0.222892, acc 0.875
2016-11-12T19:07:48.837524: step 882, loss 0.223006, acc 0.890625
2016-11-12T19:07:48.892729: step 883, loss 0.236114, acc 0.90625
2016-11-12T19:07:48.948951: step 884, loss 0.262729, acc 0.90625
2016-11-12T19:07:49.004801: step 885, loss 0.372473, acc 0.859375
2016-11-12T19:07:49.059644: step 886, loss 0.176916, acc 0.9375
2016-11-12T19:07:49.117152: step 887, loss 0.188392, acc 0.9375
2016-11-12T19:07:49.172714: step 888, loss 0.19658, acc 0.9375
2016-11-12T19:07:49.229200: step 889, loss 0.210119, acc 0.90625
2016-11-12T19:07:49.285252: step 890, loss 0.282913, acc 0.875
2016-11-12T19:07:49.342010: step 891, loss 0.24086, acc 0.90625
2016-11-12T19:07:49.397226: step 892, loss 0.261071, acc 0.921875
2016-11-12T19:07:49.453277: step 893, loss 0.254665, acc 0.90625
2016-11-12T19:07:49.512545: step 894, loss 0.280206, acc 0.875
2016-11-12T19:07:49.567885: step 895, loss 0.24349, acc 0.890625
2016-11-12T19:07:49.623727: step 896, loss 0.163356, acc 0.9375
2016-11-12T19:07:49.680246: step 897, loss 0.206697, acc 0.9375
2016-11-12T19:07:49.738000: step 898, loss 0.309963, acc 0.875
2016-11-12T19:07:49.793190: step 899, loss 0.167878, acc 0.953125
2016-11-12T19:07:49.849990: step 900, loss 0.24879, acc 0.890625

Evaluation:
2016-11-12T19:07:49.920363: step 900, loss 0.856938, acc 0.56

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-900

2016-11-12T19:07:50.418818: step 901, loss 0.283525, acc 0.890625
2016-11-12T19:07:50.474451: step 902, loss 0.153854, acc 0.96875
2016-11-12T19:07:50.530122: step 903, loss 0.300314, acc 0.890625
2016-11-12T19:07:50.587377: step 904, loss 0.235252, acc 0.90625
2016-11-12T19:07:50.645663: step 905, loss 0.249284, acc 0.921875
2016-11-12T19:07:50.701161: step 906, loss 0.227453, acc 0.921875
2016-11-12T19:07:50.756539: step 907, loss 0.124835, acc 1
2016-11-12T19:07:50.812216: step 908, loss 0.225797, acc 0.890625
2016-11-12T19:07:50.869005: step 909, loss 0.303181, acc 0.890625
2016-11-12T19:07:50.925061: step 910, loss 0.238182, acc 0.921875
2016-11-12T19:07:50.980214: step 911, loss 0.221563, acc 0.90625
2016-11-12T19:07:51.036970: step 912, loss 0.172267, acc 0.96875
2016-11-12T19:07:51.094279: step 913, loss 0.206018, acc 0.9375
2016-11-12T19:07:51.149499: step 914, loss 0.169452, acc 0.921875
2016-11-12T19:07:51.205383: step 915, loss 0.231335, acc 0.890625
2016-11-12T19:07:51.261213: step 916, loss 0.19661, acc 0.9375
2016-11-12T19:07:51.318093: step 917, loss 0.240458, acc 0.90625
2016-11-12T19:07:51.373565: step 918, loss 0.311387, acc 0.859375
2016-11-12T19:07:51.429244: step 919, loss 0.170063, acc 0.90625
2016-11-12T19:07:51.484481: step 920, loss 0.3675, acc 0.859375
2016-11-12T19:07:51.539737: step 921, loss 0.225196, acc 0.90625
2016-11-12T19:07:51.597186: step 922, loss 0.19108, acc 0.921875
2016-11-12T19:07:51.633232: step 923, loss 0.215644, acc 0.95
2016-11-12T19:07:51.688903: step 924, loss 0.360328, acc 0.796875
2016-11-12T19:07:51.744372: step 925, loss 0.212955, acc 0.90625
2016-11-12T19:07:51.800355: step 926, loss 0.194284, acc 0.890625
2016-11-12T19:07:51.856770: step 927, loss 0.195225, acc 0.921875
2016-11-12T19:07:51.913411: step 928, loss 0.230268, acc 0.9375
2016-11-12T19:07:51.969335: step 929, loss 0.211143, acc 0.90625
2016-11-12T19:07:52.025330: step 930, loss 0.187954, acc 0.9375
2016-11-12T19:07:52.084436: step 931, loss 0.229235, acc 0.859375
2016-11-12T19:07:52.141250: step 932, loss 0.153503, acc 0.96875
2016-11-12T19:07:52.195956: step 933, loss 0.139868, acc 0.953125
2016-11-12T19:07:52.253162: step 934, loss 0.275251, acc 0.921875
2016-11-12T19:07:52.309203: step 935, loss 0.165304, acc 0.921875
2016-11-12T19:07:52.370391: step 936, loss 0.151187, acc 0.96875
2016-11-12T19:07:52.427094: step 937, loss 0.157208, acc 0.953125
2016-11-12T19:07:52.483346: step 938, loss 0.13568, acc 0.953125
2016-11-12T19:07:52.540866: step 939, loss 0.166364, acc 0.921875
2016-11-12T19:07:52.597386: step 940, loss 0.210685, acc 0.890625
2016-11-12T19:07:52.651272: step 941, loss 0.290783, acc 0.90625
2016-11-12T19:07:52.706182: step 942, loss 0.149786, acc 0.9375
2016-11-12T19:07:52.764839: step 943, loss 0.258834, acc 0.890625
2016-11-12T19:07:52.820981: step 944, loss 0.216413, acc 0.921875
2016-11-12T19:07:52.876487: step 945, loss 0.125379, acc 0.96875
2016-11-12T19:07:52.932992: step 946, loss 0.17925, acc 0.9375
2016-11-12T19:07:52.988747: step 947, loss 0.205906, acc 0.90625
2016-11-12T19:07:53.045289: step 948, loss 0.175893, acc 0.96875
2016-11-12T19:07:53.101368: step 949, loss 0.168942, acc 0.953125
2016-11-12T19:07:53.157174: step 950, loss 0.276314, acc 0.890625
2016-11-12T19:07:53.216011: step 951, loss 0.204004, acc 0.90625
2016-11-12T19:07:53.272379: step 952, loss 0.205856, acc 0.921875
2016-11-12T19:07:53.327914: step 953, loss 0.329743, acc 0.859375
2016-11-12T19:07:53.383567: step 954, loss 0.201527, acc 0.890625
2016-11-12T19:07:53.438937: step 955, loss 0.21, acc 0.90625
2016-11-12T19:07:53.494482: step 956, loss 0.161506, acc 0.953125
2016-11-12T19:07:53.552998: step 957, loss 0.158271, acc 0.9375
2016-11-12T19:07:53.609935: step 958, loss 0.24049, acc 0.90625
2016-11-12T19:07:53.665351: step 959, loss 0.165247, acc 0.9375
2016-11-12T19:07:53.720513: step 960, loss 0.149765, acc 0.96875
2016-11-12T19:07:53.776193: step 961, loss 0.194215, acc 0.9375
2016-11-12T19:07:53.832144: step 962, loss 0.161946, acc 0.9375
2016-11-12T19:07:53.889371: step 963, loss 0.234839, acc 0.890625
2016-11-12T19:07:53.946059: step 964, loss 0.189424, acc 0.9375
2016-11-12T19:07:54.003873: step 965, loss 0.229474, acc 0.9375
2016-11-12T19:07:54.060504: step 966, loss 0.245959, acc 0.9375
2016-11-12T19:07:54.116669: step 967, loss 0.312725, acc 0.90625
2016-11-12T19:07:54.172528: step 968, loss 0.150731, acc 0.96875
2016-11-12T19:07:54.228132: step 969, loss 0.154912, acc 0.953125
2016-11-12T19:07:54.284521: step 970, loss 0.176303, acc 0.9375
2016-11-12T19:07:54.342399: step 971, loss 0.214741, acc 0.90625
2016-11-12T19:07:54.400167: step 972, loss 0.222229, acc 0.921875
2016-11-12T19:07:54.455624: step 973, loss 0.164419, acc 0.953125
2016-11-12T19:07:54.511380: step 974, loss 0.159692, acc 0.9375
2016-11-12T19:07:54.570078: step 975, loss 0.19363, acc 0.875
2016-11-12T19:07:54.629422: step 976, loss 0.174246, acc 0.953125
2016-11-12T19:07:54.685639: step 977, loss 0.154271, acc 0.953125
2016-11-12T19:07:54.742571: step 978, loss 0.181327, acc 0.9375
2016-11-12T19:07:54.799400: step 979, loss 0.251999, acc 0.9375
2016-11-12T19:07:54.855393: step 980, loss 0.242804, acc 0.890625
2016-11-12T19:07:54.912863: step 981, loss 0.157132, acc 0.96875
2016-11-12T19:07:54.968230: step 982, loss 0.193552, acc 0.90625
2016-11-12T19:07:55.025793: step 983, loss 0.265784, acc 0.921875
2016-11-12T19:07:55.081675: step 984, loss 0.197943, acc 0.90625
2016-11-12T19:07:55.136621: step 985, loss 0.129185, acc 0.96875
2016-11-12T19:07:55.193115: step 986, loss 0.143912, acc 0.96875
2016-11-12T19:07:55.249808: step 987, loss 0.203652, acc 0.9375
2016-11-12T19:07:55.305846: step 988, loss 0.193791, acc 0.953125
2016-11-12T19:07:55.361038: step 989, loss 0.24437, acc 0.90625
2016-11-12T19:07:55.419592: step 990, loss 0.156577, acc 0.9375
2016-11-12T19:07:55.475520: step 991, loss 0.244542, acc 0.890625
2016-11-12T19:07:55.532264: step 992, loss 0.142985, acc 0.96875
2016-11-12T19:07:55.589215: step 993, loss 0.153476, acc 0.90625
2016-11-12T19:07:55.625423: step 994, loss 0.169884, acc 0.9
2016-11-12T19:07:55.680970: step 995, loss 0.142629, acc 0.984375
2016-11-12T19:07:55.735828: step 996, loss 0.201113, acc 0.953125
2016-11-12T19:07:55.793149: step 997, loss 0.195055, acc 0.921875
2016-11-12T19:07:55.850564: step 998, loss 0.175732, acc 0.921875
2016-11-12T19:07:55.905940: step 999, loss 0.146538, acc 0.953125
2016-11-12T19:07:55.965276: step 1000, loss 0.107059, acc 0.953125

Evaluation:
2016-11-12T19:07:56.035196: step 1000, loss 0.899101, acc 0.542

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-1000

2016-11-12T19:07:56.532752: step 1001, loss 0.308999, acc 0.859375
2016-11-12T19:07:56.589186: step 1002, loss 0.197762, acc 0.921875
2016-11-12T19:07:56.646680: step 1003, loss 0.207962, acc 0.921875
2016-11-12T19:07:56.704554: step 1004, loss 0.142043, acc 0.953125
2016-11-12T19:07:56.760253: step 1005, loss 0.293431, acc 0.859375
2016-11-12T19:07:56.817068: step 1006, loss 0.166534, acc 0.9375
2016-11-12T19:07:56.873240: step 1007, loss 0.230648, acc 0.890625
2016-11-12T19:07:56.929425: step 1008, loss 0.15608, acc 0.9375
2016-11-12T19:07:56.985781: step 1009, loss 0.15171, acc 0.9375
2016-11-12T19:07:57.041303: step 1010, loss 0.134394, acc 0.953125
2016-11-12T19:07:57.097018: step 1011, loss 0.15841, acc 0.9375
2016-11-12T19:07:57.155319: step 1012, loss 0.131093, acc 0.953125
2016-11-12T19:07:57.210705: step 1013, loss 0.1611, acc 0.90625
2016-11-12T19:07:57.269225: step 1014, loss 0.202306, acc 0.9375
2016-11-12T19:07:57.327453: step 1015, loss 0.189747, acc 0.921875
2016-11-12T19:07:57.383997: step 1016, loss 0.176082, acc 0.921875
2016-11-12T19:07:57.439494: step 1017, loss 0.170279, acc 0.9375
2016-11-12T19:07:57.496795: step 1018, loss 0.17961, acc 0.921875
2016-11-12T19:07:57.552436: step 1019, loss 0.0911654, acc 0.96875
2016-11-12T19:07:57.609364: step 1020, loss 0.168087, acc 0.921875
2016-11-12T19:07:57.664580: step 1021, loss 0.294665, acc 0.84375
2016-11-12T19:07:57.719581: step 1022, loss 0.181055, acc 0.90625
2016-11-12T19:07:57.774700: step 1023, loss 0.17161, acc 0.953125
2016-11-12T19:07:57.832832: step 1024, loss 0.135376, acc 0.953125
2016-11-12T19:07:57.889693: step 1025, loss 0.138432, acc 0.984375
2016-11-12T19:07:57.944607: step 1026, loss 0.209095, acc 0.921875
2016-11-12T19:07:58.001656: step 1027, loss 0.23751, acc 0.890625
2016-11-12T19:07:58.056579: step 1028, loss 0.148077, acc 0.9375
2016-11-12T19:07:58.111861: step 1029, loss 0.341443, acc 0.84375
2016-11-12T19:07:58.167634: step 1030, loss 0.126456, acc 0.9375
2016-11-12T19:07:58.223896: step 1031, loss 0.172743, acc 0.953125
2016-11-12T19:07:58.281517: step 1032, loss 0.354804, acc 0.890625
2016-11-12T19:07:58.337176: step 1033, loss 0.236652, acc 0.90625
2016-11-12T19:07:58.393822: step 1034, loss 0.227282, acc 0.921875
2016-11-12T19:07:58.449724: step 1035, loss 0.19837, acc 0.90625
2016-11-12T19:07:58.506349: step 1036, loss 0.225506, acc 0.90625
2016-11-12T19:07:58.564602: step 1037, loss 0.156555, acc 0.9375
2016-11-12T19:07:58.621498: step 1038, loss 0.261608, acc 0.859375
2016-11-12T19:07:58.678559: step 1039, loss 0.243545, acc 0.875
2016-11-12T19:07:58.736900: step 1040, loss 0.201634, acc 0.921875
2016-11-12T19:07:58.793269: step 1041, loss 0.131647, acc 0.96875
2016-11-12T19:07:58.849465: step 1042, loss 0.207124, acc 0.90625
2016-11-12T19:07:58.906023: step 1043, loss 0.160327, acc 0.9375
2016-11-12T19:07:58.964049: step 1044, loss 0.189597, acc 0.96875
2016-11-12T19:07:59.021332: step 1045, loss 0.24669, acc 0.875
2016-11-12T19:07:59.080914: step 1046, loss 0.18152, acc 0.90625
2016-11-12T19:07:59.136358: step 1047, loss 0.268602, acc 0.921875
2016-11-12T19:07:59.193107: step 1048, loss 0.249409, acc 0.90625
2016-11-12T19:07:59.249477: step 1049, loss 0.162696, acc 0.9375
2016-11-12T19:07:59.305327: step 1050, loss 0.134393, acc 0.953125
2016-11-12T19:07:59.363013: step 1051, loss 0.193334, acc 0.921875
2016-11-12T19:07:59.419390: step 1052, loss 0.263802, acc 0.875
2016-11-12T19:07:59.476997: step 1053, loss 0.207946, acc 0.921875
2016-11-12T19:07:59.534655: step 1054, loss 0.160203, acc 0.953125
2016-11-12T19:07:59.591933: step 1055, loss 0.181169, acc 0.921875
2016-11-12T19:07:59.648029: step 1056, loss 0.109956, acc 0.984375
2016-11-12T19:07:59.704415: step 1057, loss 0.148405, acc 0.9375
2016-11-12T19:07:59.760541: step 1058, loss 0.245544, acc 0.90625
2016-11-12T19:07:59.817360: step 1059, loss 0.130847, acc 0.9375
2016-11-12T19:07:59.873338: step 1060, loss 0.240782, acc 0.9375
2016-11-12T19:07:59.928808: step 1061, loss 0.16092, acc 0.9375
2016-11-12T19:07:59.985396: step 1062, loss 0.270919, acc 0.859375
2016-11-12T19:08:00.041259: step 1063, loss 0.167995, acc 0.921875
2016-11-12T19:08:00.099841: step 1064, loss 0.187041, acc 0.921875
2016-11-12T19:08:00.136496: step 1065, loss 0.211309, acc 0.9
2016-11-12T19:08:00.193561: step 1066, loss 0.233646, acc 0.921875
2016-11-12T19:08:00.250297: step 1067, loss 0.232986, acc 0.921875
2016-11-12T19:08:00.306322: step 1068, loss 0.156816, acc 0.953125
2016-11-12T19:08:00.365378: step 1069, loss 0.170951, acc 0.9375
2016-11-12T19:08:00.420247: step 1070, loss 0.216183, acc 0.890625
2016-11-12T19:08:00.477418: step 1071, loss 0.189112, acc 0.921875
2016-11-12T19:08:00.535076: step 1072, loss 0.123712, acc 0.96875
2016-11-12T19:08:00.590730: step 1073, loss 0.270952, acc 0.890625
2016-11-12T19:08:00.649072: step 1074, loss 0.116803, acc 0.96875
2016-11-12T19:08:00.704388: step 1075, loss 0.153169, acc 0.953125
2016-11-12T19:08:00.761097: step 1076, loss 0.142496, acc 0.953125
2016-11-12T19:08:00.816437: step 1077, loss 0.163394, acc 0.953125
2016-11-12T19:08:00.871306: step 1078, loss 0.190171, acc 0.9375
2016-11-12T19:08:00.926278: step 1079, loss 0.153264, acc 0.9375
2016-11-12T19:08:00.981831: step 1080, loss 0.185476, acc 0.9375
2016-11-12T19:08:01.037411: step 1081, loss 0.234625, acc 0.921875
2016-11-12T19:08:01.093679: step 1082, loss 0.12571, acc 0.953125
2016-11-12T19:08:01.148214: step 1083, loss 0.15334, acc 0.9375
2016-11-12T19:08:01.207807: step 1084, loss 0.0864342, acc 0.984375
2016-11-12T19:08:01.265452: step 1085, loss 0.148499, acc 0.9375
2016-11-12T19:08:01.320539: step 1086, loss 0.154193, acc 0.953125
2016-11-12T19:08:01.375963: step 1087, loss 0.172933, acc 0.9375
2016-11-12T19:08:01.433154: step 1088, loss 0.211077, acc 0.890625
2016-11-12T19:08:01.489952: step 1089, loss 0.130449, acc 0.9375
2016-11-12T19:08:01.548704: step 1090, loss 0.222077, acc 0.921875
2016-11-12T19:08:01.605352: step 1091, loss 0.126361, acc 0.96875
2016-11-12T19:08:01.661288: step 1092, loss 0.189189, acc 0.90625
2016-11-12T19:08:01.717196: step 1093, loss 0.188446, acc 0.921875
2016-11-12T19:08:01.772527: step 1094, loss 0.193647, acc 0.9375
2016-11-12T19:08:01.829472: step 1095, loss 0.134431, acc 0.984375
2016-11-12T19:08:01.887727: step 1096, loss 0.213194, acc 0.921875
2016-11-12T19:08:01.942780: step 1097, loss 0.261626, acc 0.890625
2016-11-12T19:08:01.997796: step 1098, loss 0.138863, acc 0.953125
2016-11-12T19:08:02.056222: step 1099, loss 0.0797036, acc 0.984375
2016-11-12T19:08:02.113855: step 1100, loss 0.168385, acc 0.9375

Evaluation:
2016-11-12T19:08:02.183307: step 1100, loss 0.980127, acc 0.532

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-1100

2016-11-12T19:08:02.678770: step 1101, loss 0.168066, acc 0.953125
2016-11-12T19:08:02.735869: step 1102, loss 0.141864, acc 0.9375
2016-11-12T19:08:02.793367: step 1103, loss 0.101744, acc 0.96875
2016-11-12T19:08:02.848601: step 1104, loss 0.16567, acc 0.921875
2016-11-12T19:08:02.904682: step 1105, loss 0.0945292, acc 0.984375
2016-11-12T19:08:02.960265: step 1106, loss 0.140797, acc 0.9375
2016-11-12T19:08:03.016414: step 1107, loss 0.0566069, acc 1
2016-11-12T19:08:03.072682: step 1108, loss 0.206138, acc 0.9375
2016-11-12T19:08:03.128344: step 1109, loss 0.0888105, acc 0.984375
2016-11-12T19:08:03.185403: step 1110, loss 0.184301, acc 0.9375
2016-11-12T19:08:03.241804: step 1111, loss 0.158237, acc 0.90625
2016-11-12T19:08:03.297306: step 1112, loss 0.148708, acc 0.953125
2016-11-12T19:08:03.353522: step 1113, loss 0.14086, acc 0.96875
2016-11-12T19:08:03.409171: step 1114, loss 0.156928, acc 0.921875
2016-11-12T19:08:03.467697: step 1115, loss 0.0961093, acc 0.953125
2016-11-12T19:08:03.522907: step 1116, loss 0.10524, acc 0.96875
2016-11-12T19:08:03.579607: step 1117, loss 0.0986023, acc 0.96875
2016-11-12T19:08:03.637865: step 1118, loss 0.177889, acc 0.90625
2016-11-12T19:08:03.696671: step 1119, loss 0.169186, acc 0.9375
2016-11-12T19:08:03.752406: step 1120, loss 0.267347, acc 0.875
2016-11-12T19:08:03.810103: step 1121, loss 0.188043, acc 0.90625
2016-11-12T19:08:03.866812: step 1122, loss 0.119099, acc 0.96875
2016-11-12T19:08:03.922609: step 1123, loss 0.206938, acc 0.90625
2016-11-12T19:08:03.977967: step 1124, loss 0.123485, acc 0.9375
2016-11-12T19:08:04.035696: step 1125, loss 0.141883, acc 0.953125
2016-11-12T19:08:04.094114: step 1126, loss 0.149371, acc 0.921875
2016-11-12T19:08:04.150031: step 1127, loss 0.187601, acc 0.921875
2016-11-12T19:08:04.205684: step 1128, loss 0.182675, acc 0.921875
2016-11-12T19:08:04.261338: step 1129, loss 0.123128, acc 0.96875
2016-11-12T19:08:04.316708: step 1130, loss 0.128045, acc 0.96875
2016-11-12T19:08:04.372701: step 1131, loss 0.198133, acc 0.921875
2016-11-12T19:08:04.427664: step 1132, loss 0.241265, acc 0.921875
2016-11-12T19:08:04.483011: step 1133, loss 0.130999, acc 0.96875
2016-11-12T19:08:04.541214: step 1134, loss 0.156972, acc 0.921875
2016-11-12T19:08:04.597159: step 1135, loss 0.155921, acc 0.953125
2016-11-12T19:08:04.636765: step 1136, loss 0.197722, acc 0.95
2016-11-12T19:08:04.693354: step 1137, loss 0.130982, acc 0.953125
2016-11-12T19:08:04.752543: step 1138, loss 0.177699, acc 0.921875
2016-11-12T19:08:04.809705: step 1139, loss 0.167962, acc 0.9375
2016-11-12T19:08:04.867906: step 1140, loss 0.105814, acc 0.953125
2016-11-12T19:08:04.923731: step 1141, loss 0.112936, acc 0.96875
2016-11-12T19:08:04.982175: step 1142, loss 0.142107, acc 0.953125
2016-11-12T19:08:05.038236: step 1143, loss 0.105394, acc 0.96875
2016-11-12T19:08:05.094147: step 1144, loss 0.096274, acc 0.96875
2016-11-12T19:08:05.149918: step 1145, loss 0.146742, acc 0.9375
2016-11-12T19:08:05.207490: step 1146, loss 0.166033, acc 0.96875
2016-11-12T19:08:05.262966: step 1147, loss 0.145571, acc 0.9375
2016-11-12T19:08:05.318798: step 1148, loss 0.143847, acc 0.953125
2016-11-12T19:08:05.374115: step 1149, loss 0.149216, acc 0.953125
2016-11-12T19:08:05.429584: step 1150, loss 0.184553, acc 0.921875
2016-11-12T19:08:05.485362: step 1151, loss 0.117432, acc 0.953125
2016-11-12T19:08:05.541445: step 1152, loss 0.115192, acc 0.96875
2016-11-12T19:08:05.602787: step 1153, loss 0.150119, acc 0.953125
2016-11-12T19:08:05.659543: step 1154, loss 0.285944, acc 0.890625
2016-11-12T19:08:05.716981: step 1155, loss 0.142637, acc 0.921875
2016-11-12T19:08:05.773443: step 1156, loss 0.13588, acc 0.953125
2016-11-12T19:08:05.829532: step 1157, loss 0.118381, acc 0.984375
2016-11-12T19:08:05.885381: step 1158, loss 0.147663, acc 0.9375
2016-11-12T19:08:05.942074: step 1159, loss 0.108605, acc 0.96875
2016-11-12T19:08:05.997368: step 1160, loss 0.137953, acc 0.953125
2016-11-12T19:08:06.053750: step 1161, loss 0.0834772, acc 0.984375
2016-11-12T19:08:06.110858: step 1162, loss 0.106694, acc 0.953125
2016-11-12T19:08:06.166130: step 1163, loss 0.174631, acc 0.9375
2016-11-12T19:08:06.221924: step 1164, loss 0.128016, acc 0.953125
2016-11-12T19:08:06.277168: step 1165, loss 0.204442, acc 0.953125
2016-11-12T19:08:06.333609: step 1166, loss 0.118216, acc 0.9375
2016-11-12T19:08:06.388610: step 1167, loss 0.111433, acc 0.953125
2016-11-12T19:08:06.445248: step 1168, loss 0.124651, acc 0.96875
2016-11-12T19:08:06.499560: step 1169, loss 0.0920817, acc 0.984375
2016-11-12T19:08:06.556784: step 1170, loss 0.127966, acc 0.921875
2016-11-12T19:08:06.612231: step 1171, loss 0.141788, acc 0.9375
2016-11-12T19:08:06.669597: step 1172, loss 0.0935052, acc 0.984375
2016-11-12T19:08:06.726345: step 1173, loss 0.106999, acc 0.9375
2016-11-12T19:08:06.783891: step 1174, loss 0.175725, acc 0.9375
2016-11-12T19:08:06.839951: step 1175, loss 0.149824, acc 0.953125
2016-11-12T19:08:06.896882: step 1176, loss 0.117255, acc 0.96875
2016-11-12T19:08:06.953312: step 1177, loss 0.150977, acc 0.9375
2016-11-12T19:08:07.009285: step 1178, loss 0.0739188, acc 0.984375
2016-11-12T19:08:07.065432: step 1179, loss 0.138892, acc 0.953125
2016-11-12T19:08:07.121836: step 1180, loss 0.156888, acc 0.9375
2016-11-12T19:08:07.180027: step 1181, loss 0.110458, acc 0.9375
2016-11-12T19:08:07.236597: step 1182, loss 0.20001, acc 0.90625
2016-11-12T19:08:07.292715: step 1183, loss 0.186379, acc 0.890625
2016-11-12T19:08:07.348977: step 1184, loss 0.123009, acc 0.953125
2016-11-12T19:08:07.404441: step 1185, loss 0.10907, acc 0.984375
2016-11-12T19:08:07.461388: step 1186, loss 0.292811, acc 0.9375
2016-11-12T19:08:07.518219: step 1187, loss 0.0763598, acc 1
2016-11-12T19:08:07.575577: step 1188, loss 0.158208, acc 0.953125
2016-11-12T19:08:07.630284: step 1189, loss 0.140171, acc 0.9375
2016-11-12T19:08:07.685546: step 1190, loss 0.18133, acc 0.953125
2016-11-12T19:08:07.741834: step 1191, loss 0.167826, acc 0.9375
2016-11-12T19:08:07.797487: step 1192, loss 0.118203, acc 0.953125
2016-11-12T19:08:07.853730: step 1193, loss 0.118957, acc 0.9375
2016-11-12T19:08:07.912615: step 1194, loss 0.0679033, acc 0.984375
2016-11-12T19:08:07.969847: step 1195, loss 0.12143, acc 0.953125
2016-11-12T19:08:08.025993: step 1196, loss 0.164199, acc 0.953125
2016-11-12T19:08:08.082079: step 1197, loss 0.091585, acc 0.96875
2016-11-12T19:08:08.139669: step 1198, loss 0.106233, acc 0.96875
2016-11-12T19:08:08.196578: step 1199, loss 0.133078, acc 0.953125
2016-11-12T19:08:08.255541: step 1200, loss 0.0866515, acc 0.984375

Evaluation:
2016-11-12T19:08:08.325923: step 1200, loss 1.00483, acc 0.538

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-1200

2016-11-12T19:08:08.823760: step 1201, loss 0.235409, acc 0.953125
2016-11-12T19:08:08.879924: step 1202, loss 0.176281, acc 0.953125
2016-11-12T19:08:08.936789: step 1203, loss 0.0677384, acc 0.984375
2016-11-12T19:08:08.993889: step 1204, loss 0.13899, acc 0.96875
2016-11-12T19:08:09.048521: step 1205, loss 0.112549, acc 0.953125
2016-11-12T19:08:09.104134: step 1206, loss 0.161153, acc 0.9375
2016-11-12T19:08:09.140758: step 1207, loss 0.0382373, acc 1
2016-11-12T19:08:09.199153: step 1208, loss 0.131193, acc 0.984375
2016-11-12T19:08:09.256540: step 1209, loss 0.14984, acc 0.90625
2016-11-12T19:08:09.311897: step 1210, loss 0.118733, acc 0.96875
2016-11-12T19:08:09.367406: step 1211, loss 0.176581, acc 0.90625
2016-11-12T19:08:09.424581: step 1212, loss 0.101294, acc 0.984375
2016-11-12T19:08:09.480092: step 1213, loss 0.138815, acc 0.9375
2016-11-12T19:08:09.535569: step 1214, loss 0.0824283, acc 0.96875
2016-11-12T19:08:09.595362: step 1215, loss 0.106179, acc 0.953125
2016-11-12T19:08:09.652619: step 1216, loss 0.0673452, acc 0.984375
2016-11-12T19:08:09.708394: step 1217, loss 0.18558, acc 0.90625
2016-11-12T19:08:09.763639: step 1218, loss 0.107439, acc 0.96875
2016-11-12T19:08:09.820993: step 1219, loss 0.100578, acc 0.96875
2016-11-12T19:08:09.876259: step 1220, loss 0.0619983, acc 1
2016-11-12T19:08:09.932112: step 1221, loss 0.0794985, acc 0.96875
2016-11-12T19:08:09.989714: step 1222, loss 0.091993, acc 0.984375
2016-11-12T19:08:10.048328: step 1223, loss 0.113439, acc 0.96875
2016-11-12T19:08:10.103237: step 1224, loss 0.0894519, acc 0.96875
2016-11-12T19:08:10.159281: step 1225, loss 0.146532, acc 0.953125
2016-11-12T19:08:10.217621: step 1226, loss 0.12915, acc 0.953125
2016-11-12T19:08:10.277443: step 1227, loss 0.12545, acc 0.96875
2016-11-12T19:08:10.333231: step 1228, loss 0.173668, acc 0.921875
2016-11-12T19:08:10.389356: step 1229, loss 0.148128, acc 0.953125
2016-11-12T19:08:10.444215: step 1230, loss 0.215785, acc 0.9375
2016-11-12T19:08:10.501883: step 1231, loss 0.113049, acc 0.984375
2016-11-12T19:08:10.560175: step 1232, loss 0.123555, acc 0.96875
2016-11-12T19:08:10.615636: step 1233, loss 0.164729, acc 0.9375
2016-11-12T19:08:10.671635: step 1234, loss 0.128756, acc 0.953125
2016-11-12T19:08:10.728459: step 1235, loss 0.147877, acc 0.953125
2016-11-12T19:08:10.784508: step 1236, loss 0.0757776, acc 0.984375
2016-11-12T19:08:10.841300: step 1237, loss 0.106713, acc 0.96875
2016-11-12T19:08:10.897738: step 1238, loss 0.124982, acc 0.96875
2016-11-12T19:08:10.953162: step 1239, loss 0.0792936, acc 1
2016-11-12T19:08:11.009420: step 1240, loss 0.104748, acc 0.96875
2016-11-12T19:08:11.065497: step 1241, loss 0.146315, acc 0.9375
2016-11-12T19:08:11.120799: step 1242, loss 0.162438, acc 0.921875
2016-11-12T19:08:11.177256: step 1243, loss 0.0878297, acc 0.984375
2016-11-12T19:08:11.237758: step 1244, loss 0.161935, acc 0.953125
2016-11-12T19:08:11.293303: step 1245, loss 0.177146, acc 0.921875
2016-11-12T19:08:11.350694: step 1246, loss 0.145344, acc 0.9375
2016-11-12T19:08:11.406124: step 1247, loss 0.122566, acc 0.953125
2016-11-12T19:08:11.460841: step 1248, loss 0.135943, acc 0.96875
2016-11-12T19:08:11.516030: step 1249, loss 0.143789, acc 0.90625
2016-11-12T19:08:11.572619: step 1250, loss 0.189719, acc 0.921875
2016-11-12T19:08:11.629913: step 1251, loss 0.051134, acc 1
2016-11-12T19:08:11.686739: step 1252, loss 0.0603952, acc 0.984375
2016-11-12T19:08:11.744144: step 1253, loss 0.075253, acc 0.984375
2016-11-12T19:08:11.803682: step 1254, loss 0.126678, acc 0.953125
2016-11-12T19:08:11.859054: step 1255, loss 0.155024, acc 0.9375
2016-11-12T19:08:11.916841: step 1256, loss 0.10944, acc 0.953125
2016-11-12T19:08:11.973951: step 1257, loss 0.188577, acc 0.953125
2016-11-12T19:08:12.030255: step 1258, loss 0.101336, acc 0.984375
2016-11-12T19:08:12.085423: step 1259, loss 0.140494, acc 0.953125
2016-11-12T19:08:12.141614: step 1260, loss 0.0872432, acc 0.96875
2016-11-12T19:08:12.201518: step 1261, loss 0.154596, acc 0.9375
2016-11-12T19:08:12.256932: step 1262, loss 0.0551002, acc 1
2016-11-12T19:08:12.313488: step 1263, loss 0.133896, acc 0.9375
2016-11-12T19:08:12.368911: step 1264, loss 0.12498, acc 0.96875
2016-11-12T19:08:12.424612: step 1265, loss 0.158266, acc 0.9375
2016-11-12T19:08:12.481608: step 1266, loss 0.187597, acc 0.953125
2016-11-12T19:08:12.538097: step 1267, loss 0.184969, acc 0.921875
2016-11-12T19:08:12.593687: step 1268, loss 0.183082, acc 0.90625
2016-11-12T19:08:12.650728: step 1269, loss 0.109258, acc 0.953125
2016-11-12T19:08:12.708153: step 1270, loss 0.177414, acc 0.921875
2016-11-12T19:08:12.768263: step 1271, loss 0.162098, acc 0.921875
2016-11-12T19:08:12.825337: step 1272, loss 0.0781015, acc 1
2016-11-12T19:08:12.880634: step 1273, loss 0.121037, acc 0.9375
2016-11-12T19:08:12.936933: step 1274, loss 0.0928166, acc 0.953125
2016-11-12T19:08:12.994789: step 1275, loss 0.0647436, acc 1
2016-11-12T19:08:13.051387: step 1276, loss 0.107535, acc 0.96875
2016-11-12T19:08:13.106907: step 1277, loss 0.12481, acc 0.90625
2016-11-12T19:08:13.144620: step 1278, loss 0.139438, acc 0.95
2016-11-12T19:08:13.202394: step 1279, loss 0.117594, acc 0.9375
2016-11-12T19:08:13.257731: step 1280, loss 0.154686, acc 0.953125
2016-11-12T19:08:13.313702: step 1281, loss 0.121023, acc 0.9375
2016-11-12T19:08:13.372948: step 1282, loss 0.148217, acc 0.953125
2016-11-12T19:08:13.429306: step 1283, loss 0.066083, acc 0.96875
2016-11-12T19:08:13.489621: step 1284, loss 0.115404, acc 0.9375
2016-11-12T19:08:13.546042: step 1285, loss 0.0800425, acc 0.984375
2016-11-12T19:08:13.602438: step 1286, loss 0.147367, acc 0.9375
2016-11-12T19:08:13.659159: step 1287, loss 0.0611645, acc 0.96875
2016-11-12T19:08:13.716855: step 1288, loss 0.128944, acc 0.9375
2016-11-12T19:08:13.773339: step 1289, loss 0.188718, acc 0.875
2016-11-12T19:08:13.830210: step 1290, loss 0.125464, acc 0.96875
2016-11-12T19:08:13.889566: step 1291, loss 0.110142, acc 0.96875
2016-11-12T19:08:13.945569: step 1292, loss 0.120515, acc 0.953125
2016-11-12T19:08:14.000626: step 1293, loss 0.139006, acc 0.953125
2016-11-12T19:08:14.056189: step 1294, loss 0.0731351, acc 0.953125
2016-11-12T19:08:14.113492: step 1295, loss 0.082438, acc 0.96875
2016-11-12T19:08:14.168957: step 1296, loss 0.118705, acc 0.921875
2016-11-12T19:08:14.224913: step 1297, loss 0.1144, acc 0.9375
2016-11-12T19:08:14.282460: step 1298, loss 0.0667885, acc 0.96875
2016-11-12T19:08:14.339849: step 1299, loss 0.110807, acc 0.96875
2016-11-12T19:08:14.395279: step 1300, loss 0.178426, acc 0.921875

Evaluation:
2016-11-12T19:08:14.466803: step 1300, loss 1.05798, acc 0.54

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-1300

2016-11-12T19:08:14.963315: step 1301, loss 0.0912051, acc 0.96875
2016-11-12T19:08:15.021518: step 1302, loss 0.06994, acc 0.96875
2016-11-12T19:08:15.078271: step 1303, loss 0.0794842, acc 0.984375
2016-11-12T19:08:15.136466: step 1304, loss 0.109502, acc 0.96875
2016-11-12T19:08:15.193092: step 1305, loss 0.0759431, acc 0.984375
2016-11-12T19:08:15.250021: step 1306, loss 0.0725845, acc 0.984375
2016-11-12T19:08:15.308224: step 1307, loss 0.116154, acc 0.953125
2016-11-12T19:08:15.365558: step 1308, loss 0.0755769, acc 0.984375
2016-11-12T19:08:15.423568: step 1309, loss 0.0909598, acc 0.953125
2016-11-12T19:08:15.480557: step 1310, loss 0.0951416, acc 0.96875
2016-11-12T19:08:15.538166: step 1311, loss 0.120111, acc 0.9375
2016-11-12T19:08:15.594212: step 1312, loss 0.0975198, acc 0.984375
2016-11-12T19:08:15.652325: step 1313, loss 0.12841, acc 0.953125
2016-11-12T19:08:15.709403: step 1314, loss 0.133497, acc 0.9375
2016-11-12T19:08:15.765139: step 1315, loss 0.0913782, acc 0.984375
2016-11-12T19:08:15.820839: step 1316, loss 0.106633, acc 0.96875
2016-11-12T19:08:15.881016: step 1317, loss 0.106355, acc 0.96875
2016-11-12T19:08:15.937724: step 1318, loss 0.110626, acc 0.9375
2016-11-12T19:08:15.995973: step 1319, loss 0.165763, acc 0.953125
2016-11-12T19:08:16.053199: step 1320, loss 0.107246, acc 0.9375
2016-11-12T19:08:16.108775: step 1321, loss 0.0778986, acc 0.96875
2016-11-12T19:08:16.164017: step 1322, loss 0.112524, acc 0.9375
2016-11-12T19:08:16.219034: step 1323, loss 0.0437493, acc 1
2016-11-12T19:08:16.274825: step 1324, loss 0.136481, acc 0.9375
2016-11-12T19:08:16.333035: step 1325, loss 0.0739494, acc 0.984375
2016-11-12T19:08:16.392579: step 1326, loss 0.0660727, acc 0.96875
2016-11-12T19:08:16.448841: step 1327, loss 0.139057, acc 0.9375
2016-11-12T19:08:16.507695: step 1328, loss 0.116072, acc 0.96875
2016-11-12T19:08:16.562731: step 1329, loss 0.153913, acc 0.921875
2016-11-12T19:08:16.617974: step 1330, loss 0.13195, acc 0.953125
2016-11-12T19:08:16.674278: step 1331, loss 0.0667821, acc 0.984375
2016-11-12T19:08:16.731897: step 1332, loss 0.212988, acc 0.90625
2016-11-12T19:08:16.787978: step 1333, loss 0.0819273, acc 0.96875
2016-11-12T19:08:16.844851: step 1334, loss 0.105233, acc 0.953125
2016-11-12T19:08:16.899746: step 1335, loss 0.0912083, acc 0.984375
2016-11-12T19:08:16.957369: step 1336, loss 0.126114, acc 0.9375
2016-11-12T19:08:17.013346: step 1337, loss 0.156821, acc 0.90625
2016-11-12T19:08:17.069273: step 1338, loss 0.0961289, acc 0.984375
2016-11-12T19:08:17.125338: step 1339, loss 0.104271, acc 0.96875
2016-11-12T19:08:17.181905: step 1340, loss 0.107578, acc 0.96875
2016-11-12T19:08:17.239209: step 1341, loss 0.144319, acc 0.9375
2016-11-12T19:08:17.294456: step 1342, loss 0.160639, acc 0.921875
2016-11-12T19:08:17.351607: step 1343, loss 0.0977315, acc 0.96875
2016-11-12T19:08:17.408023: step 1344, loss 0.125354, acc 0.9375
2016-11-12T19:08:17.463606: step 1345, loss 0.116653, acc 0.953125
2016-11-12T19:08:17.521569: step 1346, loss 0.0884702, acc 0.96875
2016-11-12T19:08:17.577268: step 1347, loss 0.214805, acc 0.90625
2016-11-12T19:08:17.634629: step 1348, loss 0.145206, acc 0.9375
2016-11-12T19:08:17.672126: step 1349, loss 0.0537517, acc 1
2016-11-12T19:08:17.731557: step 1350, loss 0.175146, acc 0.9375
2016-11-12T19:08:17.789572: step 1351, loss 0.0760948, acc 0.984375
2016-11-12T19:08:17.845715: step 1352, loss 0.075195, acc 0.96875
2016-11-12T19:08:17.901481: step 1353, loss 0.0289057, acc 0.984375
2016-11-12T19:08:17.958880: step 1354, loss 0.11756, acc 0.96875
2016-11-12T19:08:18.014433: step 1355, loss 0.103111, acc 0.953125
2016-11-12T19:08:18.068907: step 1356, loss 0.12321, acc 0.96875
2016-11-12T19:08:18.126807: step 1357, loss 0.123793, acc 0.953125
2016-11-12T19:08:18.184454: step 1358, loss 0.0838909, acc 1
2016-11-12T19:08:18.239902: step 1359, loss 0.103064, acc 0.96875
2016-11-12T19:08:18.297675: step 1360, loss 0.0851762, acc 0.96875
2016-11-12T19:08:18.356356: step 1361, loss 0.148615, acc 0.96875
2016-11-12T19:08:18.411982: step 1362, loss 0.172408, acc 0.96875
2016-11-12T19:08:18.467804: step 1363, loss 0.0724701, acc 1
2016-11-12T19:08:18.524286: step 1364, loss 0.0467936, acc 1
2016-11-12T19:08:18.581101: step 1365, loss 0.135236, acc 0.953125
2016-11-12T19:08:18.636204: step 1366, loss 0.111336, acc 0.953125
2016-11-12T19:08:18.691545: step 1367, loss 0.0729579, acc 0.96875
2016-11-12T19:08:18.748996: step 1368, loss 0.226336, acc 0.9375
2016-11-12T19:08:18.806344: step 1369, loss 0.123441, acc 0.953125
2016-11-12T19:08:18.861405: step 1370, loss 0.0629174, acc 0.984375
2016-11-12T19:08:18.917440: step 1371, loss 0.190658, acc 0.90625
2016-11-12T19:08:18.971518: step 1372, loss 0.0584294, acc 1
2016-11-12T19:08:19.026683: step 1373, loss 0.105543, acc 0.96875
2016-11-12T19:08:19.084145: step 1374, loss 0.0596727, acc 0.96875
2016-11-12T19:08:19.140903: step 1375, loss 0.0996756, acc 0.953125
2016-11-12T19:08:19.196758: step 1376, loss 0.0579675, acc 0.984375
2016-11-12T19:08:19.252763: step 1377, loss 0.175402, acc 0.953125
2016-11-12T19:08:19.309327: step 1378, loss 0.1864, acc 0.890625
2016-11-12T19:08:19.365821: step 1379, loss 0.066062, acc 0.984375
2016-11-12T19:08:19.425130: step 1380, loss 0.100276, acc 0.953125
2016-11-12T19:08:19.481178: step 1381, loss 0.0569961, acc 0.984375
2016-11-12T19:08:19.537282: step 1382, loss 0.137861, acc 0.96875
2016-11-12T19:08:19.593179: step 1383, loss 0.18105, acc 0.890625
2016-11-12T19:08:19.648428: step 1384, loss 0.0823586, acc 0.984375
2016-11-12T19:08:19.704130: step 1385, loss 0.139293, acc 0.96875
2016-11-12T19:08:19.760357: step 1386, loss 0.0769121, acc 1
2016-11-12T19:08:19.815792: step 1387, loss 0.102228, acc 0.953125
2016-11-12T19:08:19.873967: step 1388, loss 0.127678, acc 0.953125
2016-11-12T19:08:19.930825: step 1389, loss 0.122467, acc 0.953125
2016-11-12T19:08:19.986869: step 1390, loss 0.0926102, acc 0.96875
2016-11-12T19:08:20.044144: step 1391, loss 0.084453, acc 0.953125
2016-11-12T19:08:20.100588: step 1392, loss 0.0687254, acc 0.96875
2016-11-12T19:08:20.157393: step 1393, loss 0.123587, acc 0.9375
2016-11-12T19:08:20.213610: step 1394, loss 0.0727437, acc 0.96875
2016-11-12T19:08:20.269459: step 1395, loss 0.0658534, acc 0.984375
2016-11-12T19:08:20.327287: step 1396, loss 0.0792636, acc 0.984375
2016-11-12T19:08:20.382647: step 1397, loss 0.0883757, acc 0.953125
2016-11-12T19:08:20.438343: step 1398, loss 0.0771721, acc 0.953125
2016-11-12T19:08:20.494489: step 1399, loss 0.0709184, acc 0.984375
2016-11-12T19:08:20.549754: step 1400, loss 0.128131, acc 0.96875

Evaluation:
2016-11-12T19:08:20.620159: step 1400, loss 1.1202, acc 0.544

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-1400

2016-11-12T19:08:21.116529: step 1401, loss 0.123432, acc 0.96875
2016-11-12T19:08:21.173106: step 1402, loss 0.177985, acc 0.921875
2016-11-12T19:08:21.228029: step 1403, loss 0.102653, acc 0.953125
2016-11-12T19:08:21.283552: step 1404, loss 0.106437, acc 0.9375
2016-11-12T19:08:21.339647: step 1405, loss 0.135817, acc 0.96875
2016-11-12T19:08:21.395619: step 1406, loss 0.0712235, acc 0.96875
2016-11-12T19:08:21.451801: step 1407, loss 0.0845546, acc 1
2016-11-12T19:08:21.507114: step 1408, loss 0.0728627, acc 0.984375
2016-11-12T19:08:21.564409: step 1409, loss 0.138546, acc 0.96875
2016-11-12T19:08:21.621882: step 1410, loss 0.0961077, acc 0.96875
2016-11-12T19:08:21.679418: step 1411, loss 0.0653953, acc 0.96875
2016-11-12T19:08:21.737048: step 1412, loss 0.190933, acc 0.9375
2016-11-12T19:08:21.795632: step 1413, loss 0.0548728, acc 0.984375
2016-11-12T19:08:21.852570: step 1414, loss 0.0948062, acc 0.953125
2016-11-12T19:08:21.909289: step 1415, loss 0.0648304, acc 0.96875
2016-11-12T19:08:21.966149: step 1416, loss 0.0847053, acc 0.96875
2016-11-12T19:08:22.023706: step 1417, loss 0.0754663, acc 0.96875
2016-11-12T19:08:22.079030: step 1418, loss 0.103269, acc 0.96875
2016-11-12T19:08:22.135493: step 1419, loss 0.202393, acc 0.921875
2016-11-12T19:08:22.173204: step 1420, loss 0.0894613, acc 0.95
2016-11-12T19:08:22.229625: step 1421, loss 0.076432, acc 0.984375
2016-11-12T19:08:22.285531: step 1422, loss 0.0681722, acc 0.96875
2016-11-12T19:08:22.341655: step 1423, loss 0.111769, acc 0.984375
2016-11-12T19:08:22.397699: step 1424, loss 0.111311, acc 0.96875
2016-11-12T19:08:22.457017: step 1425, loss 0.0570965, acc 0.984375
2016-11-12T19:08:22.515303: step 1426, loss 0.108003, acc 0.96875
2016-11-12T19:08:22.570247: step 1427, loss 0.10712, acc 0.953125
2016-11-12T19:08:22.627356: step 1428, loss 0.0862553, acc 0.953125
2016-11-12T19:08:22.683615: step 1429, loss 0.0705066, acc 0.984375
2016-11-12T19:08:22.739279: step 1430, loss 0.0630985, acc 1
2016-11-12T19:08:22.795087: step 1431, loss 0.0408659, acc 1
2016-11-12T19:08:22.852203: step 1432, loss 0.238038, acc 0.90625
2016-11-12T19:08:22.909491: step 1433, loss 0.113312, acc 0.953125
2016-11-12T19:08:22.970926: step 1434, loss 0.0917334, acc 0.984375
2016-11-12T19:08:23.028046: step 1435, loss 0.0990845, acc 0.953125
2016-11-12T19:08:23.084555: step 1436, loss 0.102615, acc 0.96875
2016-11-12T19:08:23.140235: step 1437, loss 0.0839576, acc 0.96875
2016-11-12T19:08:23.196518: step 1438, loss 0.190261, acc 0.921875
2016-11-12T19:08:23.251462: step 1439, loss 0.0637066, acc 0.984375
2016-11-12T19:08:23.307927: step 1440, loss 0.0855389, acc 0.96875
2016-11-12T19:08:23.364036: step 1441, loss 0.106873, acc 0.96875
2016-11-12T19:08:23.420718: step 1442, loss 0.140031, acc 0.9375
2016-11-12T19:08:23.478659: step 1443, loss 0.0506945, acc 0.984375
2016-11-12T19:08:23.536091: step 1444, loss 0.108436, acc 0.96875
2016-11-12T19:08:23.593810: step 1445, loss 0.0268296, acc 1
2016-11-12T19:08:23.649458: step 1446, loss 0.0755003, acc 0.96875
2016-11-12T19:08:23.705010: step 1447, loss 0.0821911, acc 0.984375
2016-11-12T19:08:23.761036: step 1448, loss 0.106567, acc 0.953125
2016-11-12T19:08:23.816588: step 1449, loss 0.0576367, acc 0.984375
2016-11-12T19:08:23.872473: step 1450, loss 0.0813923, acc 0.96875
2016-11-12T19:08:23.929393: step 1451, loss 0.0883885, acc 0.96875
2016-11-12T19:08:23.985021: step 1452, loss 0.0286911, acc 1
2016-11-12T19:08:24.041099: step 1453, loss 0.0549501, acc 0.984375
2016-11-12T19:08:24.097447: step 1454, loss 0.0489979, acc 0.984375
2016-11-12T19:08:24.153291: step 1455, loss 0.087531, acc 0.96875
2016-11-12T19:08:24.208078: step 1456, loss 0.1008, acc 0.96875
2016-11-12T19:08:24.265381: step 1457, loss 0.0855003, acc 0.96875
2016-11-12T19:08:24.324864: step 1458, loss 0.0925223, acc 0.953125
2016-11-12T19:08:24.382858: step 1459, loss 0.0452703, acc 0.984375
2016-11-12T19:08:24.438669: step 1460, loss 0.114919, acc 0.96875
2016-11-12T19:08:24.495249: step 1461, loss 0.072005, acc 0.96875
2016-11-12T19:08:24.553457: step 1462, loss 0.0515545, acc 0.984375
2016-11-12T19:08:24.608842: step 1463, loss 0.118693, acc 0.984375
2016-11-12T19:08:24.665419: step 1464, loss 0.0550217, acc 0.984375
2016-11-12T19:08:24.721232: step 1465, loss 0.117186, acc 0.953125
2016-11-12T19:08:24.781304: step 1466, loss 0.142096, acc 0.953125
2016-11-12T19:08:24.837363: step 1467, loss 0.0664373, acc 0.96875
2016-11-12T19:08:24.892735: step 1468, loss 0.0814865, acc 0.984375
2016-11-12T19:08:24.948317: step 1469, loss 0.055914, acc 0.984375
2016-11-12T19:08:25.003826: step 1470, loss 0.0885528, acc 0.96875
2016-11-12T19:08:25.061491: step 1471, loss 0.0837296, acc 0.96875
2016-11-12T19:08:25.117477: step 1472, loss 0.107917, acc 0.96875
2016-11-12T19:08:25.174130: step 1473, loss 0.0511891, acc 1
2016-11-12T19:08:25.231433: step 1474, loss 0.0539581, acc 0.984375
2016-11-12T19:08:25.286739: step 1475, loss 0.116595, acc 0.96875
2016-11-12T19:08:25.345442: step 1476, loss 0.0740988, acc 1
2016-11-12T19:08:25.401414: step 1477, loss 0.0923041, acc 0.953125
2016-11-12T19:08:25.456983: step 1478, loss 0.13853, acc 0.953125
2016-11-12T19:08:25.513234: step 1479, loss 0.071194, acc 0.984375
2016-11-12T19:08:25.571111: step 1480, loss 0.0768993, acc 0.984375
2016-11-12T19:08:25.628336: step 1481, loss 0.103702, acc 0.96875
2016-11-12T19:08:25.684805: step 1482, loss 0.123611, acc 0.984375
2016-11-12T19:08:25.740347: step 1483, loss 0.142784, acc 0.921875
2016-11-12T19:08:25.798844: step 1484, loss 0.0440629, acc 0.984375
2016-11-12T19:08:25.855027: step 1485, loss 0.0568565, acc 0.984375
2016-11-12T19:08:25.910234: step 1486, loss 0.0636946, acc 1
2016-11-12T19:08:25.965400: step 1487, loss 0.150945, acc 0.9375
2016-11-12T19:08:26.022190: step 1488, loss 0.0786114, acc 0.984375
2016-11-12T19:08:26.079604: step 1489, loss 0.0496787, acc 1
2016-11-12T19:08:26.136226: step 1490, loss 0.0995735, acc 0.9375
2016-11-12T19:08:26.174833: step 1491, loss 0.0346099, acc 1
2016-11-12T19:08:26.230977: step 1492, loss 0.0344259, acc 1
2016-11-12T19:08:26.286132: step 1493, loss 0.0451477, acc 1
2016-11-12T19:08:26.345246: step 1494, loss 0.0831619, acc 0.96875
2016-11-12T19:08:26.404106: step 1495, loss 0.0493089, acc 0.984375
2016-11-12T19:08:26.460042: step 1496, loss 0.117902, acc 0.953125
2016-11-12T19:08:26.520629: step 1497, loss 0.0940929, acc 0.96875
2016-11-12T19:08:26.577905: step 1498, loss 0.100762, acc 0.984375
2016-11-12T19:08:26.633966: step 1499, loss 0.0335738, acc 1
2016-11-12T19:08:26.689921: step 1500, loss 0.064639, acc 1

Evaluation:
2016-11-12T19:08:26.759709: step 1500, loss 1.1658, acc 0.558

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-1500

2016-11-12T19:08:27.259100: step 1501, loss 0.0355793, acc 1
2016-11-12T19:08:27.314934: step 1502, loss 0.0841407, acc 0.953125
2016-11-12T19:08:27.373495: step 1503, loss 0.0663439, acc 0.984375
2016-11-12T19:08:27.428593: step 1504, loss 0.159039, acc 0.953125
2016-11-12T19:08:27.486033: step 1505, loss 0.129672, acc 0.96875
2016-11-12T19:08:27.541884: step 1506, loss 0.092208, acc 0.96875
2016-11-12T19:08:27.601292: step 1507, loss 0.0777254, acc 0.984375
2016-11-12T19:08:27.657251: step 1508, loss 0.0823925, acc 0.96875
2016-11-12T19:08:27.713653: step 1509, loss 0.110575, acc 0.984375
2016-11-12T19:08:27.769665: step 1510, loss 0.0680102, acc 0.984375
2016-11-12T19:08:27.828266: step 1511, loss 0.0542245, acc 0.984375
2016-11-12T19:08:27.883845: step 1512, loss 0.0529873, acc 0.984375
2016-11-12T19:08:27.942634: step 1513, loss 0.0470932, acc 1
2016-11-12T19:08:27.997876: step 1514, loss 0.0705242, acc 0.984375
2016-11-12T19:08:28.055420: step 1515, loss 0.0274216, acc 1
2016-11-12T19:08:28.111763: step 1516, loss 0.146444, acc 0.9375
2016-11-12T19:08:28.167041: step 1517, loss 0.0354758, acc 0.984375
2016-11-12T19:08:28.225221: step 1518, loss 0.0674037, acc 0.96875
2016-11-12T19:08:28.281190: step 1519, loss 0.065077, acc 0.96875
2016-11-12T19:08:28.337291: step 1520, loss 0.107388, acc 0.96875
2016-11-12T19:08:28.394757: step 1521, loss 0.0328304, acc 1
2016-11-12T19:08:28.450859: step 1522, loss 0.0743411, acc 0.984375
2016-11-12T19:08:28.506999: step 1523, loss 0.117239, acc 0.953125
2016-11-12T19:08:28.563977: step 1524, loss 0.0333262, acc 1
2016-11-12T19:08:28.621596: step 1525, loss 0.0833154, acc 0.953125
2016-11-12T19:08:28.680496: step 1526, loss 0.0477671, acc 0.984375
2016-11-12T19:08:28.739697: step 1527, loss 0.130084, acc 0.9375
2016-11-12T19:08:28.795422: step 1528, loss 0.0697087, acc 0.96875
2016-11-12T19:08:28.852687: step 1529, loss 0.109089, acc 0.96875
2016-11-12T19:08:28.909374: step 1530, loss 0.137074, acc 0.9375
2016-11-12T19:08:28.964840: step 1531, loss 0.113655, acc 0.953125
2016-11-12T19:08:29.020995: step 1532, loss 0.0824186, acc 0.984375
2016-11-12T19:08:29.076787: step 1533, loss 0.122994, acc 0.953125
2016-11-12T19:08:29.132326: step 1534, loss 0.0609852, acc 0.96875
2016-11-12T19:08:29.188206: step 1535, loss 0.0861489, acc 0.96875
2016-11-12T19:08:29.245682: step 1536, loss 0.137787, acc 0.9375
2016-11-12T19:08:29.302452: step 1537, loss 0.0953098, acc 0.96875
2016-11-12T19:08:29.360077: step 1538, loss 0.0452585, acc 0.984375
2016-11-12T19:08:29.416462: step 1539, loss 0.0753705, acc 0.96875
2016-11-12T19:08:29.471723: step 1540, loss 0.0533161, acc 0.984375
2016-11-12T19:08:29.527891: step 1541, loss 0.0626104, acc 0.984375
2016-11-12T19:08:29.583185: step 1542, loss 0.129922, acc 0.9375
2016-11-12T19:08:29.641562: step 1543, loss 0.0552881, acc 0.984375
2016-11-12T19:08:29.698595: step 1544, loss 0.0312244, acc 1
2016-11-12T19:08:29.755192: step 1545, loss 0.0618448, acc 0.984375
2016-11-12T19:08:29.814385: step 1546, loss 0.0954947, acc 0.953125
2016-11-12T19:08:29.870402: step 1547, loss 0.0429043, acc 1
2016-11-12T19:08:29.926282: step 1548, loss 0.0765717, acc 0.984375
2016-11-12T19:08:29.983285: step 1549, loss 0.0407168, acc 0.984375
2016-11-12T19:08:30.040529: step 1550, loss 0.10428, acc 0.984375
2016-11-12T19:08:30.097384: step 1551, loss 0.0923121, acc 0.953125
2016-11-12T19:08:30.153482: step 1552, loss 0.0385032, acc 0.984375
2016-11-12T19:08:30.210718: step 1553, loss 0.0468811, acc 0.984375
2016-11-12T19:08:30.267582: step 1554, loss 0.0915677, acc 0.953125
2016-11-12T19:08:30.327139: step 1555, loss 0.165831, acc 0.9375
2016-11-12T19:08:30.383280: step 1556, loss 0.0481281, acc 1
2016-11-12T19:08:30.440535: step 1557, loss 0.0476439, acc 1
2016-11-12T19:08:30.497194: step 1558, loss 0.0694384, acc 0.96875
2016-11-12T19:08:30.553428: step 1559, loss 0.0957177, acc 0.96875
2016-11-12T19:08:30.609468: step 1560, loss 0.121251, acc 0.96875
2016-11-12T19:08:30.665564: step 1561, loss 0.067429, acc 0.953125
2016-11-12T19:08:30.704216: step 1562, loss 0.0371424, acc 1
2016-11-12T19:08:30.761375: step 1563, loss 0.026339, acc 0.984375
2016-11-12T19:08:30.816815: step 1564, loss 0.0290425, acc 1
2016-11-12T19:08:30.873120: step 1565, loss 0.0973748, acc 0.96875
2016-11-12T19:08:30.928733: step 1566, loss 0.0369992, acc 1
2016-11-12T19:08:30.984382: step 1567, loss 0.0629361, acc 0.984375
2016-11-12T19:08:31.041359: step 1568, loss 0.0599206, acc 1
2016-11-12T19:08:31.097075: step 1569, loss 0.111214, acc 0.953125
2016-11-12T19:08:31.154586: step 1570, loss 0.0886318, acc 0.984375
2016-11-12T19:08:31.210118: step 1571, loss 0.0681496, acc 0.96875
2016-11-12T19:08:31.268917: step 1572, loss 0.0577489, acc 0.984375
2016-11-12T19:08:31.326236: step 1573, loss 0.0450092, acc 1
2016-11-12T19:08:31.382095: step 1574, loss 0.0166964, acc 1
2016-11-12T19:08:31.440815: step 1575, loss 0.100521, acc 0.96875
2016-11-12T19:08:31.497391: step 1576, loss 0.107248, acc 0.953125
2016-11-12T19:08:31.552814: step 1577, loss 0.0401062, acc 0.984375
2016-11-12T19:08:31.608880: step 1578, loss 0.0424584, acc 0.96875
2016-11-12T19:08:31.663978: step 1579, loss 0.0992809, acc 0.953125
2016-11-12T19:08:31.719580: step 1580, loss 0.113068, acc 0.953125
2016-11-12T19:08:31.776733: step 1581, loss 0.0831061, acc 0.96875
2016-11-12T19:08:31.833448: step 1582, loss 0.0975315, acc 0.953125
2016-11-12T19:08:31.891339: step 1583, loss 0.0788427, acc 0.984375
2016-11-12T19:08:31.949006: step 1584, loss 0.0345973, acc 1
2016-11-12T19:08:32.006410: step 1585, loss 0.0788022, acc 0.984375
2016-11-12T19:08:32.064759: step 1586, loss 0.087413, acc 0.96875
2016-11-12T19:08:32.119890: step 1587, loss 0.066893, acc 0.984375
2016-11-12T19:08:32.176457: step 1588, loss 0.103197, acc 0.96875
2016-11-12T19:08:32.233152: step 1589, loss 0.0568083, acc 0.984375
2016-11-12T19:08:32.290411: step 1590, loss 0.108882, acc 0.96875
2016-11-12T19:08:32.345859: step 1591, loss 0.0946176, acc 0.953125
2016-11-12T19:08:32.404266: step 1592, loss 0.0487716, acc 1
2016-11-12T19:08:32.462504: step 1593, loss 0.130369, acc 0.953125
2016-11-12T19:08:32.519744: step 1594, loss 0.048466, acc 0.984375
2016-11-12T19:08:32.577794: step 1595, loss 0.0257187, acc 1
2016-11-12T19:08:32.634703: step 1596, loss 0.0945542, acc 0.984375
2016-11-12T19:08:32.692605: step 1597, loss 0.0370701, acc 0.984375
2016-11-12T19:08:32.748051: step 1598, loss 0.0814836, acc 0.984375
2016-11-12T19:08:32.805548: step 1599, loss 0.044811, acc 1
2016-11-12T19:08:32.861937: step 1600, loss 0.0687237, acc 0.984375

Evaluation:
2016-11-12T19:08:32.932037: step 1600, loss 1.23366, acc 0.55

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-1600

2016-11-12T19:08:33.430848: step 1601, loss 0.0805855, acc 0.96875
2016-11-12T19:08:33.486829: step 1602, loss 0.0689142, acc 0.96875
2016-11-12T19:08:33.542824: step 1603, loss 0.089151, acc 0.953125
2016-11-12T19:08:33.599547: step 1604, loss 0.08892, acc 0.953125
2016-11-12T19:08:33.655565: step 1605, loss 0.050027, acc 1
2016-11-12T19:08:33.713033: step 1606, loss 0.105737, acc 0.96875
2016-11-12T19:08:33.769350: step 1607, loss 0.114388, acc 0.96875
2016-11-12T19:08:33.825061: step 1608, loss 0.0829362, acc 0.96875
2016-11-12T19:08:33.881156: step 1609, loss 0.0679975, acc 0.953125
2016-11-12T19:08:33.937085: step 1610, loss 0.0689982, acc 0.96875
2016-11-12T19:08:33.993222: step 1611, loss 0.11199, acc 0.984375
2016-11-12T19:08:34.049816: step 1612, loss 0.0912512, acc 0.96875
2016-11-12T19:08:34.106579: step 1613, loss 0.100864, acc 0.9375
2016-11-12T19:08:34.163654: step 1614, loss 0.0514326, acc 0.984375
2016-11-12T19:08:34.221408: step 1615, loss 0.0533744, acc 0.984375
2016-11-12T19:08:34.277624: step 1616, loss 0.0751484, acc 0.96875
2016-11-12T19:08:34.332842: step 1617, loss 0.0326338, acc 1
2016-11-12T19:08:34.388669: step 1618, loss 0.0658085, acc 0.984375
2016-11-12T19:08:34.445571: step 1619, loss 0.0536158, acc 1
2016-11-12T19:08:34.502576: step 1620, loss 0.0527954, acc 0.984375
2016-11-12T19:08:34.560000: step 1621, loss 0.0468755, acc 1
2016-11-12T19:08:34.615130: step 1622, loss 0.143645, acc 0.921875
2016-11-12T19:08:34.674201: step 1623, loss 0.0201327, acc 1
2016-11-12T19:08:34.733383: step 1624, loss 0.0952573, acc 0.984375
2016-11-12T19:08:34.790376: step 1625, loss 0.0413743, acc 1
2016-11-12T19:08:34.846234: step 1626, loss 0.0283638, acc 1
2016-11-12T19:08:34.903533: step 1627, loss 0.168517, acc 0.953125
2016-11-12T19:08:34.959968: step 1628, loss 0.0992103, acc 0.9375
2016-11-12T19:08:35.016182: step 1629, loss 0.0960207, acc 0.96875
2016-11-12T19:08:35.074605: step 1630, loss 0.0675338, acc 0.984375
2016-11-12T19:08:35.132932: step 1631, loss 0.0972702, acc 0.96875
2016-11-12T19:08:35.191004: step 1632, loss 0.0912868, acc 0.96875
2016-11-12T19:08:35.230805: step 1633, loss 0.0874782, acc 0.95
2016-11-12T19:08:35.289110: step 1634, loss 0.0871783, acc 0.953125
2016-11-12T19:08:35.345108: step 1635, loss 0.0846776, acc 0.953125
2016-11-12T19:08:35.402067: step 1636, loss 0.0472797, acc 0.984375
2016-11-12T19:08:35.458261: step 1637, loss 0.0352428, acc 1
2016-11-12T19:08:35.513984: step 1638, loss 0.0711535, acc 0.984375
2016-11-12T19:08:35.569272: step 1639, loss 0.0626856, acc 0.984375
2016-11-12T19:08:35.625654: step 1640, loss 0.0336447, acc 1
2016-11-12T19:08:35.682503: step 1641, loss 0.0379798, acc 0.984375
2016-11-12T19:08:35.739945: step 1642, loss 0.0313662, acc 1
2016-11-12T19:08:35.795868: step 1643, loss 0.0369812, acc 1
2016-11-12T19:08:35.852736: step 1644, loss 0.0502679, acc 1
2016-11-12T19:08:35.909648: step 1645, loss 0.0743102, acc 0.96875
2016-11-12T19:08:35.965612: step 1646, loss 0.0831676, acc 0.953125
2016-11-12T19:08:36.022392: step 1647, loss 0.0557558, acc 0.984375
2016-11-12T19:08:36.078608: step 1648, loss 0.116642, acc 0.96875
2016-11-12T19:08:36.136393: step 1649, loss 0.0700521, acc 0.984375
2016-11-12T19:08:36.192423: step 1650, loss 0.0710402, acc 0.984375
2016-11-12T19:08:36.248891: step 1651, loss 0.0238863, acc 1
2016-11-12T19:08:36.305573: step 1652, loss 0.0540175, acc 0.984375
2016-11-12T19:08:36.362753: step 1653, loss 0.0264817, acc 1
2016-11-12T19:08:36.417779: step 1654, loss 0.0571056, acc 1
2016-11-12T19:08:36.474425: step 1655, loss 0.0602009, acc 0.984375
2016-11-12T19:08:36.532078: step 1656, loss 0.0618427, acc 0.984375
2016-11-12T19:08:36.588743: step 1657, loss 0.047223, acc 0.984375
2016-11-12T19:08:36.644653: step 1658, loss 0.0583092, acc 0.984375
2016-11-12T19:08:36.701353: step 1659, loss 0.0418812, acc 0.96875
2016-11-12T19:08:36.759121: step 1660, loss 0.0321403, acc 0.984375
2016-11-12T19:08:36.817111: step 1661, loss 0.0579278, acc 1
2016-11-12T19:08:36.875812: step 1662, loss 0.0801341, acc 0.953125
2016-11-12T19:08:36.931486: step 1663, loss 0.0526699, acc 1
2016-11-12T19:08:36.989662: step 1664, loss 0.0713009, acc 0.984375
2016-11-12T19:08:37.046901: step 1665, loss 0.0784385, acc 0.96875
2016-11-12T19:08:37.105220: step 1666, loss 0.0299326, acc 0.984375
2016-11-12T19:08:37.160083: step 1667, loss 0.10791, acc 0.953125
2016-11-12T19:08:37.217452: step 1668, loss 0.0749601, acc 0.984375
2016-11-12T19:08:37.273375: step 1669, loss 0.0215407, acc 1
2016-11-12T19:08:37.333628: step 1670, loss 0.0759446, acc 0.96875
2016-11-12T19:08:37.389345: step 1671, loss 0.0599475, acc 1
2016-11-12T19:08:37.445079: step 1672, loss 0.0984175, acc 0.953125
2016-11-12T19:08:37.500883: step 1673, loss 0.0447951, acc 1
2016-11-12T19:08:37.556805: step 1674, loss 0.0937466, acc 0.953125
2016-11-12T19:08:37.613023: step 1675, loss 0.0304387, acc 1
2016-11-12T19:08:37.674153: step 1676, loss 0.0294587, acc 1
2016-11-12T19:08:37.730743: step 1677, loss 0.0154101, acc 1
2016-11-12T19:08:37.786492: step 1678, loss 0.0171771, acc 1
2016-11-12T19:08:37.841993: step 1679, loss 0.0439871, acc 0.984375
2016-11-12T19:08:37.898954: step 1680, loss 0.0330629, acc 1
2016-11-12T19:08:37.955172: step 1681, loss 0.0445944, acc 1
2016-11-12T19:08:38.010074: step 1682, loss 0.0717714, acc 0.96875
2016-11-12T19:08:38.065315: step 1683, loss 0.073815, acc 0.984375
2016-11-12T19:08:38.123362: step 1684, loss 0.052754, acc 0.984375
2016-11-12T19:08:38.181249: step 1685, loss 0.0796194, acc 0.953125
2016-11-12T19:08:38.240314: step 1686, loss 0.0818869, acc 0.984375
2016-11-12T19:08:38.295659: step 1687, loss 0.0443939, acc 0.984375
2016-11-12T19:08:38.352927: step 1688, loss 0.0393987, acc 1
2016-11-12T19:08:38.409377: step 1689, loss 0.052011, acc 1
2016-11-12T19:08:38.465846: step 1690, loss 0.109307, acc 0.96875
2016-11-12T19:08:38.523572: step 1691, loss 0.016202, acc 1
2016-11-12T19:08:38.582177: step 1692, loss 0.0981909, acc 0.96875
2016-11-12T19:08:38.639589: step 1693, loss 0.0530423, acc 0.984375
2016-11-12T19:08:38.696923: step 1694, loss 0.0564575, acc 0.984375
2016-11-12T19:08:38.753650: step 1695, loss 0.106954, acc 0.953125
2016-11-12T19:08:38.810311: step 1696, loss 0.0619241, acc 1
2016-11-12T19:08:38.867118: step 1697, loss 0.0508364, acc 0.96875
2016-11-12T19:08:38.924511: step 1698, loss 0.0185087, acc 1
2016-11-12T19:08:38.981008: step 1699, loss 0.109651, acc 0.96875
2016-11-12T19:08:39.039542: step 1700, loss 0.115799, acc 0.953125

Evaluation:
2016-11-12T19:08:39.109445: step 1700, loss 1.26259, acc 0.558

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-1700

2016-11-12T19:08:39.603576: step 1701, loss 0.0776823, acc 0.96875
2016-11-12T19:08:39.659386: step 1702, loss 0.0464728, acc 1
2016-11-12T19:08:39.717353: step 1703, loss 0.0529151, acc 0.984375
2016-11-12T19:08:39.756321: step 1704, loss 0.0421547, acc 1
2016-11-12T19:08:39.816144: step 1705, loss 0.0562644, acc 0.96875
2016-11-12T19:08:39.872700: step 1706, loss 0.0604888, acc 0.96875
2016-11-12T19:08:39.929941: step 1707, loss 0.0348003, acc 1
2016-11-12T19:08:39.986732: step 1708, loss 0.0216649, acc 1
2016-11-12T19:08:40.043670: step 1709, loss 0.0193567, acc 1
2016-11-12T19:08:40.099324: step 1710, loss 0.0657969, acc 0.96875
2016-11-12T19:08:40.157532: step 1711, loss 0.0462498, acc 0.984375
2016-11-12T19:08:40.213377: step 1712, loss 0.0833385, acc 0.984375
2016-11-12T19:08:40.269288: step 1713, loss 0.0367782, acc 1
2016-11-12T19:08:40.327328: step 1714, loss 0.0462507, acc 0.984375
2016-11-12T19:08:40.384389: step 1715, loss 0.0276411, acc 1
2016-11-12T19:08:40.439965: step 1716, loss 0.0618634, acc 0.984375
2016-11-12T19:08:40.497828: step 1717, loss 0.06156, acc 0.984375
2016-11-12T19:08:40.557488: step 1718, loss 0.0496301, acc 0.984375
2016-11-12T19:08:40.613662: step 1719, loss 0.0184247, acc 1
2016-11-12T19:08:40.669370: step 1720, loss 0.0298334, acc 1
2016-11-12T19:08:40.727538: step 1721, loss 0.0298857, acc 1
2016-11-12T19:08:40.783682: step 1722, loss 0.0343375, acc 0.984375
2016-11-12T19:08:40.841338: step 1723, loss 0.0440009, acc 1
2016-11-12T19:08:40.897347: step 1724, loss 0.0331118, acc 1
2016-11-12T19:08:40.953548: step 1725, loss 0.073808, acc 0.96875
2016-11-12T19:08:41.010797: step 1726, loss 0.0934465, acc 0.96875
2016-11-12T19:08:41.068689: step 1727, loss 0.0563459, acc 0.984375
2016-11-12T19:08:41.124461: step 1728, loss 0.0626651, acc 0.984375
2016-11-12T19:08:41.180454: step 1729, loss 0.0562097, acc 0.96875
2016-11-12T19:08:41.236483: step 1730, loss 0.0638268, acc 0.96875
2016-11-12T19:08:41.293117: step 1731, loss 0.0758189, acc 0.953125
2016-11-12T19:08:41.350946: step 1732, loss 0.0883577, acc 0.96875
2016-11-12T19:08:41.408462: step 1733, loss 0.01771, acc 1
2016-11-12T19:08:41.464884: step 1734, loss 0.0497009, acc 0.984375
2016-11-12T19:08:41.520620: step 1735, loss 0.041524, acc 1
2016-11-12T19:08:41.578145: step 1736, loss 0.0838356, acc 0.953125
2016-11-12T19:08:41.634539: step 1737, loss 0.15964, acc 0.953125
2016-11-12T19:08:41.692353: step 1738, loss 0.0322832, acc 0.984375
2016-11-12T19:08:41.749353: step 1739, loss 0.0706092, acc 0.953125
2016-11-12T19:08:41.806000: step 1740, loss 0.0546435, acc 1
2016-11-12T19:08:41.860909: step 1741, loss 0.0412396, acc 0.984375
2016-11-12T19:08:41.918612: step 1742, loss 0.0510125, acc 0.96875
2016-11-12T19:08:41.974168: step 1743, loss 0.023684, acc 1
2016-11-12T19:08:42.032541: step 1744, loss 0.0771846, acc 0.96875
2016-11-12T19:08:42.090677: step 1745, loss 0.0580632, acc 0.984375
2016-11-12T19:08:42.149435: step 1746, loss 0.0847557, acc 0.96875
2016-11-12T19:08:42.205418: step 1747, loss 0.00871893, acc 1
2016-11-12T19:08:42.261690: step 1748, loss 0.0475448, acc 0.96875
2016-11-12T19:08:42.318815: step 1749, loss 0.0331748, acc 0.984375
2016-11-12T19:08:42.375131: step 1750, loss 0.194075, acc 0.953125
2016-11-12T19:08:42.431083: step 1751, loss 0.0966583, acc 0.96875
2016-11-12T19:08:42.487480: step 1752, loss 0.0947009, acc 0.96875
2016-11-12T19:08:42.544012: step 1753, loss 0.0788466, acc 0.953125
2016-11-12T19:08:42.599879: step 1754, loss 0.0522766, acc 0.96875
2016-11-12T19:08:42.657085: step 1755, loss 0.117658, acc 0.96875
2016-11-12T19:08:42.712103: step 1756, loss 0.0262437, acc 1
2016-11-12T19:08:42.769179: step 1757, loss 0.127066, acc 0.96875
2016-11-12T19:08:42.825141: step 1758, loss 0.0523628, acc 0.984375
2016-11-12T19:08:42.881640: step 1759, loss 0.0459107, acc 0.984375
2016-11-12T19:08:42.937596: step 1760, loss 0.0858678, acc 0.96875
2016-11-12T19:08:42.995182: step 1761, loss 0.0658819, acc 0.96875
2016-11-12T19:08:43.052860: step 1762, loss 0.087813, acc 0.953125
2016-11-12T19:08:43.108217: step 1763, loss 0.055653, acc 0.984375
2016-11-12T19:08:43.163646: step 1764, loss 0.0331315, acc 1
2016-11-12T19:08:43.219878: step 1765, loss 0.055963, acc 0.96875
2016-11-12T19:08:43.279187: step 1766, loss 0.0405994, acc 0.984375
2016-11-12T19:08:43.335477: step 1767, loss 0.0565425, acc 0.984375
2016-11-12T19:08:43.391941: step 1768, loss 0.0230339, acc 1
2016-11-12T19:08:43.447941: step 1769, loss 0.0401529, acc 0.96875
2016-11-12T19:08:43.505204: step 1770, loss 0.0426053, acc 0.984375
2016-11-12T19:08:43.561481: step 1771, loss 0.10609, acc 0.96875
2016-11-12T19:08:43.617574: step 1772, loss 0.0921831, acc 0.96875
2016-11-12T19:08:43.677273: step 1773, loss 0.0955635, acc 0.96875
2016-11-12T19:08:43.733411: step 1774, loss 0.0336769, acc 1
2016-11-12T19:08:43.770049: step 1775, loss 0.20196, acc 0.95
2016-11-12T19:08:43.826256: step 1776, loss 0.0458942, acc 0.984375
2016-11-12T19:08:43.882110: step 1777, loss 0.03605, acc 1
2016-11-12T19:08:43.938316: step 1778, loss 0.0720324, acc 0.984375
2016-11-12T19:08:43.996169: step 1779, loss 0.0998207, acc 0.96875
2016-11-12T19:08:44.052262: step 1780, loss 0.0919854, acc 0.953125
2016-11-12T19:08:44.108765: step 1781, loss 0.0336547, acc 1
2016-11-12T19:08:44.165670: step 1782, loss 0.0399665, acc 0.984375
2016-11-12T19:08:44.221572: step 1783, loss 0.090759, acc 0.953125
2016-11-12T19:08:44.280355: step 1784, loss 0.036634, acc 1
2016-11-12T19:08:44.337628: step 1785, loss 0.0533909, acc 1
2016-11-12T19:08:44.393274: step 1786, loss 0.0430388, acc 0.984375
2016-11-12T19:08:44.449241: step 1787, loss 0.154151, acc 0.96875
2016-11-12T19:08:44.504668: step 1788, loss 0.100306, acc 0.984375
2016-11-12T19:08:44.561231: step 1789, loss 0.0489785, acc 0.96875
2016-11-12T19:08:44.616701: step 1790, loss 0.0172349, acc 1
2016-11-12T19:08:44.673550: step 1791, loss 0.0380665, acc 0.984375
2016-11-12T19:08:44.729417: step 1792, loss 0.0641371, acc 0.96875
2016-11-12T19:08:44.785954: step 1793, loss 0.0244482, acc 1
2016-11-12T19:08:44.843246: step 1794, loss 0.0428553, acc 0.96875
2016-11-12T19:08:44.899238: step 1795, loss 0.0426071, acc 0.984375
2016-11-12T19:08:44.955609: step 1796, loss 0.0404963, acc 1
2016-11-12T19:08:45.013562: step 1797, loss 0.0541095, acc 1
2016-11-12T19:08:45.068584: step 1798, loss 0.0517817, acc 0.984375
2016-11-12T19:08:45.126827: step 1799, loss 0.024618, acc 0.984375
2016-11-12T19:08:45.183294: step 1800, loss 0.018317, acc 1

Evaluation:
2016-11-12T19:08:45.253730: step 1800, loss 1.32187, acc 0.556

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-1800

2016-11-12T19:08:45.749085: step 1801, loss 0.0510721, acc 0.984375
2016-11-12T19:08:45.805493: step 1802, loss 0.0265808, acc 1
2016-11-12T19:08:45.861678: step 1803, loss 0.0232415, acc 1
2016-11-12T19:08:45.917225: step 1804, loss 0.0226759, acc 1
2016-11-12T19:08:45.975750: step 1805, loss 0.0282195, acc 1
2016-11-12T19:08:46.033041: step 1806, loss 0.0389751, acc 0.984375
2016-11-12T19:08:46.089147: step 1807, loss 0.042025, acc 0.984375
2016-11-12T19:08:46.145217: step 1808, loss 0.0422122, acc 0.96875
2016-11-12T19:08:46.201375: step 1809, loss 0.051191, acc 0.96875
2016-11-12T19:08:46.257041: step 1810, loss 0.040661, acc 0.984375
2016-11-12T19:08:46.312781: step 1811, loss 0.0646861, acc 0.96875
2016-11-12T19:08:46.369223: step 1812, loss 0.0395918, acc 0.984375
2016-11-12T19:08:46.425493: step 1813, loss 0.0433079, acc 0.984375
2016-11-12T19:08:46.481984: step 1814, loss 0.0386233, acc 1
2016-11-12T19:08:46.539050: step 1815, loss 0.0186776, acc 1
2016-11-12T19:08:46.596038: step 1816, loss 0.0280425, acc 1
2016-11-12T19:08:46.652794: step 1817, loss 0.116988, acc 0.96875
2016-11-12T19:08:46.708397: step 1818, loss 0.0299386, acc 0.984375
2016-11-12T19:08:46.764848: step 1819, loss 0.0426987, acc 0.96875
2016-11-12T19:08:46.821412: step 1820, loss 0.0658158, acc 0.96875
2016-11-12T19:08:46.877454: step 1821, loss 0.0370149, acc 1
2016-11-12T19:08:46.932601: step 1822, loss 0.0490793, acc 0.984375
2016-11-12T19:08:46.989415: step 1823, loss 0.0483598, acc 0.96875
2016-11-12T19:08:47.047608: step 1824, loss 0.0657034, acc 0.984375
2016-11-12T19:08:47.103313: step 1825, loss 0.112096, acc 0.984375
2016-11-12T19:08:47.159569: step 1826, loss 0.0445237, acc 0.984375
2016-11-12T19:08:47.217322: step 1827, loss 0.0245857, acc 1
2016-11-12T19:08:47.273363: step 1828, loss 0.0309835, acc 0.984375
2016-11-12T19:08:47.332410: step 1829, loss 0.0494932, acc 0.984375
2016-11-12T19:08:47.389542: step 1830, loss 0.0319756, acc 1
2016-11-12T19:08:47.449148: step 1831, loss 0.094878, acc 0.96875
2016-11-12T19:08:47.507088: step 1832, loss 0.079161, acc 0.96875
2016-11-12T19:08:47.565341: step 1833, loss 0.0389381, acc 1
2016-11-12T19:08:47.622177: step 1834, loss 0.0638015, acc 0.984375
2016-11-12T19:08:47.680861: step 1835, loss 0.0545499, acc 0.984375
2016-11-12T19:08:47.736869: step 1836, loss 0.161267, acc 0.953125
2016-11-12T19:08:47.792888: step 1837, loss 0.0139544, acc 1
2016-11-12T19:08:47.848730: step 1838, loss 0.0185621, acc 1
2016-11-12T19:08:47.905613: step 1839, loss 0.0461385, acc 1
2016-11-12T19:08:47.962672: step 1840, loss 0.0441061, acc 1
2016-11-12T19:08:48.018903: step 1841, loss 0.0541819, acc 0.984375
2016-11-12T19:08:48.076390: step 1842, loss 0.0733658, acc 0.953125
2016-11-12T19:08:48.133605: step 1843, loss 0.0322238, acc 1
2016-11-12T19:08:48.191558: step 1844, loss 0.10743, acc 0.96875
2016-11-12T19:08:48.248658: step 1845, loss 0.039031, acc 0.984375
2016-11-12T19:08:48.288946: step 1846, loss 0.0504121, acc 0.95
2016-11-12T19:08:48.347022: step 1847, loss 0.0374328, acc 0.984375
2016-11-12T19:08:48.405032: step 1848, loss 0.0823492, acc 0.96875
2016-11-12T19:08:48.461131: step 1849, loss 0.0596668, acc 0.96875
2016-11-12T19:08:48.517222: step 1850, loss 0.0469279, acc 0.984375
2016-11-12T19:08:48.573381: step 1851, loss 0.117298, acc 0.921875
2016-11-12T19:08:48.629397: step 1852, loss 0.0481026, acc 0.984375
2016-11-12T19:08:48.686427: step 1853, loss 0.0510239, acc 0.984375
2016-11-12T19:08:48.744959: step 1854, loss 0.0446599, acc 0.984375
2016-11-12T19:08:48.801080: step 1855, loss 0.076349, acc 0.953125
2016-11-12T19:08:48.856356: step 1856, loss 0.108106, acc 0.953125
2016-11-12T19:08:48.913090: step 1857, loss 0.0756377, acc 0.953125
2016-11-12T19:08:48.969595: step 1858, loss 0.0638537, acc 0.96875
2016-11-12T19:08:49.025367: step 1859, loss 0.0520482, acc 0.984375
2016-11-12T19:08:49.082944: step 1860, loss 0.0408947, acc 1
2016-11-12T19:08:49.138983: step 1861, loss 0.018281, acc 1
2016-11-12T19:08:49.197550: step 1862, loss 0.0313965, acc 1
2016-11-12T19:08:49.253643: step 1863, loss 0.0466231, acc 0.984375
2016-11-12T19:08:49.312164: step 1864, loss 0.0479352, acc 0.984375
2016-11-12T19:08:49.369328: step 1865, loss 0.0359596, acc 1
2016-11-12T19:08:49.425117: step 1866, loss 0.100215, acc 0.96875
2016-11-12T19:08:49.483273: step 1867, loss 0.0469963, acc 0.984375
2016-11-12T19:08:49.541082: step 1868, loss 0.0458938, acc 0.984375
2016-11-12T19:08:49.597003: step 1869, loss 0.0673594, acc 0.984375
2016-11-12T19:08:49.652665: step 1870, loss 0.0416216, acc 0.984375
2016-11-12T19:08:49.708113: step 1871, loss 0.0704728, acc 0.96875
2016-11-12T19:08:49.766140: step 1872, loss 0.0416147, acc 0.96875
2016-11-12T19:08:49.821598: step 1873, loss 0.0204725, acc 1
2016-11-12T19:08:49.880829: step 1874, loss 0.0276061, acc 0.984375
2016-11-12T19:08:49.935842: step 1875, loss 0.117339, acc 0.96875
2016-11-12T19:08:49.993682: step 1876, loss 0.0819146, acc 0.984375
2016-11-12T19:08:50.050066: step 1877, loss 0.035261, acc 1
2016-11-12T19:08:50.106632: step 1878, loss 0.0623038, acc 0.96875
2016-11-12T19:08:50.163705: step 1879, loss 0.0294399, acc 1
2016-11-12T19:08:50.219299: step 1880, loss 0.0722102, acc 0.984375
2016-11-12T19:08:50.275298: step 1881, loss 0.0389924, acc 1
2016-11-12T19:08:50.331223: step 1882, loss 0.0694176, acc 0.96875
2016-11-12T19:08:50.390449: step 1883, loss 0.0453874, acc 0.984375
2016-11-12T19:08:50.446387: step 1884, loss 0.0473276, acc 1
2016-11-12T19:08:50.503061: step 1885, loss 0.0546079, acc 0.96875
2016-11-12T19:08:50.559694: step 1886, loss 0.0186356, acc 1
2016-11-12T19:08:50.615708: step 1887, loss 0.0584667, acc 0.984375
2016-11-12T19:08:50.671616: step 1888, loss 0.0391713, acc 1
2016-11-12T19:08:50.727450: step 1889, loss 0.0246927, acc 1
2016-11-12T19:08:50.785101: step 1890, loss 0.065385, acc 0.953125
2016-11-12T19:08:50.843487: step 1891, loss 0.0522383, acc 0.984375
2016-11-12T19:08:50.902004: step 1892, loss 0.0381121, acc 0.984375
2016-11-12T19:08:50.959483: step 1893, loss 0.024875, acc 1
2016-11-12T19:08:51.017693: step 1894, loss 0.0277755, acc 1
2016-11-12T19:08:51.073193: step 1895, loss 0.0547135, acc 0.96875
2016-11-12T19:08:51.129251: step 1896, loss 0.016707, acc 1
2016-11-12T19:08:51.184418: step 1897, loss 0.0882392, acc 0.96875
2016-11-12T19:08:51.244003: step 1898, loss 0.0774174, acc 0.96875
2016-11-12T19:08:51.301308: step 1899, loss 0.0336512, acc 1
2016-11-12T19:08:51.358004: step 1900, loss 0.0529976, acc 1

Evaluation:
2016-11-12T19:08:51.429426: step 1900, loss 1.35687, acc 0.556

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-1900

2016-11-12T19:08:51.926329: step 1901, loss 0.0104222, acc 1
2016-11-12T19:08:51.985658: step 1902, loss 0.0442021, acc 0.984375
2016-11-12T19:08:52.041311: step 1903, loss 0.0389429, acc 0.984375
2016-11-12T19:08:52.097047: step 1904, loss 0.0599645, acc 0.96875
2016-11-12T19:08:52.153541: step 1905, loss 0.0881163, acc 0.9375
2016-11-12T19:08:52.209730: step 1906, loss 0.0900381, acc 0.984375
2016-11-12T19:08:52.266339: step 1907, loss 0.0236413, acc 1
2016-11-12T19:08:52.322129: step 1908, loss 0.0316155, acc 1
2016-11-12T19:08:52.381782: step 1909, loss 0.0271339, acc 1
2016-11-12T19:08:52.437245: step 1910, loss 0.06257, acc 0.984375
2016-11-12T19:08:52.495126: step 1911, loss 0.0325052, acc 1
2016-11-12T19:08:52.551829: step 1912, loss 0.0286965, acc 1
2016-11-12T19:08:52.607948: step 1913, loss 0.0395144, acc 0.984375
2016-11-12T19:08:52.665198: step 1914, loss 0.0608088, acc 0.96875
2016-11-12T19:08:52.720861: step 1915, loss 0.0328054, acc 1
2016-11-12T19:08:52.776599: step 1916, loss 0.0173484, acc 1
2016-11-12T19:08:52.812779: step 1917, loss 0.0603213, acc 0.95
2016-11-12T19:08:52.871812: step 1918, loss 0.0210066, acc 1
2016-11-12T19:08:52.929229: step 1919, loss 0.013959, acc 1
2016-11-12T19:08:52.985702: step 1920, loss 0.0149402, acc 1
2016-11-12T19:08:53.043400: step 1921, loss 0.0517963, acc 0.984375
2016-11-12T19:08:53.099525: step 1922, loss 0.133154, acc 0.953125
2016-11-12T19:08:53.155754: step 1923, loss 0.0697755, acc 0.984375
2016-11-12T19:08:53.213216: step 1924, loss 0.0229594, acc 1
2016-11-12T19:08:53.270187: step 1925, loss 0.0444488, acc 0.984375
2016-11-12T19:08:53.328080: step 1926, loss 0.0357604, acc 0.984375
2016-11-12T19:08:53.385393: step 1927, loss 0.0602514, acc 0.96875
2016-11-12T19:08:53.441700: step 1928, loss 0.0600257, acc 0.984375
2016-11-12T19:08:53.499902: step 1929, loss 0.0270668, acc 1
2016-11-12T19:08:53.557432: step 1930, loss 0.0299195, acc 0.984375
2016-11-12T19:08:53.615843: step 1931, loss 0.0149056, acc 1
2016-11-12T19:08:53.673286: step 1932, loss 0.0220067, acc 1
2016-11-12T19:08:53.730902: step 1933, loss 0.0412394, acc 0.984375
2016-11-12T19:08:53.787076: step 1934, loss 0.101342, acc 0.984375
2016-11-12T19:08:53.843133: step 1935, loss 0.14434, acc 0.984375
2016-11-12T19:08:53.900667: step 1936, loss 0.0315377, acc 1
2016-11-12T19:08:53.956156: step 1937, loss 0.0371212, acc 0.984375
2016-11-12T19:08:54.013620: step 1938, loss 0.0218776, acc 1
2016-11-12T19:08:54.069677: step 1939, loss 0.0118029, acc 1
2016-11-12T19:08:54.124516: step 1940, loss 0.0185735, acc 1
2016-11-12T19:08:54.181824: step 1941, loss 0.0199849, acc 1
2016-11-12T19:08:54.237069: step 1942, loss 0.016631, acc 1
2016-11-12T19:08:54.293820: step 1943, loss 0.0546833, acc 1
2016-11-12T19:08:54.351423: step 1944, loss 0.019512, acc 1
2016-11-12T19:08:54.409150: step 1945, loss 0.0169654, acc 1
2016-11-12T19:08:54.465516: step 1946, loss 0.0193618, acc 1
2016-11-12T19:08:54.522560: step 1947, loss 0.114381, acc 0.953125
2016-11-12T19:08:54.579849: step 1948, loss 0.0538979, acc 0.984375
2016-11-12T19:08:54.635749: step 1949, loss 0.023492, acc 1
2016-11-12T19:08:54.691604: step 1950, loss 0.0595546, acc 0.984375
2016-11-12T19:08:54.747781: step 1951, loss 0.0320049, acc 1
2016-11-12T19:08:54.805430: step 1952, loss 0.0486817, acc 0.984375
2016-11-12T19:08:54.863490: step 1953, loss 0.0443336, acc 0.984375
2016-11-12T19:08:54.919563: step 1954, loss 0.0178913, acc 1
2016-11-12T19:08:54.976320: step 1955, loss 0.0666855, acc 0.96875
2016-11-12T19:08:55.033095: step 1956, loss 0.0338449, acc 0.984375
2016-11-12T19:08:55.089658: step 1957, loss 0.0221124, acc 1
2016-11-12T19:08:55.146198: step 1958, loss 0.0686086, acc 0.96875
2016-11-12T19:08:55.205403: step 1959, loss 0.0425218, acc 0.984375
2016-11-12T19:08:55.261122: step 1960, loss 0.0506233, acc 0.984375
2016-11-12T19:08:55.321746: step 1961, loss 0.0527007, acc 0.96875
2016-11-12T19:08:55.378056: step 1962, loss 0.0881633, acc 0.953125
2016-11-12T19:08:55.434333: step 1963, loss 0.0494554, acc 0.984375
2016-11-12T19:08:55.490077: step 1964, loss 0.057743, acc 0.96875
2016-11-12T19:08:55.546733: step 1965, loss 0.0641463, acc 0.984375
2016-11-12T19:08:55.603281: step 1966, loss 0.0635955, acc 0.96875
2016-11-12T19:08:55.659626: step 1967, loss 0.060007, acc 0.96875
2016-11-12T19:08:55.716242: step 1968, loss 0.0378651, acc 0.984375
2016-11-12T19:08:55.773895: step 1969, loss 0.0917847, acc 0.96875
2016-11-12T19:08:55.830418: step 1970, loss 0.075527, acc 0.984375
2016-11-12T19:08:55.887243: step 1971, loss 0.0293463, acc 0.984375
2016-11-12T19:08:55.943926: step 1972, loss 0.0289717, acc 0.984375
2016-11-12T19:08:56.000741: step 1973, loss 0.0313358, acc 0.984375
2016-11-12T19:08:56.057819: step 1974, loss 0.0446087, acc 0.984375
2016-11-12T19:08:56.117131: step 1975, loss 0.0504789, acc 0.984375
2016-11-12T19:08:56.173658: step 1976, loss 0.0452119, acc 1
2016-11-12T19:08:56.231466: step 1977, loss 0.0175333, acc 1
2016-11-12T19:08:56.289134: step 1978, loss 0.040069, acc 0.984375
2016-11-12T19:08:56.345353: step 1979, loss 0.0399775, acc 1
2016-11-12T19:08:56.401641: step 1980, loss 0.0279069, acc 0.984375
2016-11-12T19:08:56.457778: step 1981, loss 0.0310431, acc 0.984375
2016-11-12T19:08:56.513320: step 1982, loss 0.0438518, acc 0.96875
2016-11-12T19:08:56.569101: step 1983, loss 0.0727784, acc 0.96875
2016-11-12T19:08:56.624769: step 1984, loss 0.0210076, acc 1
2016-11-12T19:08:56.681334: step 1985, loss 0.0309663, acc 1
2016-11-12T19:08:56.739625: step 1986, loss 0.0259194, acc 1
2016-11-12T19:08:56.796542: step 1987, loss 0.102636, acc 0.96875
2016-11-12T19:08:56.836252: step 1988, loss 0.164289, acc 0.95
2016-11-12T19:08:56.893579: step 1989, loss 0.0353853, acc 0.984375
2016-11-12T19:08:56.953085: step 1990, loss 0.0417657, acc 0.984375
2016-11-12T19:08:57.010011: step 1991, loss 0.0492985, acc 0.984375
2016-11-12T19:08:57.068329: step 1992, loss 0.0394407, acc 1
2016-11-12T19:08:57.125107: step 1993, loss 0.0803319, acc 0.953125
2016-11-12T19:08:57.181606: step 1994, loss 0.0537556, acc 0.984375
2016-11-12T19:08:57.237797: step 1995, loss 0.0759835, acc 0.921875
2016-11-12T19:08:57.294285: step 1996, loss 0.0751136, acc 0.96875
2016-11-12T19:08:57.352955: step 1997, loss 0.0293739, acc 1
2016-11-12T19:08:57.409569: step 1998, loss 0.0186678, acc 1
2016-11-12T19:08:57.467576: step 1999, loss 0.0625134, acc 0.96875
2016-11-12T19:08:57.525535: step 2000, loss 0.0492719, acc 0.984375

Evaluation:
2016-11-12T19:08:57.598254: step 2000, loss 1.41739, acc 0.546

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-2000

2016-11-12T19:08:58.094895: step 2001, loss 0.0374916, acc 1
2016-11-12T19:08:58.151693: step 2002, loss 0.0142431, acc 1
2016-11-12T19:08:58.210157: step 2003, loss 0.0334433, acc 0.984375
2016-11-12T19:08:58.270076: step 2004, loss 0.0576436, acc 0.984375
2016-11-12T19:08:58.328040: step 2005, loss 0.0160549, acc 1
2016-11-12T19:08:58.384040: step 2006, loss 0.0278033, acc 0.984375
2016-11-12T19:08:58.440510: step 2007, loss 0.0421162, acc 0.984375
2016-11-12T19:08:58.498448: step 2008, loss 0.0355371, acc 1
2016-11-12T19:08:58.553837: step 2009, loss 0.0293814, acc 1
2016-11-12T19:08:58.612870: step 2010, loss 0.0406931, acc 0.96875
2016-11-12T19:08:58.669465: step 2011, loss 0.0418202, acc 0.984375
2016-11-12T19:08:58.726607: step 2012, loss 0.0165772, acc 1
2016-11-12T19:08:58.784095: step 2013, loss 0.0194778, acc 1
2016-11-12T19:08:58.839595: step 2014, loss 0.0431121, acc 0.984375
2016-11-12T19:08:58.898193: step 2015, loss 0.0150987, acc 1
2016-11-12T19:08:58.957187: step 2016, loss 0.0524613, acc 0.96875
2016-11-12T19:08:59.014357: step 2017, loss 0.033068, acc 0.984375
2016-11-12T19:08:59.070686: step 2018, loss 0.0616297, acc 0.96875
2016-11-12T19:08:59.127730: step 2019, loss 0.0132863, acc 1
2016-11-12T19:08:59.184625: step 2020, loss 0.0350308, acc 0.984375
2016-11-12T19:08:59.241233: step 2021, loss 0.027585, acc 0.984375
2016-11-12T19:08:59.297525: step 2022, loss 0.012264, acc 1
2016-11-12T19:08:59.353681: step 2023, loss 0.0229501, acc 1
2016-11-12T19:08:59.411775: step 2024, loss 0.0541947, acc 0.984375
2016-11-12T19:08:59.468962: step 2025, loss 0.0436808, acc 0.984375
2016-11-12T19:08:59.524617: step 2026, loss 0.0189632, acc 1
2016-11-12T19:08:59.580015: step 2027, loss 0.0280039, acc 1
2016-11-12T19:08:59.635455: step 2028, loss 0.0563413, acc 0.984375
2016-11-12T19:08:59.691496: step 2029, loss 0.03184, acc 0.984375
2016-11-12T19:08:59.748180: step 2030, loss 0.10469, acc 0.96875
2016-11-12T19:08:59.805010: step 2031, loss 0.0561397, acc 0.984375
2016-11-12T19:08:59.860299: step 2032, loss 0.0651634, acc 0.984375
2016-11-12T19:08:59.916431: step 2033, loss 0.0283419, acc 1
2016-11-12T19:08:59.972626: step 2034, loss 0.0197401, acc 1
2016-11-12T19:09:00.028485: step 2035, loss 0.0210804, acc 1
2016-11-12T19:09:00.085270: step 2036, loss 0.0436471, acc 0.96875
2016-11-12T19:09:00.141576: step 2037, loss 0.0493278, acc 0.984375
2016-11-12T19:09:00.197674: step 2038, loss 0.0566779, acc 0.984375
2016-11-12T19:09:00.257038: step 2039, loss 0.0450644, acc 0.984375
2016-11-12T19:09:00.314086: step 2040, loss 0.0369379, acc 0.984375
2016-11-12T19:09:00.371773: step 2041, loss 0.0452967, acc 0.984375
2016-11-12T19:09:00.429439: step 2042, loss 0.0406802, acc 0.984375
2016-11-12T19:09:00.487140: step 2043, loss 0.0189121, acc 1
2016-11-12T19:09:00.543984: step 2044, loss 0.0516475, acc 0.984375
2016-11-12T19:09:00.602718: step 2045, loss 0.0297119, acc 1
2016-11-12T19:09:00.660853: step 2046, loss 0.00677549, acc 1
2016-11-12T19:09:00.717746: step 2047, loss 0.113876, acc 0.96875
2016-11-12T19:09:00.773779: step 2048, loss 0.154986, acc 0.9375
2016-11-12T19:09:00.832600: step 2049, loss 0.0340589, acc 1
2016-11-12T19:09:00.890197: step 2050, loss 0.0377667, acc 0.984375
2016-11-12T19:09:00.947592: step 2051, loss 0.0342762, acc 1
2016-11-12T19:09:01.005893: step 2052, loss 0.0604592, acc 0.96875
2016-11-12T19:09:01.061243: step 2053, loss 0.0350686, acc 0.984375
2016-11-12T19:09:01.118814: step 2054, loss 0.0251218, acc 0.984375
2016-11-12T19:09:01.177199: step 2055, loss 0.0609904, acc 0.96875
2016-11-12T19:09:01.237151: step 2056, loss 0.0545514, acc 0.984375
2016-11-12T19:09:01.293384: step 2057, loss 0.0357597, acc 0.984375
2016-11-12T19:09:01.349788: step 2058, loss 0.0681063, acc 0.96875
2016-11-12T19:09:01.386647: step 2059, loss 0.00730303, acc 1
2016-11-12T19:09:01.444529: step 2060, loss 0.0315436, acc 0.984375
2016-11-12T19:09:01.500896: step 2061, loss 0.0157266, acc 1
2016-11-12T19:09:01.557189: step 2062, loss 0.0202913, acc 1
2016-11-12T19:09:01.613250: step 2063, loss 0.0151148, acc 1
2016-11-12T19:09:01.669053: step 2064, loss 0.0892321, acc 0.96875
2016-11-12T19:09:01.725301: step 2065, loss 0.0206248, acc 1
2016-11-12T19:09:01.781375: step 2066, loss 0.0415827, acc 0.96875
2016-11-12T19:09:01.837424: step 2067, loss 0.0149819, acc 1
2016-11-12T19:09:01.893316: step 2068, loss 0.015389, acc 1
2016-11-12T19:09:01.951219: step 2069, loss 0.0102564, acc 1
2016-11-12T19:09:02.010036: step 2070, loss 0.0151315, acc 1
2016-11-12T19:09:02.065662: step 2071, loss 0.0920042, acc 0.953125
2016-11-12T19:09:02.121439: step 2072, loss 0.0322962, acc 0.984375
2016-11-12T19:09:02.181544: step 2073, loss 0.0625375, acc 0.984375
2016-11-12T19:09:02.241121: step 2074, loss 0.0170818, acc 1
2016-11-12T19:09:02.297297: step 2075, loss 0.0502917, acc 0.984375
2016-11-12T19:09:02.355963: step 2076, loss 0.0609169, acc 0.96875
2016-11-12T19:09:02.411809: step 2077, loss 0.0274423, acc 1
2016-11-12T19:09:02.468116: step 2078, loss 0.0132785, acc 1
2016-11-12T19:09:02.525570: step 2079, loss 0.0636238, acc 0.984375
2016-11-12T19:09:02.582846: step 2080, loss 0.0617449, acc 0.96875
2016-11-12T19:09:02.640416: step 2081, loss 0.059679, acc 0.96875
2016-11-12T19:09:02.696828: step 2082, loss 0.0606555, acc 0.96875
2016-11-12T19:09:02.752867: step 2083, loss 0.0484692, acc 0.984375
2016-11-12T19:09:02.808590: step 2084, loss 0.0590646, acc 0.984375
2016-11-12T19:09:02.865405: step 2085, loss 0.0616268, acc 0.984375
2016-11-12T19:09:02.924236: step 2086, loss 0.0458284, acc 0.984375
2016-11-12T19:09:02.983324: step 2087, loss 0.22089, acc 0.9375
2016-11-12T19:09:03.039728: step 2088, loss 0.0931495, acc 0.96875
2016-11-12T19:09:03.098132: step 2089, loss 0.022987, acc 0.984375
2016-11-12T19:09:03.155930: step 2090, loss 0.043821, acc 0.984375
2016-11-12T19:09:03.211956: step 2091, loss 0.0149464, acc 1
2016-11-12T19:09:03.268728: step 2092, loss 0.0396946, acc 0.984375
2016-11-12T19:09:03.326781: step 2093, loss 0.0198167, acc 1
2016-11-12T19:09:03.385374: step 2094, loss 0.0298504, acc 1
2016-11-12T19:09:03.444082: step 2095, loss 0.0274167, acc 1
2016-11-12T19:09:03.503091: step 2096, loss 0.071009, acc 0.984375
2016-11-12T19:09:03.560163: step 2097, loss 0.0158781, acc 1
2016-11-12T19:09:03.616357: step 2098, loss 0.0127507, acc 1
2016-11-12T19:09:03.672557: step 2099, loss 0.0346528, acc 0.984375
2016-11-12T19:09:03.730003: step 2100, loss 0.0159445, acc 1

Evaluation:
2016-11-12T19:09:03.799808: step 2100, loss 1.46627, acc 0.552

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-2100

2016-11-12T19:09:04.294614: step 2101, loss 0.0897119, acc 0.9375
2016-11-12T19:09:04.351580: step 2102, loss 0.0215161, acc 1
2016-11-12T19:09:04.409839: step 2103, loss 0.0664198, acc 0.96875
2016-11-12T19:09:04.467597: step 2104, loss 0.0465857, acc 1
2016-11-12T19:09:04.522533: step 2105, loss 0.0870506, acc 0.953125
2016-11-12T19:09:04.578189: step 2106, loss 0.0361485, acc 0.984375
2016-11-12T19:09:04.634092: step 2107, loss 0.0288147, acc 0.984375
2016-11-12T19:09:04.692485: step 2108, loss 0.0273803, acc 1
2016-11-12T19:09:04.750488: step 2109, loss 0.0161792, acc 1
2016-11-12T19:09:04.805915: step 2110, loss 0.0644459, acc 0.96875
2016-11-12T19:09:04.865558: step 2111, loss 0.0269405, acc 1
2016-11-12T19:09:04.921190: step 2112, loss 0.0218875, acc 1
2016-11-12T19:09:04.977415: step 2113, loss 0.00513878, acc 1
2016-11-12T19:09:05.034718: step 2114, loss 0.0274013, acc 0.984375
2016-11-12T19:09:05.090513: step 2115, loss 0.0960377, acc 0.953125
2016-11-12T19:09:05.147264: step 2116, loss 0.0839096, acc 0.984375
2016-11-12T19:09:05.203543: step 2117, loss 0.0891848, acc 0.953125
2016-11-12T19:09:05.260993: step 2118, loss 0.0406699, acc 0.984375
2016-11-12T19:09:05.318072: step 2119, loss 0.011828, acc 1
2016-11-12T19:09:05.376165: step 2120, loss 0.0206522, acc 1
2016-11-12T19:09:05.433392: step 2121, loss 0.026937, acc 0.984375
2016-11-12T19:09:05.489492: step 2122, loss 0.0302013, acc 1
2016-11-12T19:09:05.544521: step 2123, loss 0.0495815, acc 0.96875
2016-11-12T19:09:05.600610: step 2124, loss 0.0617105, acc 0.96875
2016-11-12T19:09:05.657443: step 2125, loss 0.0318695, acc 0.984375
2016-11-12T19:09:05.714124: step 2126, loss 0.0180916, acc 1
2016-11-12T19:09:05.769580: step 2127, loss 0.0816454, acc 0.953125
2016-11-12T19:09:05.828279: step 2128, loss 0.0176466, acc 1
2016-11-12T19:09:05.888317: step 2129, loss 0.0277411, acc 1
2016-11-12T19:09:05.927123: step 2130, loss 0.0258971, acc 1
2016-11-12T19:09:05.984037: step 2131, loss 0.0380727, acc 0.984375
2016-11-12T19:09:06.042555: step 2132, loss 0.0810629, acc 0.96875
2016-11-12T19:09:06.099414: step 2133, loss 0.0151352, acc 1
2016-11-12T19:09:06.155729: step 2134, loss 0.0382221, acc 0.984375
2016-11-12T19:09:06.212145: step 2135, loss 0.0259796, acc 0.984375
2016-11-12T19:09:06.270358: step 2136, loss 0.0509245, acc 0.984375
2016-11-12T19:09:06.329321: step 2137, loss 0.0186983, acc 1
2016-11-12T19:09:06.386406: step 2138, loss 0.0178197, acc 1
2016-11-12T19:09:06.443084: step 2139, loss 0.0401555, acc 0.984375
2016-11-12T19:09:06.499776: step 2140, loss 0.0289526, acc 0.984375
2016-11-12T19:09:06.557425: step 2141, loss 0.0441373, acc 0.984375
2016-11-12T19:09:06.616366: step 2142, loss 0.0890524, acc 0.96875
2016-11-12T19:09:06.673205: step 2143, loss 0.0184849, acc 1
2016-11-12T19:09:06.729471: step 2144, loss 0.0247427, acc 1
2016-11-12T19:09:06.785970: step 2145, loss 0.0716837, acc 0.96875
2016-11-12T19:09:06.843271: step 2146, loss 0.0392763, acc 1
2016-11-12T19:09:06.899016: step 2147, loss 0.0130961, acc 1
2016-11-12T19:09:06.957461: step 2148, loss 0.0127787, acc 1
2016-11-12T19:09:07.015078: step 2149, loss 0.0292782, acc 0.984375
2016-11-12T19:09:07.070909: step 2150, loss 0.0610823, acc 0.953125
2016-11-12T19:09:07.126924: step 2151, loss 0.125042, acc 0.984375
2016-11-12T19:09:07.185661: step 2152, loss 0.0444292, acc 0.96875
2016-11-12T19:09:07.242415: step 2153, loss 0.0336478, acc 0.984375
2016-11-12T19:09:07.300574: step 2154, loss 0.0433369, acc 0.984375
2016-11-12T19:09:07.357277: step 2155, loss 0.0325107, acc 0.984375
2016-11-12T19:09:07.413391: step 2156, loss 0.0370965, acc 1
2016-11-12T19:09:07.469694: step 2157, loss 0.0415981, acc 0.984375
2016-11-12T19:09:07.528820: step 2158, loss 0.0225564, acc 1
2016-11-12T19:09:07.584835: step 2159, loss 0.0127812, acc 1
2016-11-12T19:09:07.640608: step 2160, loss 0.0203912, acc 1
2016-11-12T19:09:07.696752: step 2161, loss 0.046745, acc 0.984375
2016-11-12T19:09:07.753448: step 2162, loss 0.0408168, acc 0.984375
2016-11-12T19:09:07.811381: step 2163, loss 0.0138114, acc 1
2016-11-12T19:09:07.868663: step 2164, loss 0.0152301, acc 1
2016-11-12T19:09:07.929527: step 2165, loss 0.0131972, acc 1
2016-11-12T19:09:07.985924: step 2166, loss 0.0127976, acc 1
2016-11-12T19:09:08.041985: step 2167, loss 0.022771, acc 1
2016-11-12T19:09:08.098774: step 2168, loss 0.0302841, acc 0.984375
2016-11-12T19:09:08.154733: step 2169, loss 0.0251115, acc 1
2016-11-12T19:09:08.213495: step 2170, loss 0.050597, acc 0.96875
2016-11-12T19:09:08.270992: step 2171, loss 0.0325137, acc 0.984375
2016-11-12T19:09:08.328858: step 2172, loss 0.0278799, acc 1
2016-11-12T19:09:08.385834: step 2173, loss 0.0332295, acc 0.984375
2016-11-12T19:09:08.442423: step 2174, loss 0.0168227, acc 1
2016-11-12T19:09:08.498359: step 2175, loss 0.030475, acc 0.984375
2016-11-12T19:09:08.554232: step 2176, loss 0.017868, acc 1
2016-11-12T19:09:08.609731: step 2177, loss 0.0162002, acc 1
2016-11-12T19:09:08.665641: step 2178, loss 0.0814859, acc 0.953125
2016-11-12T19:09:08.721612: step 2179, loss 0.0590228, acc 0.96875
2016-11-12T19:09:08.779659: step 2180, loss 0.0638683, acc 0.984375
2016-11-12T19:09:08.835434: step 2181, loss 0.0170061, acc 1
2016-11-12T19:09:08.893367: step 2182, loss 0.0365301, acc 0.984375
2016-11-12T19:09:08.950737: step 2183, loss 0.0798658, acc 0.96875
2016-11-12T19:09:09.007387: step 2184, loss 0.058568, acc 0.984375
2016-11-12T19:09:09.065835: step 2185, loss 0.0289758, acc 0.984375
2016-11-12T19:09:09.123972: step 2186, loss 0.0587631, acc 0.96875
2016-11-12T19:09:09.181325: step 2187, loss 0.0335673, acc 0.984375
2016-11-12T19:09:09.237498: step 2188, loss 0.0589558, acc 0.984375
2016-11-12T19:09:09.297113: step 2189, loss 0.0432927, acc 0.96875
2016-11-12T19:09:09.354800: step 2190, loss 0.0350242, acc 0.96875
2016-11-12T19:09:09.413197: step 2191, loss 0.0510384, acc 0.984375
2016-11-12T19:09:09.468494: step 2192, loss 0.0201628, acc 1
2016-11-12T19:09:09.525146: step 2193, loss 0.0317665, acc 1
2016-11-12T19:09:09.580894: step 2194, loss 0.0268091, acc 1
2016-11-12T19:09:09.638526: step 2195, loss 0.0191, acc 1
2016-11-12T19:09:09.696919: step 2196, loss 0.0509897, acc 0.96875
2016-11-12T19:09:09.753418: step 2197, loss 0.0125177, acc 1
2016-11-12T19:09:09.809584: step 2198, loss 0.0213296, acc 1
2016-11-12T19:09:09.867522: step 2199, loss 0.0510645, acc 0.984375
2016-11-12T19:09:09.925268: step 2200, loss 0.0233159, acc 0.984375

Evaluation:
2016-11-12T19:09:09.998128: step 2200, loss 1.51749, acc 0.56

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-2200

2016-11-12T19:09:10.475588: step 2201, loss 0.00446018, acc 1
2016-11-12T19:09:10.533642: step 2202, loss 0.0987451, acc 0.953125
2016-11-12T19:09:10.588989: step 2203, loss 0.0195625, acc 1
2016-11-12T19:09:10.646113: step 2204, loss 0.0173372, acc 1
2016-11-12T19:09:10.702279: step 2205, loss 0.00617531, acc 1
2016-11-12T19:09:10.761272: step 2206, loss 0.024993, acc 0.984375
2016-11-12T19:09:10.817643: step 2207, loss 0.0222146, acc 0.984375
2016-11-12T19:09:10.873850: step 2208, loss 0.00872015, acc 1
2016-11-12T19:09:10.930211: step 2209, loss 0.0317154, acc 0.984375
2016-11-12T19:09:10.987246: step 2210, loss 0.0169519, acc 1
2016-11-12T19:09:11.045933: step 2211, loss 0.0744145, acc 0.96875
2016-11-12T19:09:11.103170: step 2212, loss 0.0425417, acc 0.96875
2016-11-12T19:09:11.159748: step 2213, loss 0.0474756, acc 0.984375
2016-11-12T19:09:11.215612: step 2214, loss 0.0273642, acc 1
2016-11-12T19:09:11.271705: step 2215, loss 0.00934236, acc 1
2016-11-12T19:09:11.328244: step 2216, loss 0.0349657, acc 1
2016-11-12T19:09:11.386013: step 2217, loss 0.0339963, acc 1
2016-11-12T19:09:11.441767: step 2218, loss 0.0226276, acc 1
2016-11-12T19:09:11.500653: step 2219, loss 0.071695, acc 0.984375
2016-11-12T19:09:11.557926: step 2220, loss 0.0167883, acc 1
2016-11-12T19:09:11.614272: step 2221, loss 0.026653, acc 1
2016-11-12T19:09:11.673762: step 2222, loss 0.051031, acc 0.953125
2016-11-12T19:09:11.732659: step 2223, loss 0.058577, acc 0.96875
2016-11-12T19:09:11.789913: step 2224, loss 0.0288882, acc 1
2016-11-12T19:09:11.846059: step 2225, loss 0.0309531, acc 1
2016-11-12T19:09:11.904971: step 2226, loss 0.058776, acc 0.96875
2016-11-12T19:09:11.961422: step 2227, loss 0.0664283, acc 0.984375
2016-11-12T19:09:12.017653: step 2228, loss 0.10289, acc 0.96875
2016-11-12T19:09:12.074817: step 2229, loss 0.0142159, acc 1
2016-11-12T19:09:12.130461: step 2230, loss 0.0180531, acc 1
2016-11-12T19:09:12.187374: step 2231, loss 0.0246079, acc 0.984375
2016-11-12T19:09:12.244491: step 2232, loss 0.0268336, acc 0.984375
2016-11-12T19:09:12.301481: step 2233, loss 0.0326916, acc 0.984375
2016-11-12T19:09:12.358278: step 2234, loss 0.00647816, acc 1
2016-11-12T19:09:12.416446: step 2235, loss 0.0682305, acc 0.96875
2016-11-12T19:09:12.472306: step 2236, loss 0.0464126, acc 0.96875
2016-11-12T19:09:12.529541: step 2237, loss 0.0117446, acc 1
2016-11-12T19:09:12.587855: step 2238, loss 0.167623, acc 0.9375
2016-11-12T19:09:12.644833: step 2239, loss 0.0220783, acc 0.984375
2016-11-12T19:09:12.701050: step 2240, loss 0.0211814, acc 1
2016-11-12T19:09:12.760070: step 2241, loss 0.00660183, acc 1
2016-11-12T19:09:12.816897: step 2242, loss 0.0670768, acc 0.96875
2016-11-12T19:09:12.873486: step 2243, loss 0.0245265, acc 1
2016-11-12T19:09:12.929137: step 2244, loss 0.0242503, acc 1
2016-11-12T19:09:12.985814: step 2245, loss 0.0342514, acc 1
2016-11-12T19:09:13.042823: step 2246, loss 0.0258464, acc 0.984375
2016-11-12T19:09:13.100280: step 2247, loss 0.0277554, acc 1
2016-11-12T19:09:13.157491: step 2248, loss 0.0109901, acc 1
2016-11-12T19:09:13.215295: step 2249, loss 0.0203555, acc 1
2016-11-12T19:09:13.271525: step 2250, loss 0.0250924, acc 0.984375
2016-11-12T19:09:13.329449: step 2251, loss 0.0250442, acc 1
2016-11-12T19:09:13.387375: step 2252, loss 0.0758746, acc 0.96875
2016-11-12T19:09:13.445365: step 2253, loss 0.0258812, acc 0.984375
2016-11-12T19:09:13.503564: step 2254, loss 0.0222711, acc 1
2016-11-12T19:09:13.559709: step 2255, loss 0.0315682, acc 0.984375
2016-11-12T19:09:13.616767: step 2256, loss 0.0110539, acc 1
2016-11-12T19:09:13.675553: step 2257, loss 0.0892081, acc 0.984375
2016-11-12T19:09:13.731378: step 2258, loss 0.011665, acc 1
2016-11-12T19:09:13.789559: step 2259, loss 0.0164131, acc 1
2016-11-12T19:09:13.846678: step 2260, loss 0.0583222, acc 0.984375
2016-11-12T19:09:13.905027: step 2261, loss 0.0462004, acc 0.984375
2016-11-12T19:09:13.961713: step 2262, loss 0.0750383, acc 0.96875
2016-11-12T19:09:14.018983: step 2263, loss 0.0116419, acc 1
2016-11-12T19:09:14.080289: step 2264, loss 0.024416, acc 1
2016-11-12T19:09:14.137846: step 2265, loss 0.0193243, acc 1
2016-11-12T19:09:14.193782: step 2266, loss 0.014244, acc 1
2016-11-12T19:09:14.254936: step 2267, loss 0.0163975, acc 1
2016-11-12T19:09:14.312911: step 2268, loss 0.0139208, acc 1
2016-11-12T19:09:14.368849: step 2269, loss 0.0664141, acc 0.984375
2016-11-12T19:09:14.426415: step 2270, loss 0.0243425, acc 0.984375
2016-11-12T19:09:14.485237: step 2271, loss 0.0176329, acc 1
2016-11-12T19:09:14.522747: step 2272, loss 0.00488636, acc 1
2016-11-12T19:09:14.581251: step 2273, loss 0.0299913, acc 1
2016-11-12T19:09:14.638259: step 2274, loss 0.0280018, acc 1
2016-11-12T19:09:14.695306: step 2275, loss 0.0500148, acc 0.984375
2016-11-12T19:09:14.753575: step 2276, loss 0.0143672, acc 1
2016-11-12T19:09:14.809077: step 2277, loss 0.0163849, acc 1
2016-11-12T19:09:14.868258: step 2278, loss 0.0269254, acc 0.984375
2016-11-12T19:09:14.925210: step 2279, loss 0.050486, acc 0.984375
2016-11-12T19:09:14.982249: step 2280, loss 0.0720631, acc 0.96875
2016-11-12T19:09:15.038816: step 2281, loss 0.0225772, acc 1
2016-11-12T19:09:15.097023: step 2282, loss 0.0166217, acc 1
2016-11-12T19:09:15.154259: step 2283, loss 0.0308676, acc 0.984375
2016-11-12T19:09:15.212649: step 2284, loss 0.0376648, acc 0.984375
2016-11-12T19:09:15.268626: step 2285, loss 0.00845259, acc 1
2016-11-12T19:09:15.326221: step 2286, loss 0.0191656, acc 1
2016-11-12T19:09:15.384439: step 2287, loss 0.0139091, acc 1
2016-11-12T19:09:15.442776: step 2288, loss 0.0106656, acc 1
2016-11-12T19:09:15.501434: step 2289, loss 0.0234837, acc 1
2016-11-12T19:09:15.562027: step 2290, loss 0.0092641, acc 1
2016-11-12T19:09:15.619918: step 2291, loss 0.019002, acc 1
2016-11-12T19:09:15.675183: step 2292, loss 0.0155485, acc 1
2016-11-12T19:09:15.732278: step 2293, loss 0.033334, acc 0.984375
2016-11-12T19:09:15.788751: step 2294, loss 0.026841, acc 1
2016-11-12T19:09:15.845822: step 2295, loss 0.0616594, acc 0.96875
2016-11-12T19:09:15.904967: step 2296, loss 0.0069589, acc 1
2016-11-12T19:09:15.960716: step 2297, loss 0.0481543, acc 0.984375
2016-11-12T19:09:16.019251: step 2298, loss 0.0463128, acc 0.984375
2016-11-12T19:09:16.076085: step 2299, loss 0.0538579, acc 0.96875
2016-11-12T19:09:16.135880: step 2300, loss 0.00978991, acc 1

Evaluation:
2016-11-12T19:09:16.205685: step 2300, loss 1.55807, acc 0.552

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-2300

2016-11-12T19:09:16.699114: step 2301, loss 0.0210075, acc 1
2016-11-12T19:09:16.757241: step 2302, loss 0.0301741, acc 1
2016-11-12T19:09:16.812909: step 2303, loss 0.0398789, acc 0.96875
2016-11-12T19:09:16.868520: step 2304, loss 0.0181099, acc 1
2016-11-12T19:09:16.923579: step 2305, loss 0.0415114, acc 0.984375
2016-11-12T19:09:16.981601: step 2306, loss 0.0137726, acc 1
2016-11-12T19:09:17.037001: step 2307, loss 0.00713466, acc 1
2016-11-12T19:09:17.092901: step 2308, loss 0.0127104, acc 1
2016-11-12T19:09:17.149433: step 2309, loss 0.0732356, acc 0.984375
2016-11-12T19:09:17.206049: step 2310, loss 0.0360961, acc 0.984375
2016-11-12T19:09:17.262901: step 2311, loss 0.0179264, acc 1
2016-11-12T19:09:17.320886: step 2312, loss 0.0204444, acc 1
2016-11-12T19:09:17.379097: step 2313, loss 0.0403766, acc 1
2016-11-12T19:09:17.435407: step 2314, loss 0.0067722, acc 1
2016-11-12T19:09:17.490462: step 2315, loss 0.0196152, acc 1
2016-11-12T19:09:17.546993: step 2316, loss 0.00790232, acc 1
2016-11-12T19:09:17.605263: step 2317, loss 0.0236691, acc 1
2016-11-12T19:09:17.661673: step 2318, loss 0.0118046, acc 1
2016-11-12T19:09:17.718541: step 2319, loss 0.01244, acc 1
2016-11-12T19:09:17.777191: step 2320, loss 0.0578829, acc 0.984375
2016-11-12T19:09:17.833177: step 2321, loss 0.00781324, acc 1
2016-11-12T19:09:17.890419: step 2322, loss 0.0260586, acc 0.984375
2016-11-12T19:09:17.949921: step 2323, loss 0.0277627, acc 0.984375
2016-11-12T19:09:18.007085: step 2324, loss 0.0637648, acc 0.984375
2016-11-12T19:09:18.064687: step 2325, loss 0.0897752, acc 0.984375
2016-11-12T19:09:18.121348: step 2326, loss 0.00890222, acc 1
2016-11-12T19:09:18.180815: step 2327, loss 0.0292413, acc 1
2016-11-12T19:09:18.237254: step 2328, loss 0.015701, acc 1
2016-11-12T19:09:18.294265: step 2329, loss 0.0588518, acc 0.984375
2016-11-12T19:09:18.351748: step 2330, loss 0.0264326, acc 0.984375
2016-11-12T19:09:18.408462: step 2331, loss 0.00722137, acc 1
2016-11-12T19:09:18.465303: step 2332, loss 0.0781652, acc 0.953125
2016-11-12T19:09:18.522651: step 2333, loss 0.0611148, acc 0.984375
2016-11-12T19:09:18.579452: step 2334, loss 0.0297396, acc 1
2016-11-12T19:09:18.635970: step 2335, loss 0.00845011, acc 1
2016-11-12T19:09:18.691982: step 2336, loss 0.0167725, acc 1
2016-11-12T19:09:18.748367: step 2337, loss 0.0141589, acc 1
2016-11-12T19:09:18.804355: step 2338, loss 0.0197727, acc 1
2016-11-12T19:09:18.861794: step 2339, loss 0.0212258, acc 1
2016-11-12T19:09:18.917845: step 2340, loss 0.0422771, acc 0.984375
2016-11-12T19:09:18.977567: step 2341, loss 0.0265977, acc 0.984375
2016-11-12T19:09:19.034003: step 2342, loss 0.0306702, acc 0.984375
2016-11-12T19:09:19.071935: step 2343, loss 0.017384, acc 1
2016-11-12T19:09:19.133018: step 2344, loss 0.0160376, acc 1
2016-11-12T19:09:19.190104: step 2345, loss 0.0184166, acc 1
2016-11-12T19:09:19.248674: step 2346, loss 0.0376634, acc 0.984375
2016-11-12T19:09:19.309307: step 2347, loss 0.0129169, acc 1
2016-11-12T19:09:19.367637: step 2348, loss 0.0130656, acc 1
2016-11-12T19:09:19.425669: step 2349, loss 0.0119811, acc 1
2016-11-12T19:09:19.481412: step 2350, loss 0.0470865, acc 0.984375
2016-11-12T19:09:19.538081: step 2351, loss 0.0232852, acc 1
2016-11-12T19:09:19.595852: step 2352, loss 0.099548, acc 0.96875
2016-11-12T19:09:19.651966: step 2353, loss 0.0327213, acc 1
2016-11-12T19:09:19.709894: step 2354, loss 0.00407231, acc 1
2016-11-12T19:09:19.768838: step 2355, loss 0.0124373, acc 1
2016-11-12T19:09:19.825573: step 2356, loss 0.0058825, acc 1
2016-11-12T19:09:19.881938: step 2357, loss 0.029707, acc 0.96875
2016-11-12T19:09:19.940812: step 2358, loss 0.0290489, acc 0.984375
2016-11-12T19:09:19.998155: step 2359, loss 0.0148399, acc 1
2016-11-12T19:09:20.056557: step 2360, loss 0.0383144, acc 0.984375
2016-11-12T19:09:20.113226: step 2361, loss 0.0288817, acc 1
2016-11-12T19:09:20.173446: step 2362, loss 0.00537148, acc 1
2016-11-12T19:09:20.229215: step 2363, loss 0.0417739, acc 0.984375
2016-11-12T19:09:20.286794: step 2364, loss 0.0132409, acc 1
2016-11-12T19:09:20.343164: step 2365, loss 0.0319071, acc 0.984375
2016-11-12T19:09:20.401074: step 2366, loss 0.0396738, acc 0.96875
2016-11-12T19:09:20.457940: step 2367, loss 0.0366501, acc 0.984375
2016-11-12T19:09:20.516174: step 2368, loss 0.0165734, acc 1
2016-11-12T19:09:20.572011: step 2369, loss 0.0219953, acc 1
2016-11-12T19:09:20.629346: step 2370, loss 0.00890274, acc 1
2016-11-12T19:09:20.688229: step 2371, loss 0.0412497, acc 0.984375
2016-11-12T19:09:20.745360: step 2372, loss 0.024025, acc 1
2016-11-12T19:09:20.804857: step 2373, loss 0.0219963, acc 0.984375
2016-11-12T19:09:20.861439: step 2374, loss 0.0158614, acc 1
2016-11-12T19:09:20.917954: step 2375, loss 0.0353732, acc 0.984375
2016-11-12T19:09:20.975389: step 2376, loss 0.0195535, acc 1
2016-11-12T19:09:21.034230: step 2377, loss 0.0307817, acc 0.984375
2016-11-12T19:09:21.091566: step 2378, loss 0.0254388, acc 1
2016-11-12T19:09:21.149436: step 2379, loss 0.0312847, acc 1
2016-11-12T19:09:21.207548: step 2380, loss 0.0223601, acc 1
2016-11-12T19:09:21.264782: step 2381, loss 0.0104414, acc 1
2016-11-12T19:09:21.321491: step 2382, loss 0.0403336, acc 0.96875
2016-11-12T19:09:21.379671: step 2383, loss 0.0262099, acc 1
2016-11-12T19:09:21.435570: step 2384, loss 0.0162546, acc 1
2016-11-12T19:09:21.492290: step 2385, loss 0.0108719, acc 1
2016-11-12T19:09:21.549159: step 2386, loss 0.0424926, acc 0.96875
2016-11-12T19:09:21.608154: step 2387, loss 0.0095545, acc 1
2016-11-12T19:09:21.665388: step 2388, loss 0.0118038, acc 1
2016-11-12T19:09:21.721451: step 2389, loss 0.0943181, acc 0.96875
2016-11-12T19:09:21.776918: step 2390, loss 0.00945621, acc 1
2016-11-12T19:09:21.832717: step 2391, loss 0.0294608, acc 0.984375
2016-11-12T19:09:21.889707: step 2392, loss 0.00527455, acc 1
2016-11-12T19:09:21.945029: step 2393, loss 0.0104087, acc 1
2016-11-12T19:09:22.001806: step 2394, loss 0.0594295, acc 0.984375
2016-11-12T19:09:22.064089: step 2395, loss 0.0231492, acc 0.984375
2016-11-12T19:09:22.121775: step 2396, loss 0.0516662, acc 0.96875
2016-11-12T19:09:22.179333: step 2397, loss 0.0546714, acc 0.984375
2016-11-12T19:09:22.236379: step 2398, loss 0.018496, acc 1
2016-11-12T19:09:22.293572: step 2399, loss 0.0452134, acc 0.984375
2016-11-12T19:09:22.349138: step 2400, loss 0.0673207, acc 0.9375

Evaluation:
2016-11-12T19:09:22.420632: step 2400, loss 1.60903, acc 0.56

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-2400

2016-11-12T19:09:22.913291: step 2401, loss 0.02254, acc 0.984375
2016-11-12T19:09:22.970912: step 2402, loss 0.0658929, acc 0.96875
2016-11-12T19:09:23.029971: step 2403, loss 0.00906879, acc 1
2016-11-12T19:09:23.088976: step 2404, loss 0.00902726, acc 1
2016-11-12T19:09:23.145056: step 2405, loss 0.0137027, acc 1
2016-11-12T19:09:23.203623: step 2406, loss 0.0131757, acc 1
2016-11-12T19:09:23.260635: step 2407, loss 0.0114643, acc 1
2016-11-12T19:09:23.321974: step 2408, loss 0.0192205, acc 1
2016-11-12T19:09:23.378008: step 2409, loss 0.0198264, acc 1
2016-11-12T19:09:23.437184: step 2410, loss 0.0409452, acc 0.984375
2016-11-12T19:09:23.493803: step 2411, loss 0.06526, acc 0.984375
2016-11-12T19:09:23.551070: step 2412, loss 0.0276639, acc 0.984375
2016-11-12T19:09:23.607116: step 2413, loss 0.0302033, acc 0.984375
2016-11-12T19:09:23.644490: step 2414, loss 0.074667, acc 1
2016-11-12T19:09:23.701365: step 2415, loss 0.0174768, acc 1
2016-11-12T19:09:23.757618: step 2416, loss 0.0150308, acc 1
2016-11-12T19:09:23.815674: step 2417, loss 0.0243858, acc 1
2016-11-12T19:09:23.871580: step 2418, loss 0.0145744, acc 1
2016-11-12T19:09:23.928367: step 2419, loss 0.0298781, acc 0.984375
2016-11-12T19:09:23.985038: step 2420, loss 0.0157748, acc 1
2016-11-12T19:09:24.040709: step 2421, loss 0.0245704, acc 1
2016-11-12T19:09:24.098302: step 2422, loss 0.0061556, acc 1
2016-11-12T19:09:24.157533: step 2423, loss 0.0228429, acc 1
2016-11-12T19:09:24.213544: step 2424, loss 0.0265896, acc 1
2016-11-12T19:09:24.268909: step 2425, loss 0.0270739, acc 1
2016-11-12T19:09:24.329567: step 2426, loss 0.0336819, acc 1
2016-11-12T19:09:24.385959: step 2427, loss 0.0335548, acc 0.984375
2016-11-12T19:09:24.442391: step 2428, loss 0.0349423, acc 0.984375
2016-11-12T19:09:24.499453: step 2429, loss 0.0437445, acc 0.984375
2016-11-12T19:09:24.557392: step 2430, loss 0.0535398, acc 0.96875
2016-11-12T19:09:24.615991: step 2431, loss 0.0263933, acc 0.984375
2016-11-12T19:09:24.673443: step 2432, loss 0.0241576, acc 1
2016-11-12T19:09:24.729300: step 2433, loss 0.0443351, acc 0.984375
2016-11-12T19:09:24.785702: step 2434, loss 0.00570482, acc 1
2016-11-12T19:09:24.843717: step 2435, loss 0.00586814, acc 1
2016-11-12T19:09:24.900577: step 2436, loss 0.0219224, acc 1
2016-11-12T19:09:24.957450: step 2437, loss 0.00523541, acc 1
2016-11-12T19:09:25.015322: step 2438, loss 0.00410463, acc 1
2016-11-12T19:09:25.070996: step 2439, loss 0.0224812, acc 1
2016-11-12T19:09:25.128640: step 2440, loss 0.030199, acc 0.984375
2016-11-12T19:09:25.187022: step 2441, loss 0.0316517, acc 0.984375
2016-11-12T19:09:25.244088: step 2442, loss 0.0710741, acc 0.984375
2016-11-12T19:09:25.302562: step 2443, loss 0.0266073, acc 1
2016-11-12T19:09:25.361087: step 2444, loss 0.0431464, acc 0.984375
2016-11-12T19:09:25.418075: step 2445, loss 0.0256922, acc 1
2016-11-12T19:09:25.476801: step 2446, loss 0.00771094, acc 1
2016-11-12T19:09:25.533446: step 2447, loss 0.029964, acc 0.984375
2016-11-12T19:09:25.589902: step 2448, loss 0.0435147, acc 0.984375
2016-11-12T19:09:25.645703: step 2449, loss 0.0187701, acc 0.984375
2016-11-12T19:09:25.701138: step 2450, loss 0.0279022, acc 0.984375
2016-11-12T19:09:25.757762: step 2451, loss 0.0561311, acc 0.984375
2016-11-12T19:09:25.814798: step 2452, loss 0.0304728, acc 0.984375
2016-11-12T19:09:25.871064: step 2453, loss 0.00454104, acc 1
2016-11-12T19:09:25.928793: step 2454, loss 0.0338217, acc 0.984375
2016-11-12T19:09:25.986410: step 2455, loss 0.027392, acc 1
2016-11-12T19:09:26.043767: step 2456, loss 0.0145575, acc 1
2016-11-12T19:09:26.100467: step 2457, loss 0.0490646, acc 0.96875
2016-11-12T19:09:26.158033: step 2458, loss 0.040416, acc 0.96875
2016-11-12T19:09:26.213879: step 2459, loss 0.0508393, acc 0.96875
2016-11-12T19:09:26.270668: step 2460, loss 0.0337702, acc 0.984375
2016-11-12T19:09:26.327306: step 2461, loss 0.040564, acc 0.984375
2016-11-12T19:09:26.386180: step 2462, loss 0.0194739, acc 0.984375
2016-11-12T19:09:26.442648: step 2463, loss 0.0101074, acc 1
2016-11-12T19:09:26.501357: step 2464, loss 0.0290583, acc 0.984375
2016-11-12T19:09:26.559414: step 2465, loss 0.0335313, acc 0.984375
2016-11-12T19:09:26.617584: step 2466, loss 0.15194, acc 0.984375
2016-11-12T19:09:26.677697: step 2467, loss 0.0198736, acc 1
2016-11-12T19:09:26.734042: step 2468, loss 0.0237715, acc 0.984375
2016-11-12T19:09:26.792351: step 2469, loss 0.0262858, acc 0.984375
2016-11-12T19:09:26.849241: step 2470, loss 0.0322998, acc 0.984375
2016-11-12T19:09:26.905263: step 2471, loss 0.0150774, acc 1
2016-11-12T19:09:26.962206: step 2472, loss 0.0207378, acc 1
2016-11-12T19:09:27.019113: step 2473, loss 0.0168603, acc 1
2016-11-12T19:09:27.076005: step 2474, loss 0.0108025, acc 1
2016-11-12T19:09:27.132471: step 2475, loss 0.0370096, acc 0.984375
2016-11-12T19:09:27.189805: step 2476, loss 0.0628414, acc 0.96875
2016-11-12T19:09:27.246055: step 2477, loss 0.0234296, acc 0.984375
2016-11-12T19:09:27.303172: step 2478, loss 0.031764, acc 1
2016-11-12T19:09:27.362420: step 2479, loss 0.018652, acc 1
2016-11-12T19:09:27.419059: step 2480, loss 0.00554087, acc 1
2016-11-12T19:09:27.477349: step 2481, loss 0.0194864, acc 1
2016-11-12T19:09:27.533388: step 2482, loss 0.0541341, acc 0.984375
2016-11-12T19:09:27.591641: step 2483, loss 0.0432156, acc 0.984375
2016-11-12T19:09:27.650179: step 2484, loss 0.0357826, acc 0.984375
2016-11-12T19:09:27.689119: step 2485, loss 0.00932566, acc 1
2016-11-12T19:09:27.748403: step 2486, loss 0.0178336, acc 0.984375
2016-11-12T19:09:27.805520: step 2487, loss 0.06571, acc 0.96875
2016-11-12T19:09:27.861685: step 2488, loss 0.013539, acc 1
2016-11-12T19:09:27.917338: step 2489, loss 0.00901611, acc 1
2016-11-12T19:09:27.973277: step 2490, loss 0.0145324, acc 1
2016-11-12T19:09:28.029597: step 2491, loss 0.0145584, acc 1
2016-11-12T19:09:28.087625: step 2492, loss 0.0273424, acc 0.984375
2016-11-12T19:09:28.143920: step 2493, loss 0.00580474, acc 1
2016-11-12T19:09:28.201462: step 2494, loss 0.0288535, acc 1
2016-11-12T19:09:28.257702: step 2495, loss 0.066707, acc 0.984375
2016-11-12T19:09:28.313575: step 2496, loss 0.0145688, acc 1
2016-11-12T19:09:28.369558: step 2497, loss 0.0301501, acc 0.984375
2016-11-12T19:09:28.428775: step 2498, loss 0.0326203, acc 0.96875
2016-11-12T19:09:28.488485: step 2499, loss 0.0102701, acc 1
2016-11-12T19:09:28.544367: step 2500, loss 0.0287269, acc 0.984375

Evaluation:
2016-11-12T19:09:28.614972: step 2500, loss 1.67081, acc 0.57

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-2500

2016-11-12T19:09:29.110668: step 2501, loss 0.0708599, acc 0.96875
2016-11-12T19:09:29.167337: step 2502, loss 0.0136933, acc 1
2016-11-12T19:09:29.223099: step 2503, loss 0.0049269, acc 1
2016-11-12T19:09:29.281170: step 2504, loss 0.0190317, acc 1
2016-11-12T19:09:29.337238: step 2505, loss 0.0280781, acc 0.984375
2016-11-12T19:09:29.394884: step 2506, loss 0.021357, acc 0.984375
2016-11-12T19:09:29.451987: step 2507, loss 0.0469834, acc 0.96875
2016-11-12T19:09:29.509800: step 2508, loss 0.0334682, acc 0.984375
2016-11-12T19:09:29.568931: step 2509, loss 0.0415401, acc 0.96875
2016-11-12T19:09:29.626773: step 2510, loss 0.0119805, acc 1
2016-11-12T19:09:29.685735: step 2511, loss 0.0176755, acc 1
2016-11-12T19:09:29.742087: step 2512, loss 0.0177488, acc 1
2016-11-12T19:09:29.799370: step 2513, loss 0.0331429, acc 0.984375
2016-11-12T19:09:29.857364: step 2514, loss 0.0246296, acc 0.984375
2016-11-12T19:09:29.915611: step 2515, loss 0.0152563, acc 1
2016-11-12T19:09:29.973144: step 2516, loss 0.0232932, acc 1
2016-11-12T19:09:30.029579: step 2517, loss 0.0277088, acc 1
2016-11-12T19:09:30.085808: step 2518, loss 0.0068636, acc 1
2016-11-12T19:09:30.143707: step 2519, loss 0.0199564, acc 1
2016-11-12T19:09:30.200716: step 2520, loss 0.0348024, acc 0.984375
2016-11-12T19:09:30.257489: step 2521, loss 0.0175454, acc 1
2016-11-12T19:09:30.314094: step 2522, loss 0.0123553, acc 1
2016-11-12T19:09:30.373666: step 2523, loss 0.0115877, acc 1
2016-11-12T19:09:30.434092: step 2524, loss 0.0100311, acc 1
2016-11-12T19:09:30.492140: step 2525, loss 0.0203512, acc 1
2016-11-12T19:09:30.550347: step 2526, loss 0.00963445, acc 1
2016-11-12T19:09:30.609250: step 2527, loss 0.0188036, acc 1
2016-11-12T19:09:30.666197: step 2528, loss 0.00977732, acc 1
2016-11-12T19:09:30.721922: step 2529, loss 0.0180675, acc 1
2016-11-12T19:09:30.778050: step 2530, loss 0.0123959, acc 1
2016-11-12T19:09:30.836140: step 2531, loss 0.113479, acc 0.96875
2016-11-12T19:09:30.894098: step 2532, loss 0.0175836, acc 1
2016-11-12T19:09:30.953117: step 2533, loss 0.0124038, acc 1
2016-11-12T19:09:31.012273: step 2534, loss 0.0388957, acc 0.984375
2016-11-12T19:09:31.069535: step 2535, loss 0.0175042, acc 1
2016-11-12T19:09:31.126738: step 2536, loss 0.0141901, acc 1
2016-11-12T19:09:31.183044: step 2537, loss 0.00606664, acc 1
2016-11-12T19:09:31.238833: step 2538, loss 0.165488, acc 0.984375
2016-11-12T19:09:31.297699: step 2539, loss 0.0218228, acc 0.984375
2016-11-12T19:09:31.354587: step 2540, loss 0.0301991, acc 0.984375
2016-11-12T19:09:31.410930: step 2541, loss 0.0103879, acc 1
2016-11-12T19:09:31.468398: step 2542, loss 0.00828494, acc 1
2016-11-12T19:09:31.524566: step 2543, loss 0.0068489, acc 1
2016-11-12T19:09:31.581612: step 2544, loss 0.0386201, acc 0.984375
2016-11-12T19:09:31.638766: step 2545, loss 0.00456233, acc 1
2016-11-12T19:09:31.694456: step 2546, loss 0.0351325, acc 0.984375
2016-11-12T19:09:31.750296: step 2547, loss 0.0242639, acc 0.984375
2016-11-12T19:09:31.807822: step 2548, loss 0.0719834, acc 0.96875
2016-11-12T19:09:31.864397: step 2549, loss 0.0121941, acc 1
2016-11-12T19:09:31.920182: step 2550, loss 0.0193705, acc 1
2016-11-12T19:09:31.977762: step 2551, loss 0.0190152, acc 1
2016-11-12T19:09:32.033828: step 2552, loss 0.0201045, acc 1
2016-11-12T19:09:32.089546: step 2553, loss 0.00599157, acc 1
2016-11-12T19:09:32.145202: step 2554, loss 0.030978, acc 0.984375
2016-11-12T19:09:32.201605: step 2555, loss 0.0294808, acc 0.984375
2016-11-12T19:09:32.238344: step 2556, loss 0.00112939, acc 1
2016-11-12T19:09:32.294573: step 2557, loss 0.0166231, acc 1
2016-11-12T19:09:32.354117: step 2558, loss 0.0889979, acc 0.96875
2016-11-12T19:09:32.411069: step 2559, loss 0.0229601, acc 0.984375
2016-11-12T19:09:32.467659: step 2560, loss 0.0830387, acc 0.96875
2016-11-12T19:09:32.525984: step 2561, loss 0.0337315, acc 0.984375
2016-11-12T19:09:32.584355: step 2562, loss 0.0288001, acc 1
2016-11-12T19:09:32.640584: step 2563, loss 0.0180067, acc 1
2016-11-12T19:09:32.701202: step 2564, loss 0.0190564, acc 0.984375
2016-11-12T19:09:32.760590: step 2565, loss 0.0261006, acc 0.984375
2016-11-12T19:09:32.817342: step 2566, loss 0.0477703, acc 0.96875
2016-11-12T19:09:32.874212: step 2567, loss 0.0242796, acc 0.984375
2016-11-12T19:09:32.932835: step 2568, loss 0.00456369, acc 1
2016-11-12T19:09:32.989690: step 2569, loss 0.031941, acc 1
2016-11-12T19:09:33.046264: step 2570, loss 0.0274459, acc 1
2016-11-12T19:09:33.103111: step 2571, loss 0.0345616, acc 0.984375
2016-11-12T19:09:33.161489: step 2572, loss 0.021579, acc 0.984375
2016-11-12T19:09:33.217791: step 2573, loss 0.00815471, acc 1
2016-11-12T19:09:33.273101: step 2574, loss 0.0084719, acc 1
2016-11-12T19:09:33.331550: step 2575, loss 0.0174001, acc 1
2016-11-12T19:09:33.392145: step 2576, loss 0.0193837, acc 1
2016-11-12T19:09:33.447729: step 2577, loss 0.0102377, acc 1
2016-11-12T19:09:33.503880: step 2578, loss 0.0248872, acc 0.984375
2016-11-12T19:09:33.559209: step 2579, loss 0.0177423, acc 1
2016-11-12T19:09:33.616449: step 2580, loss 0.0122237, acc 1
2016-11-12T19:09:33.674173: step 2581, loss 0.0122428, acc 1
2016-11-12T19:09:33.731403: step 2582, loss 0.00579162, acc 1
2016-11-12T19:09:33.788992: step 2583, loss 0.0259444, acc 0.984375
2016-11-12T19:09:33.845469: step 2584, loss 0.0873609, acc 0.984375
2016-11-12T19:09:33.901027: step 2585, loss 0.00872528, acc 1
2016-11-12T19:09:33.957527: step 2586, loss 0.0234279, acc 1
2016-11-12T19:09:34.013918: step 2587, loss 0.0523551, acc 0.96875
2016-11-12T19:09:34.073854: step 2588, loss 0.00619749, acc 1
2016-11-12T19:09:34.131297: step 2589, loss 0.00479877, acc 1
2016-11-12T19:09:34.188370: step 2590, loss 0.00274253, acc 1
2016-11-12T19:09:34.245676: step 2591, loss 0.00601455, acc 1
2016-11-12T19:09:34.301546: step 2592, loss 0.0349996, acc 0.984375
2016-11-12T19:09:34.359755: step 2593, loss 0.0143391, acc 1
2016-11-12T19:09:34.416972: step 2594, loss 0.0145799, acc 1
2016-11-12T19:09:34.475333: step 2595, loss 0.0409914, acc 0.984375
2016-11-12T19:09:34.532114: step 2596, loss 0.0118559, acc 1
2016-11-12T19:09:34.589494: step 2597, loss 0.0113015, acc 1
2016-11-12T19:09:34.647246: step 2598, loss 0.00369575, acc 1
2016-11-12T19:09:34.705297: step 2599, loss 0.0445115, acc 0.984375
2016-11-12T19:09:34.762929: step 2600, loss 0.0285588, acc 1

Evaluation:
2016-11-12T19:09:34.833760: step 2600, loss 1.70586, acc 0.566

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-2600

2016-11-12T19:09:35.327595: step 2601, loss 0.083667, acc 0.953125
2016-11-12T19:09:35.385833: step 2602, loss 0.05484, acc 0.984375
2016-11-12T19:09:35.443529: step 2603, loss 0.0472192, acc 0.96875
2016-11-12T19:09:35.500368: step 2604, loss 0.0266659, acc 1
2016-11-12T19:09:35.555912: step 2605, loss 0.00461461, acc 1
2016-11-12T19:09:35.613482: step 2606, loss 0.00610375, acc 1
2016-11-12T19:09:35.670012: step 2607, loss 0.00949951, acc 1
2016-11-12T19:09:35.728266: step 2608, loss 0.00902876, acc 1
2016-11-12T19:09:35.785113: step 2609, loss 0.0188389, acc 1
2016-11-12T19:09:35.842554: step 2610, loss 0.00855326, acc 1
2016-11-12T19:09:35.898940: step 2611, loss 0.0145561, acc 1
2016-11-12T19:09:35.955349: step 2612, loss 0.0225116, acc 1
2016-11-12T19:09:36.013627: step 2613, loss 0.0196121, acc 0.984375
2016-11-12T19:09:36.072650: step 2614, loss 0.00429244, acc 1
2016-11-12T19:09:36.128875: step 2615, loss 0.0251796, acc 0.984375
2016-11-12T19:09:36.186381: step 2616, loss 0.0531297, acc 0.984375
2016-11-12T19:09:36.245864: step 2617, loss 0.0112384, acc 1
2016-11-12T19:09:36.302241: step 2618, loss 0.0107018, acc 1
2016-11-12T19:09:36.359373: step 2619, loss 0.0555123, acc 0.96875
2016-11-12T19:09:36.417844: step 2620, loss 0.00910742, acc 1
2016-11-12T19:09:36.476239: step 2621, loss 0.0103645, acc 1
2016-11-12T19:09:36.534179: step 2622, loss 0.00830383, acc 1
2016-11-12T19:09:36.590902: step 2623, loss 0.0384166, acc 0.984375
2016-11-12T19:09:36.647188: step 2624, loss 0.0152543, acc 1
2016-11-12T19:09:36.705672: step 2625, loss 0.0341257, acc 0.984375
2016-11-12T19:09:36.764078: step 2626, loss 0.00856465, acc 1
2016-11-12T19:09:36.800931: step 2627, loss 0.000437869, acc 1
2016-11-12T19:09:36.858011: step 2628, loss 0.0513635, acc 0.96875
2016-11-12T19:09:36.915874: step 2629, loss 0.00910992, acc 1
2016-11-12T19:09:36.972671: step 2630, loss 0.00699304, acc 1
2016-11-12T19:09:37.030685: step 2631, loss 0.00593954, acc 1
2016-11-12T19:09:37.088627: step 2632, loss 0.0360128, acc 0.984375
2016-11-12T19:09:37.145951: step 2633, loss 0.0125073, acc 1
2016-11-12T19:09:37.204054: step 2634, loss 0.113458, acc 0.984375
2016-11-12T19:09:37.261882: step 2635, loss 0.00615076, acc 1
2016-11-12T19:09:37.319269: step 2636, loss 0.0105989, acc 1
2016-11-12T19:09:37.378826: step 2637, loss 0.00818761, acc 1
2016-11-12T19:09:37.436122: step 2638, loss 0.0436269, acc 0.984375
2016-11-12T19:09:37.492415: step 2639, loss 0.0101053, acc 1
2016-11-12T19:09:37.549424: step 2640, loss 0.0149367, acc 1
2016-11-12T19:09:37.605113: step 2641, loss 0.0296373, acc 0.984375
2016-11-12T19:09:37.661430: step 2642, loss 0.0212317, acc 0.984375
2016-11-12T19:09:37.718082: step 2643, loss 0.0323777, acc 0.984375
2016-11-12T19:09:37.776741: step 2644, loss 0.0281478, acc 0.984375
2016-11-12T19:09:37.832589: step 2645, loss 0.00396223, acc 1
2016-11-12T19:09:37.888613: step 2646, loss 0.00469866, acc 1
2016-11-12T19:09:37.945555: step 2647, loss 0.0109434, acc 1
2016-11-12T19:09:38.001382: step 2648, loss 0.0311196, acc 0.984375
2016-11-12T19:09:38.058044: step 2649, loss 0.00907826, acc 1
2016-11-12T19:09:38.114542: step 2650, loss 0.006996, acc 1
2016-11-12T19:09:38.170131: step 2651, loss 0.0210691, acc 1
2016-11-12T19:09:38.228141: step 2652, loss 0.0204444, acc 0.984375
2016-11-12T19:09:38.284730: step 2653, loss 0.00401828, acc 1
2016-11-12T19:09:38.341417: step 2654, loss 0.0118696, acc 1
2016-11-12T19:09:38.397293: step 2655, loss 0.0512588, acc 0.96875
2016-11-12T19:09:38.457877: step 2656, loss 0.0309781, acc 0.984375
2016-11-12T19:09:38.516790: step 2657, loss 0.00993172, acc 1
2016-11-12T19:09:38.572073: step 2658, loss 0.0292742, acc 0.984375
2016-11-12T19:09:38.629839: step 2659, loss 0.00368531, acc 1
2016-11-12T19:09:38.687180: step 2660, loss 0.00358621, acc 1
2016-11-12T19:09:38.742421: step 2661, loss 0.0202156, acc 0.984375
2016-11-12T19:09:38.798436: step 2662, loss 0.0101096, acc 1
2016-11-12T19:09:38.853877: step 2663, loss 0.0124967, acc 1
2016-11-12T19:09:38.911347: step 2664, loss 0.0413099, acc 0.984375
2016-11-12T19:09:38.969607: step 2665, loss 0.0224225, acc 1
2016-11-12T19:09:39.025885: step 2666, loss 0.0605483, acc 0.96875
2016-11-12T19:09:39.084238: step 2667, loss 0.0164546, acc 0.984375
2016-11-12T19:09:39.143040: step 2668, loss 0.00477478, acc 1
2016-11-12T19:09:39.199534: step 2669, loss 0.0183013, acc 1
2016-11-12T19:09:39.256376: step 2670, loss 0.00914818, acc 1
2016-11-12T19:09:39.312739: step 2671, loss 0.00479807, acc 1
2016-11-12T19:09:39.368450: step 2672, loss 0.0114176, acc 1
2016-11-12T19:09:39.426264: step 2673, loss 0.00868314, acc 1
2016-11-12T19:09:39.485475: step 2674, loss 0.0298814, acc 0.984375
2016-11-12T19:09:39.545270: step 2675, loss 0.0167645, acc 1
2016-11-12T19:09:39.600928: step 2676, loss 0.0149059, acc 1
2016-11-12T19:09:39.657566: step 2677, loss 0.0127781, acc 1
2016-11-12T19:09:39.715005: step 2678, loss 0.0199849, acc 1
2016-11-12T19:09:39.772194: step 2679, loss 0.0301741, acc 0.984375
2016-11-12T19:09:39.828661: step 2680, loss 0.0286372, acc 1
2016-11-12T19:09:39.885816: step 2681, loss 0.0119864, acc 1
2016-11-12T19:09:39.941567: step 2682, loss 0.0679032, acc 0.984375
2016-11-12T19:09:39.998912: step 2683, loss 0.00881551, acc 1
2016-11-12T19:09:40.056763: step 2684, loss 0.023205, acc 0.984375
2016-11-12T19:09:40.115578: step 2685, loss 0.0119598, acc 1
2016-11-12T19:09:40.173535: step 2686, loss 0.0149877, acc 1
2016-11-12T19:09:40.231566: step 2687, loss 0.00563326, acc 1
2016-11-12T19:09:40.289564: step 2688, loss 0.0059583, acc 1
2016-11-12T19:09:40.348836: step 2689, loss 0.0516019, acc 0.96875
2016-11-12T19:09:40.405238: step 2690, loss 0.0465518, acc 0.984375
2016-11-12T19:09:40.463004: step 2691, loss 0.0260461, acc 1
2016-11-12T19:09:40.519048: step 2692, loss 0.00859276, acc 1
2016-11-12T19:09:40.579010: step 2693, loss 0.0173782, acc 1
2016-11-12T19:09:40.634896: step 2694, loss 0.00908278, acc 1
2016-11-12T19:09:40.690913: step 2695, loss 0.0255229, acc 1
2016-11-12T19:09:40.748072: step 2696, loss 0.0226661, acc 1
2016-11-12T19:09:40.804075: step 2697, loss 0.0234515, acc 0.984375
2016-11-12T19:09:40.841792: step 2698, loss 0.0116405, acc 1
2016-11-12T19:09:40.900668: step 2699, loss 0.0271761, acc 0.984375
2016-11-12T19:09:40.957310: step 2700, loss 0.00504653, acc 1

Evaluation:
2016-11-12T19:09:41.028009: step 2700, loss 1.7596, acc 0.564

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-2700

2016-11-12T19:09:41.525113: step 2701, loss 0.00892591, acc 1
2016-11-12T19:09:41.584423: step 2702, loss 0.0103948, acc 1
2016-11-12T19:09:41.641965: step 2703, loss 0.0136372, acc 1
2016-11-12T19:09:41.699425: step 2704, loss 0.00447816, acc 1
2016-11-12T19:09:41.755545: step 2705, loss 0.068113, acc 0.984375
2016-11-12T19:09:41.813523: step 2706, loss 0.0342997, acc 0.984375
2016-11-12T19:09:41.869342: step 2707, loss 0.0102848, acc 1
2016-11-12T19:09:41.929477: step 2708, loss 0.0433931, acc 0.984375
2016-11-12T19:09:41.988329: step 2709, loss 0.00954215, acc 1
2016-11-12T19:09:42.045386: step 2710, loss 0.0371009, acc 0.96875
2016-11-12T19:09:42.103715: step 2711, loss 0.0117971, acc 1
2016-11-12T19:09:42.161609: step 2712, loss 0.0173316, acc 0.984375
2016-11-12T19:09:42.220138: step 2713, loss 0.00528208, acc 1
2016-11-12T19:09:42.276442: step 2714, loss 0.00433678, acc 1
2016-11-12T19:09:42.332477: step 2715, loss 0.00954731, acc 1
2016-11-12T19:09:42.389155: step 2716, loss 0.0146983, acc 1
2016-11-12T19:09:42.446107: step 2717, loss 0.0243126, acc 1
2016-11-12T19:09:42.502791: step 2718, loss 0.0507483, acc 0.96875
2016-11-12T19:09:42.561535: step 2719, loss 0.0175296, acc 1
2016-11-12T19:09:42.621103: step 2720, loss 0.109318, acc 0.984375
2016-11-12T19:09:42.678934: step 2721, loss 0.00947096, acc 1
2016-11-12T19:09:42.735753: step 2722, loss 0.0103851, acc 1
2016-11-12T19:09:42.795707: step 2723, loss 0.0626628, acc 0.984375
2016-11-12T19:09:42.852082: step 2724, loss 0.0476273, acc 0.96875
2016-11-12T19:09:42.910378: step 2725, loss 0.00613223, acc 1
2016-11-12T19:09:42.969238: step 2726, loss 0.00753932, acc 1
2016-11-12T19:09:43.028695: step 2727, loss 0.0142425, acc 1
2016-11-12T19:09:43.084972: step 2728, loss 0.0526212, acc 0.984375
2016-11-12T19:09:43.141630: step 2729, loss 0.0160283, acc 1
2016-11-12T19:09:43.198296: step 2730, loss 0.00546433, acc 1
2016-11-12T19:09:43.257397: step 2731, loss 0.0030698, acc 1
2016-11-12T19:09:43.319281: step 2732, loss 0.0121562, acc 1
2016-11-12T19:09:43.376560: step 2733, loss 0.142249, acc 0.96875
2016-11-12T19:09:43.434158: step 2734, loss 0.0098534, acc 1
2016-11-12T19:09:43.490692: step 2735, loss 0.00850857, acc 1
2016-11-12T19:09:43.548778: step 2736, loss 0.00641186, acc 1
2016-11-12T19:09:43.606236: step 2737, loss 0.0302387, acc 0.96875
2016-11-12T19:09:43.662489: step 2738, loss 0.00479865, acc 1
2016-11-12T19:09:43.718703: step 2739, loss 0.00374669, acc 1
2016-11-12T19:09:43.775234: step 2740, loss 0.0977852, acc 0.96875
2016-11-12T19:09:43.833498: step 2741, loss 0.00536671, acc 1
2016-11-12T19:09:43.890159: step 2742, loss 0.0130177, acc 1
2016-11-12T19:09:43.945295: step 2743, loss 0.00407578, acc 1
2016-11-12T19:09:44.000855: step 2744, loss 0.0276639, acc 0.984375
2016-11-12T19:09:44.061520: step 2745, loss 0.0326528, acc 0.984375
2016-11-12T19:09:44.120039: step 2746, loss 0.00989717, acc 1
2016-11-12T19:09:44.176460: step 2747, loss 0.00721656, acc 1
2016-11-12T19:09:44.232894: step 2748, loss 0.00737557, acc 1
2016-11-12T19:09:44.289013: step 2749, loss 0.0168754, acc 1
2016-11-12T19:09:44.344339: step 2750, loss 0.152709, acc 0.921875
2016-11-12T19:09:44.399867: step 2751, loss 0.0294617, acc 0.984375
2016-11-12T19:09:44.458878: step 2752, loss 0.00620017, acc 1
2016-11-12T19:09:44.517628: step 2753, loss 0.0418906, acc 0.96875
2016-11-12T19:09:44.576497: step 2754, loss 0.0111431, acc 1
2016-11-12T19:09:44.633207: step 2755, loss 0.0079626, acc 1
2016-11-12T19:09:44.689071: step 2756, loss 0.0221468, acc 1
2016-11-12T19:09:44.745431: step 2757, loss 0.00276128, acc 1
2016-11-12T19:09:44.800834: step 2758, loss 0.0295067, acc 0.984375
2016-11-12T19:09:44.857489: step 2759, loss 0.0173548, acc 1
2016-11-12T19:09:44.914415: step 2760, loss 0.027464, acc 1
2016-11-12T19:09:44.973933: step 2761, loss 0.0389165, acc 0.984375
2016-11-12T19:09:45.031068: step 2762, loss 0.00286365, acc 1
2016-11-12T19:09:45.088157: step 2763, loss 0.0148897, acc 1
2016-11-12T19:09:45.143906: step 2764, loss 0.0296775, acc 1
2016-11-12T19:09:45.200266: step 2765, loss 0.0095444, acc 1
2016-11-12T19:09:45.257791: step 2766, loss 0.0131518, acc 1
2016-11-12T19:09:45.314617: step 2767, loss 0.0167206, acc 1
2016-11-12T19:09:45.376886: step 2768, loss 0.0446867, acc 0.984375
2016-11-12T19:09:45.417058: step 2769, loss 0.0050383, acc 1
2016-11-12T19:09:45.477607: step 2770, loss 0.00820004, acc 1
2016-11-12T19:09:45.534156: step 2771, loss 0.0116955, acc 1
2016-11-12T19:09:45.592500: step 2772, loss 0.0704162, acc 0.953125
2016-11-12T19:09:45.649323: step 2773, loss 0.00439411, acc 1
2016-11-12T19:09:45.705379: step 2774, loss 0.0289277, acc 0.96875
2016-11-12T19:09:45.763836: step 2775, loss 0.0170116, acc 1
2016-11-12T19:09:45.819978: step 2776, loss 0.0079676, acc 1
2016-11-12T19:09:45.877323: step 2777, loss 0.00499403, acc 1
2016-11-12T19:09:45.936693: step 2778, loss 0.0420169, acc 0.96875
2016-11-12T19:09:45.994619: step 2779, loss 0.0121531, acc 1
2016-11-12T19:09:46.051663: step 2780, loss 0.0784267, acc 0.96875
2016-11-12T19:09:46.108612: step 2781, loss 0.0292874, acc 0.984375
2016-11-12T19:09:46.166932: step 2782, loss 0.0183004, acc 1
2016-11-12T19:09:46.223384: step 2783, loss 0.0432635, acc 0.984375
2016-11-12T19:09:46.280699: step 2784, loss 0.0170579, acc 0.984375
2016-11-12T19:09:46.337485: step 2785, loss 0.015147, acc 0.984375
2016-11-12T19:09:46.397199: step 2786, loss 0.0301966, acc 0.984375
2016-11-12T19:09:46.455614: step 2787, loss 0.0527936, acc 0.984375
2016-11-12T19:09:46.513750: step 2788, loss 0.0107926, acc 1
2016-11-12T19:09:46.571038: step 2789, loss 0.0511138, acc 0.96875
2016-11-12T19:09:46.628152: step 2790, loss 0.00898184, acc 1
2016-11-12T19:09:46.685155: step 2791, loss 0.00373359, acc 1
2016-11-12T19:09:46.744003: step 2792, loss 0.0493944, acc 0.984375
2016-11-12T19:09:46.801865: step 2793, loss 0.0182651, acc 1
2016-11-12T19:09:46.858533: step 2794, loss 0.0103666, acc 1
2016-11-12T19:09:46.914766: step 2795, loss 0.0079377, acc 1
2016-11-12T19:09:46.971495: step 2796, loss 0.00945205, acc 1
2016-11-12T19:09:47.028073: step 2797, loss 0.0118978, acc 1
2016-11-12T19:09:47.086297: step 2798, loss 0.00544995, acc 1
2016-11-12T19:09:47.145100: step 2799, loss 0.0188394, acc 0.984375
2016-11-12T19:09:47.205383: step 2800, loss 0.00501262, acc 1

Evaluation:
2016-11-12T19:09:47.275912: step 2800, loss 1.78991, acc 0.566

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-2800

2016-11-12T19:09:47.768494: step 2801, loss 0.0496555, acc 0.984375
2016-11-12T19:09:47.825629: step 2802, loss 0.00356262, acc 1
2016-11-12T19:09:47.883521: step 2803, loss 0.0183579, acc 0.984375
2016-11-12T19:09:47.941337: step 2804, loss 0.00687657, acc 1
2016-11-12T19:09:47.997809: step 2805, loss 0.02644, acc 0.984375
2016-11-12T19:09:48.054740: step 2806, loss 0.0313329, acc 0.984375
2016-11-12T19:09:48.113568: step 2807, loss 0.0404126, acc 0.984375
2016-11-12T19:09:48.172953: step 2808, loss 0.00556586, acc 1
2016-11-12T19:09:48.229725: step 2809, loss 0.00795772, acc 1
2016-11-12T19:09:48.286330: step 2810, loss 0.024662, acc 0.984375
2016-11-12T19:09:48.344302: step 2811, loss 0.0108365, acc 1
2016-11-12T19:09:48.402460: step 2812, loss 0.110972, acc 0.984375
2016-11-12T19:09:48.459548: step 2813, loss 0.0257009, acc 0.984375
2016-11-12T19:09:48.517236: step 2814, loss 0.0287022, acc 1
2016-11-12T19:09:48.573294: step 2815, loss 0.00236595, acc 1
2016-11-12T19:09:48.629569: step 2816, loss 0.00240807, acc 1
2016-11-12T19:09:48.686677: step 2817, loss 0.0094127, acc 1
2016-11-12T19:09:48.745760: step 2818, loss 0.00319125, acc 1
2016-11-12T19:09:48.805565: step 2819, loss 0.0100297, acc 1
2016-11-12T19:09:48.862123: step 2820, loss 0.0240115, acc 1
2016-11-12T19:09:48.921313: step 2821, loss 0.00557303, acc 1
2016-11-12T19:09:48.977477: step 2822, loss 0.00464244, acc 1
2016-11-12T19:09:49.033682: step 2823, loss 0.0266767, acc 1
2016-11-12T19:09:49.088983: step 2824, loss 0.00257228, acc 1
2016-11-12T19:09:49.146479: step 2825, loss 0.00978266, acc 1
2016-11-12T19:09:49.204807: step 2826, loss 0.0214831, acc 1
2016-11-12T19:09:49.261912: step 2827, loss 0.0316071, acc 0.984375
2016-11-12T19:09:49.317815: step 2828, loss 0.0181446, acc 1
2016-11-12T19:09:49.375434: step 2829, loss 0.0258537, acc 0.984375
2016-11-12T19:09:49.433012: step 2830, loss 0.0108591, acc 1
2016-11-12T19:09:49.489750: step 2831, loss 0.013257, acc 1
2016-11-12T19:09:49.548965: step 2832, loss 0.0129931, acc 1
2016-11-12T19:09:49.605680: step 2833, loss 0.0095124, acc 1
2016-11-12T19:09:49.665045: step 2834, loss 0.0179152, acc 1
2016-11-12T19:09:49.720767: step 2835, loss 0.00560775, acc 1
2016-11-12T19:09:49.776357: step 2836, loss 0.022369, acc 0.984375
2016-11-12T19:09:49.832991: step 2837, loss 0.00469871, acc 1
2016-11-12T19:09:49.889421: step 2838, loss 0.00881271, acc 1
2016-11-12T19:09:49.945113: step 2839, loss 0.114858, acc 0.96875
2016-11-12T19:09:49.983994: step 2840, loss 0.0889792, acc 0.95
2016-11-12T19:09:50.042982: step 2841, loss 0.0109749, acc 1
2016-11-12T19:09:50.100679: step 2842, loss 0.00351305, acc 1
2016-11-12T19:09:50.157531: step 2843, loss 0.0528217, acc 0.96875
2016-11-12T19:09:50.214099: step 2844, loss 0.00745994, acc 1
2016-11-12T19:09:50.273062: step 2845, loss 0.0198607, acc 1
2016-11-12T19:09:50.328857: step 2846, loss 0.0132743, acc 1
2016-11-12T19:09:50.385185: step 2847, loss 0.040298, acc 0.984375
2016-11-12T19:09:50.442307: step 2848, loss 0.00630791, acc 1
2016-11-12T19:09:50.498718: step 2849, loss 0.0112925, acc 1
2016-11-12T19:09:50.556994: step 2850, loss 0.0153588, acc 1
2016-11-12T19:09:50.614526: step 2851, loss 0.0320342, acc 0.984375
2016-11-12T19:09:50.673426: step 2852, loss 0.00397984, acc 1
2016-11-12T19:09:50.732770: step 2853, loss 0.0166329, acc 0.984375
2016-11-12T19:09:50.789075: step 2854, loss 0.0106311, acc 1
2016-11-12T19:09:50.846042: step 2855, loss 0.00975806, acc 1
2016-11-12T19:09:50.904163: step 2856, loss 0.0149186, acc 1
2016-11-12T19:09:50.961783: step 2857, loss 0.0313877, acc 0.984375
2016-11-12T19:09:51.021354: step 2858, loss 0.00516022, acc 1
2016-11-12T19:09:51.077901: step 2859, loss 0.00690323, acc 1
2016-11-12T19:09:51.133834: step 2860, loss 0.00561581, acc 1
2016-11-12T19:09:51.190633: step 2861, loss 0.0317471, acc 0.984375
2016-11-12T19:09:51.247094: step 2862, loss 0.0136263, acc 1
2016-11-12T19:09:51.302949: step 2863, loss 0.0052988, acc 1
2016-11-12T19:09:51.359432: step 2864, loss 0.00436253, acc 1
2016-11-12T19:09:51.415439: step 2865, loss 0.0251152, acc 0.984375
2016-11-12T19:09:51.473800: step 2866, loss 0.0709624, acc 0.96875
2016-11-12T19:09:51.531395: step 2867, loss 0.033784, acc 0.984375
2016-11-12T19:09:51.587690: step 2868, loss 0.00893953, acc 1
2016-11-12T19:09:51.643809: step 2869, loss 0.0160914, acc 1
2016-11-12T19:09:51.699099: step 2870, loss 0.012908, acc 1
2016-11-12T19:09:51.757573: step 2871, loss 0.0586847, acc 0.984375
2016-11-12T19:09:51.816831: step 2872, loss 0.0152373, acc 1
2016-11-12T19:09:51.874780: step 2873, loss 0.00567345, acc 1
2016-11-12T19:09:51.931896: step 2874, loss 0.0135148, acc 1
2016-11-12T19:09:51.990288: step 2875, loss 0.0116082, acc 1
2016-11-12T19:09:52.048508: step 2876, loss 0.014136, acc 1
2016-11-12T19:09:52.105155: step 2877, loss 0.0489596, acc 0.984375
2016-11-12T19:09:52.161895: step 2878, loss 0.0184315, acc 1
2016-11-12T19:09:52.218328: step 2879, loss 0.0053659, acc 1
2016-11-12T19:09:52.274390: step 2880, loss 0.0274854, acc 0.984375
2016-11-12T19:09:52.332519: step 2881, loss 0.0585349, acc 0.984375
2016-11-12T19:09:52.390306: step 2882, loss 0.00921914, acc 1
2016-11-12T19:09:52.447529: step 2883, loss 0.0223112, acc 0.984375
2016-11-12T19:09:52.504752: step 2884, loss 0.0535199, acc 0.984375
2016-11-12T19:09:52.561223: step 2885, loss 0.0181588, acc 1
2016-11-12T19:09:52.619984: step 2886, loss 0.00936498, acc 1
2016-11-12T19:09:52.676116: step 2887, loss 0.0466169, acc 0.96875
2016-11-12T19:09:52.733374: step 2888, loss 0.108016, acc 0.96875
2016-11-12T19:09:52.793566: step 2889, loss 0.0120111, acc 1
2016-11-12T19:09:52.851597: step 2890, loss 0.0298347, acc 0.984375
2016-11-12T19:09:52.908011: step 2891, loss 0.010626, acc 1
2016-11-12T19:09:52.964484: step 2892, loss 0.00817211, acc 1
2016-11-12T19:09:53.021424: step 2893, loss 0.0153076, acc 1
2016-11-12T19:09:53.080091: step 2894, loss 0.00544966, acc 1
2016-11-12T19:09:53.138678: step 2895, loss 0.0119353, acc 1
2016-11-12T19:09:53.197575: step 2896, loss 0.01196, acc 1
2016-11-12T19:09:53.255406: step 2897, loss 0.00785267, acc 1
2016-11-12T19:09:53.312852: step 2898, loss 0.00290438, acc 1
2016-11-12T19:09:53.369849: step 2899, loss 0.0338401, acc 0.984375
2016-11-12T19:09:53.427455: step 2900, loss 0.0355349, acc 0.984375

Evaluation:
2016-11-12T19:09:53.498361: step 2900, loss 1.80308, acc 0.566

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-2900

2016-11-12T19:09:53.991008: step 2901, loss 0.023239, acc 0.984375
2016-11-12T19:09:54.047316: step 2902, loss 0.0189386, acc 1
2016-11-12T19:09:54.104096: step 2903, loss 0.00601482, acc 1
2016-11-12T19:09:54.160384: step 2904, loss 0.00465583, acc 1
2016-11-12T19:09:54.217010: step 2905, loss 0.0238188, acc 1
2016-11-12T19:09:54.272553: step 2906, loss 0.00623576, acc 1
2016-11-12T19:09:54.329281: step 2907, loss 0.0157077, acc 1
2016-11-12T19:09:54.385080: step 2908, loss 0.00355659, acc 1
2016-11-12T19:09:54.441165: step 2909, loss 0.0126027, acc 1
2016-11-12T19:09:54.497065: step 2910, loss 0.0109269, acc 1
2016-11-12T19:09:54.535225: step 2911, loss 0.0050561, acc 1
2016-11-12T19:09:54.593383: step 2912, loss 0.00290071, acc 1
2016-11-12T19:09:54.652005: step 2913, loss 0.0263784, acc 0.984375
2016-11-12T19:09:54.708832: step 2914, loss 0.0046974, acc 1
2016-11-12T19:09:54.765315: step 2915, loss 0.0048535, acc 1
2016-11-12T19:09:54.824548: step 2916, loss 0.0609864, acc 0.984375
2016-11-12T19:09:54.880477: step 2917, loss 0.00984265, acc 1
2016-11-12T19:09:54.937422: step 2918, loss 0.0105917, acc 1
2016-11-12T19:09:54.993821: step 2919, loss 0.0135776, acc 1
2016-11-12T19:09:55.050889: step 2920, loss 0.00512141, acc 1
2016-11-12T19:09:55.106568: step 2921, loss 0.0124476, acc 1
2016-11-12T19:09:55.163789: step 2922, loss 0.0107068, acc 1
2016-11-12T19:09:55.221091: step 2923, loss 0.00895116, acc 1
2016-11-12T19:09:55.277798: step 2924, loss 0.0249775, acc 0.984375
2016-11-12T19:09:55.335366: step 2925, loss 0.00871675, acc 1
2016-11-12T19:09:55.392172: step 2926, loss 0.00485334, acc 1
2016-11-12T19:09:55.448721: step 2927, loss 0.0126217, acc 1
2016-11-12T19:09:55.504681: step 2928, loss 0.00757357, acc 1
2016-11-12T19:09:55.561162: step 2929, loss 0.00817795, acc 1
2016-11-12T19:09:55.617528: step 2930, loss 0.009995, acc 1
2016-11-12T19:09:55.674086: step 2931, loss 0.0613633, acc 0.96875
2016-11-12T19:09:55.732806: step 2932, loss 0.0301825, acc 0.984375
2016-11-12T19:09:55.789799: step 2933, loss 0.0205346, acc 0.984375
2016-11-12T19:09:55.846531: step 2934, loss 0.00696375, acc 1
2016-11-12T19:09:55.905177: step 2935, loss 0.00423162, acc 1
2016-11-12T19:09:55.964095: step 2936, loss 0.0236246, acc 0.984375
2016-11-12T19:09:56.021468: step 2937, loss 0.0260934, acc 0.984375
2016-11-12T19:09:56.077595: step 2938, loss 0.0457077, acc 0.984375
2016-11-12T19:09:56.136838: step 2939, loss 0.0143112, acc 1
2016-11-12T19:09:56.193584: step 2940, loss 0.0109856, acc 1
2016-11-12T19:09:56.249293: step 2941, loss 0.0184662, acc 0.984375
2016-11-12T19:09:56.309334: step 2942, loss 0.00448444, acc 1
2016-11-12T19:09:56.366556: step 2943, loss 0.0313399, acc 0.984375
2016-11-12T19:09:56.425573: step 2944, loss 0.00426079, acc 1
2016-11-12T19:09:56.482263: step 2945, loss 0.0316674, acc 0.984375
2016-11-12T19:09:56.538477: step 2946, loss 0.004606, acc 1
2016-11-12T19:09:56.599515: step 2947, loss 0.043706, acc 0.984375
2016-11-12T19:09:56.656380: step 2948, loss 0.0023014, acc 1
2016-11-12T19:09:56.712006: step 2949, loss 0.0847921, acc 0.96875
2016-11-12T19:09:56.768444: step 2950, loss 0.0133604, acc 1
2016-11-12T19:09:56.825467: step 2951, loss 0.0124566, acc 1
2016-11-12T19:09:56.884460: step 2952, loss 0.0298062, acc 0.984375
2016-11-12T19:09:56.941540: step 2953, loss 0.0196358, acc 1
2016-11-12T19:09:56.999476: step 2954, loss 0.0393406, acc 0.984375
2016-11-12T19:09:57.055794: step 2955, loss 0.00632583, acc 1
2016-11-12T19:09:57.111507: step 2956, loss 0.0125945, acc 1
2016-11-12T19:09:57.169994: step 2957, loss 0.0467041, acc 0.984375
2016-11-12T19:09:57.226307: step 2958, loss 0.0137042, acc 1
2016-11-12T19:09:57.285884: step 2959, loss 0.00888679, acc 1
2016-11-12T19:09:57.344849: step 2960, loss 0.0264866, acc 1
2016-11-12T19:09:57.401336: step 2961, loss 0.00504315, acc 1
2016-11-12T19:09:57.457286: step 2962, loss 0.0081338, acc 1
2016-11-12T19:09:57.517409: step 2963, loss 0.00531515, acc 1
2016-11-12T19:09:57.573258: step 2964, loss 0.037488, acc 0.984375
2016-11-12T19:09:57.628980: step 2965, loss 0.125608, acc 0.96875
2016-11-12T19:09:57.688939: step 2966, loss 0.109074, acc 0.984375
2016-11-12T19:09:57.748831: step 2967, loss 0.00221727, acc 1
2016-11-12T19:09:57.805024: step 2968, loss 0.00266976, acc 1
2016-11-12T19:09:57.860792: step 2969, loss 0.00723445, acc 1
2016-11-12T19:09:57.917176: step 2970, loss 0.00532082, acc 1
2016-11-12T19:09:57.973838: step 2971, loss 0.0218114, acc 0.984375
2016-11-12T19:09:58.030617: step 2972, loss 0.0356321, acc 0.96875
2016-11-12T19:09:58.086929: step 2973, loss 0.0178188, acc 1
2016-11-12T19:09:58.144023: step 2974, loss 0.00591983, acc 1
2016-11-12T19:09:58.201559: step 2975, loss 0.031104, acc 0.984375
2016-11-12T19:09:58.260101: step 2976, loss 0.0366423, acc 0.984375
2016-11-12T19:09:58.316587: step 2977, loss 0.0211979, acc 0.984375
2016-11-12T19:09:58.373132: step 2978, loss 0.00477882, acc 1
2016-11-12T19:09:58.428908: step 2979, loss 0.0161806, acc 1
2016-11-12T19:09:58.489087: step 2980, loss 0.0292834, acc 0.984375
2016-11-12T19:09:58.547286: step 2981, loss 0.0366935, acc 0.984375
2016-11-12T19:09:58.585562: step 2982, loss 0.00406131, acc 1
2016-11-12T19:09:58.642834: step 2983, loss 0.0104362, acc 1
2016-11-12T19:09:58.701088: step 2984, loss 0.00416605, acc 1
2016-11-12T19:09:58.760845: step 2985, loss 0.0574261, acc 0.984375
2016-11-12T19:09:58.818650: step 2986, loss 0.0107416, acc 1
2016-11-12T19:09:58.875454: step 2987, loss 0.0290047, acc 0.984375
2016-11-12T19:09:58.932343: step 2988, loss 0.0117103, acc 1
2016-11-12T19:09:58.989407: step 2989, loss 0.0768083, acc 0.953125
2016-11-12T19:09:59.046105: step 2990, loss 0.076968, acc 0.96875
2016-11-12T19:09:59.101907: step 2991, loss 0.00775913, acc 1
2016-11-12T19:09:59.160029: step 2992, loss 0.007738, acc 1
2016-11-12T19:09:59.220888: step 2993, loss 0.0066117, acc 1
2016-11-12T19:09:59.279399: step 2994, loss 0.0153082, acc 1
2016-11-12T19:09:59.336266: step 2995, loss 0.0125162, acc 1
2016-11-12T19:09:59.392924: step 2996, loss 0.0192509, acc 0.984375
2016-11-12T19:09:59.449762: step 2997, loss 0.0176194, acc 0.984375
2016-11-12T19:09:59.508977: step 2998, loss 0.012668, acc 1
2016-11-12T19:09:59.567286: step 2999, loss 0.0289543, acc 0.984375
2016-11-12T19:09:59.624890: step 3000, loss 0.0190043, acc 1

Evaluation:
2016-11-12T19:09:59.695676: step 3000, loss 1.88042, acc 0.568

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-3000

2016-11-12T19:10:00.187887: step 3001, loss 0.00820699, acc 1
2016-11-12T19:10:00.245831: step 3002, loss 0.0156332, acc 0.984375
2016-11-12T19:10:00.306864: step 3003, loss 0.0043189, acc 1
2016-11-12T19:10:00.364912: step 3004, loss 0.0592726, acc 0.984375
2016-11-12T19:10:00.421364: step 3005, loss 0.0174919, acc 0.984375
2016-11-12T19:10:00.477378: step 3006, loss 0.0253387, acc 0.984375
2016-11-12T19:10:00.534100: step 3007, loss 0.0775841, acc 0.984375
2016-11-12T19:10:00.590490: step 3008, loss 0.0778605, acc 0.96875
2016-11-12T19:10:00.649399: step 3009, loss 0.0115866, acc 1
2016-11-12T19:10:00.705147: step 3010, loss 0.0364723, acc 0.984375
2016-11-12T19:10:00.762506: step 3011, loss 0.0279185, acc 0.984375
2016-11-12T19:10:00.819962: step 3012, loss 0.0180861, acc 1
2016-11-12T19:10:00.877037: step 3013, loss 0.0348608, acc 0.984375
2016-11-12T19:10:00.933676: step 3014, loss 0.00320899, acc 1
2016-11-12T19:10:00.989797: step 3015, loss 0.00462879, acc 1
2016-11-12T19:10:01.047978: step 3016, loss 0.00384309, acc 1
2016-11-12T19:10:01.104782: step 3017, loss 0.00332105, acc 1
2016-11-12T19:10:01.161797: step 3018, loss 0.031049, acc 0.984375
2016-11-12T19:10:01.221445: step 3019, loss 0.00492971, acc 1
2016-11-12T19:10:01.276823: step 3020, loss 0.0784606, acc 0.96875
2016-11-12T19:10:01.333327: step 3021, loss 0.0122785, acc 1
2016-11-12T19:10:01.390809: step 3022, loss 0.00233881, acc 1
2016-11-12T19:10:01.447279: step 3023, loss 0.00217314, acc 1
2016-11-12T19:10:01.505674: step 3024, loss 0.00846924, acc 1
2016-11-12T19:10:01.562725: step 3025, loss 0.0381719, acc 0.984375
2016-11-12T19:10:01.619089: step 3026, loss 0.0349497, acc 0.984375
2016-11-12T19:10:01.676804: step 3027, loss 0.0051131, acc 1
2016-11-12T19:10:01.734314: step 3028, loss 0.0101674, acc 1
2016-11-12T19:10:01.791192: step 3029, loss 0.0475142, acc 0.984375
2016-11-12T19:10:01.849331: step 3030, loss 0.0549722, acc 0.984375
2016-11-12T19:10:01.905699: step 3031, loss 0.00505846, acc 1
2016-11-12T19:10:01.963596: step 3032, loss 0.0156328, acc 0.984375
2016-11-12T19:10:02.022874: step 3033, loss 0.0752445, acc 0.96875
2016-11-12T19:10:02.079099: step 3034, loss 0.0414531, acc 0.984375
2016-11-12T19:10:02.137401: step 3035, loss 0.0141076, acc 1
2016-11-12T19:10:02.194039: step 3036, loss 0.00251336, acc 1
2016-11-12T19:10:02.250765: step 3037, loss 0.00854095, acc 1
2016-11-12T19:10:02.306402: step 3038, loss 0.0128949, acc 1
2016-11-12T19:10:02.365472: step 3039, loss 0.0212186, acc 0.984375
2016-11-12T19:10:02.424537: step 3040, loss 0.0399816, acc 0.984375
2016-11-12T19:10:02.481530: step 3041, loss 0.0102762, acc 1
2016-11-12T19:10:02.537797: step 3042, loss 0.00542148, acc 1
2016-11-12T19:10:02.594621: step 3043, loss 0.0110502, acc 1
2016-11-12T19:10:02.653202: step 3044, loss 0.00793636, acc 1
2016-11-12T19:10:02.709603: step 3045, loss 0.0151593, acc 1
2016-11-12T19:10:02.768114: step 3046, loss 0.0415758, acc 0.984375
2016-11-12T19:10:02.825895: step 3047, loss 0.0294321, acc 0.984375
2016-11-12T19:10:02.882604: step 3048, loss 0.0079228, acc 1
2016-11-12T19:10:02.940159: step 3049, loss 0.0263899, acc 0.984375
2016-11-12T19:10:02.997442: step 3050, loss 0.0116789, acc 1
2016-11-12T19:10:03.056430: step 3051, loss 0.00983951, acc 1
2016-11-12T19:10:03.112490: step 3052, loss 0.0433484, acc 0.984375
2016-11-12T19:10:03.149620: step 3053, loss 0.00628181, acc 1
2016-11-12T19:10:03.208451: step 3054, loss 0.00689074, acc 1
2016-11-12T19:10:03.265482: step 3055, loss 0.0145585, acc 1
2016-11-12T19:10:03.321167: step 3056, loss 0.0246916, acc 0.984375
2016-11-12T19:10:03.379438: step 3057, loss 0.00401402, acc 1
2016-11-12T19:10:03.436155: step 3058, loss 0.00930315, acc 1
2016-11-12T19:10:03.492145: step 3059, loss 0.00273793, acc 1
2016-11-12T19:10:03.547505: step 3060, loss 0.00493326, acc 1
2016-11-12T19:10:03.604229: step 3061, loss 0.00690388, acc 1
2016-11-12T19:10:03.661456: step 3062, loss 0.0445927, acc 0.984375
2016-11-12T19:10:03.720384: step 3063, loss 0.00351888, acc 1
2016-11-12T19:10:03.777541: step 3064, loss 0.00794056, acc 1
2016-11-12T19:10:03.834804: step 3065, loss 0.0200015, acc 1
2016-11-12T19:10:03.892553: step 3066, loss 0.0109286, acc 1
2016-11-12T19:10:03.950563: step 3067, loss 0.0117195, acc 1
2016-11-12T19:10:04.008332: step 3068, loss 0.0158802, acc 0.984375
2016-11-12T19:10:04.068103: step 3069, loss 0.00300563, acc 1
2016-11-12T19:10:04.124939: step 3070, loss 0.00404796, acc 1
2016-11-12T19:10:04.181476: step 3071, loss 0.0103303, acc 1
2016-11-12T19:10:04.237633: step 3072, loss 0.00393322, acc 1
2016-11-12T19:10:04.295356: step 3073, loss 0.0180323, acc 0.984375
2016-11-12T19:10:04.352343: step 3074, loss 0.00296529, acc 1
2016-11-12T19:10:04.409113: step 3075, loss 0.00171898, acc 1
2016-11-12T19:10:04.466985: step 3076, loss 0.0169636, acc 1
2016-11-12T19:10:04.525110: step 3077, loss 0.0183282, acc 0.984375
2016-11-12T19:10:04.584802: step 3078, loss 0.0143319, acc 0.984375
2016-11-12T19:10:04.641961: step 3079, loss 0.0119452, acc 1
2016-11-12T19:10:04.702313: step 3080, loss 0.0119169, acc 0.984375
2016-11-12T19:10:04.760888: step 3081, loss 0.00679867, acc 1
2016-11-12T19:10:04.817219: step 3082, loss 0.0110619, acc 1
2016-11-12T19:10:04.872728: step 3083, loss 0.0458838, acc 0.984375
2016-11-12T19:10:04.930640: step 3084, loss 0.00745767, acc 1
2016-11-12T19:10:04.989807: step 3085, loss 0.0249066, acc 0.984375
2016-11-12T19:10:05.048324: step 3086, loss 0.0224791, acc 0.984375
2016-11-12T19:10:05.105561: step 3087, loss 0.00811231, acc 1
2016-11-12T19:10:05.161154: step 3088, loss 0.0296519, acc 0.984375
2016-11-12T19:10:05.218939: step 3089, loss 0.00514717, acc 1
2016-11-12T19:10:05.274124: step 3090, loss 0.00858247, acc 1
2016-11-12T19:10:05.330401: step 3091, loss 0.0342286, acc 0.96875
2016-11-12T19:10:05.386390: step 3092, loss 0.0048436, acc 1
2016-11-12T19:10:05.444693: step 3093, loss 0.00517331, acc 1
2016-11-12T19:10:05.501250: step 3094, loss 0.00466122, acc 1
2016-11-12T19:10:05.558430: step 3095, loss 0.0055792, acc 1
2016-11-12T19:10:05.617325: step 3096, loss 0.00525791, acc 1
2016-11-12T19:10:05.674062: step 3097, loss 0.00651055, acc 1
2016-11-12T19:10:05.730227: step 3098, loss 0.0365691, acc 0.984375
2016-11-12T19:10:05.788334: step 3099, loss 0.0224486, acc 1
2016-11-12T19:10:05.845206: step 3100, loss 0.0201514, acc 0.984375

Evaluation:
2016-11-12T19:10:05.916898: step 3100, loss 1.8738, acc 0.572

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-3100

2016-11-12T19:10:06.406723: step 3101, loss 0.102139, acc 0.96875
2016-11-12T19:10:06.465573: step 3102, loss 0.0116374, acc 1
2016-11-12T19:10:06.525220: step 3103, loss 0.0129977, acc 1
2016-11-12T19:10:06.582642: step 3104, loss 0.00695797, acc 1
2016-11-12T19:10:06.638475: step 3105, loss 0.00321074, acc 1
2016-11-12T19:10:06.696098: step 3106, loss 0.0026794, acc 1
2016-11-12T19:10:06.754015: step 3107, loss 0.00337495, acc 1
2016-11-12T19:10:06.811438: step 3108, loss 0.0245121, acc 0.984375
2016-11-12T19:10:06.869966: step 3109, loss 0.00546295, acc 1
2016-11-12T19:10:06.927859: step 3110, loss 0.0128818, acc 1
2016-11-12T19:10:06.984398: step 3111, loss 0.0852183, acc 0.984375
2016-11-12T19:10:07.040702: step 3112, loss 0.0274081, acc 0.984375
2016-11-12T19:10:07.099445: step 3113, loss 0.045601, acc 0.96875
2016-11-12T19:10:07.159247: step 3114, loss 0.0524678, acc 0.984375
2016-11-12T19:10:07.218370: step 3115, loss 0.00574545, acc 1
2016-11-12T19:10:07.276915: step 3116, loss 0.00656228, acc 1
2016-11-12T19:10:07.335933: step 3117, loss 0.00144622, acc 1
2016-11-12T19:10:07.393324: step 3118, loss 0.0340437, acc 0.984375
2016-11-12T19:10:07.451514: step 3119, loss 0.0069167, acc 1
2016-11-12T19:10:07.509092: step 3120, loss 0.0171514, acc 1
2016-11-12T19:10:07.567448: step 3121, loss 0.0475793, acc 0.984375
2016-11-12T19:10:07.626592: step 3122, loss 0.0145228, acc 1
2016-11-12T19:10:07.685770: step 3123, loss 0.00251021, acc 1
2016-11-12T19:10:07.724333: step 3124, loss 0.00638985, acc 1
2016-11-12T19:10:07.783399: step 3125, loss 0.0108806, acc 1
2016-11-12T19:10:07.840330: step 3126, loss 0.00320092, acc 1
2016-11-12T19:10:07.896613: step 3127, loss 0.00430806, acc 1
2016-11-12T19:10:07.954236: step 3128, loss 0.00750519, acc 1
2016-11-12T19:10:08.010378: step 3129, loss 0.0213628, acc 0.984375
2016-11-12T19:10:08.066614: step 3130, loss 0.00632512, acc 1
2016-11-12T19:10:08.125601: step 3131, loss 0.00338296, acc 1
2016-11-12T19:10:08.181171: step 3132, loss 0.0610795, acc 0.96875
2016-11-12T19:10:08.236524: step 3133, loss 0.0711312, acc 0.96875
2016-11-12T19:10:08.293079: step 3134, loss 0.0314473, acc 0.984375
2016-11-12T19:10:08.349220: step 3135, loss 0.0175327, acc 1
2016-11-12T19:10:08.405610: step 3136, loss 0.00314152, acc 1
2016-11-12T19:10:08.463097: step 3137, loss 0.00671387, acc 1
2016-11-12T19:10:08.518596: step 3138, loss 0.0280113, acc 0.984375
2016-11-12T19:10:08.575760: step 3139, loss 0.0106747, acc 1
2016-11-12T19:10:08.633339: step 3140, loss 0.0168633, acc 1
2016-11-12T19:10:08.690028: step 3141, loss 0.0117586, acc 1
2016-11-12T19:10:08.747926: step 3142, loss 0.0038287, acc 1
2016-11-12T19:10:08.805231: step 3143, loss 0.0061711, acc 1
2016-11-12T19:10:08.862394: step 3144, loss 0.0310785, acc 0.984375
2016-11-12T19:10:08.920596: step 3145, loss 0.023956, acc 0.984375
2016-11-12T19:10:08.978353: step 3146, loss 0.0197719, acc 0.984375
2016-11-12T19:10:09.034590: step 3147, loss 0.012762, acc 1
2016-11-12T19:10:09.091668: step 3148, loss 0.00683052, acc 1
2016-11-12T19:10:09.147848: step 3149, loss 0.008146, acc 1
2016-11-12T19:10:09.205342: step 3150, loss 0.0092368, acc 1
2016-11-12T19:10:09.264300: step 3151, loss 0.00780051, acc 1
2016-11-12T19:10:09.320626: step 3152, loss 0.00689949, acc 1
2016-11-12T19:10:09.377604: step 3153, loss 0.00354696, acc 1
2016-11-12T19:10:09.433236: step 3154, loss 0.0051837, acc 1
2016-11-12T19:10:09.489794: step 3155, loss 0.00977798, acc 1
2016-11-12T19:10:09.546731: step 3156, loss 0.0105007, acc 1
2016-11-12T19:10:09.604930: step 3157, loss 0.0091051, acc 1
2016-11-12T19:10:09.662789: step 3158, loss 0.00653592, acc 1
2016-11-12T19:10:09.719498: step 3159, loss 0.0236809, acc 0.984375
2016-11-12T19:10:09.777202: step 3160, loss 0.049428, acc 0.96875
2016-11-12T19:10:09.834232: step 3161, loss 0.00758038, acc 1
2016-11-12T19:10:09.890227: step 3162, loss 0.0523894, acc 0.984375
2016-11-12T19:10:09.948168: step 3163, loss 0.00518864, acc 1
2016-11-12T19:10:10.005252: step 3164, loss 0.039237, acc 0.984375
2016-11-12T19:10:10.061993: step 3165, loss 0.00665531, acc 1
2016-11-12T19:10:10.120315: step 3166, loss 0.00224459, acc 1
2016-11-12T19:10:10.176296: step 3167, loss 0.0248049, acc 1
2016-11-12T19:10:10.233197: step 3168, loss 0.0109466, acc 1
2016-11-12T19:10:10.290031: step 3169, loss 0.00747692, acc 1
2016-11-12T19:10:10.346445: step 3170, loss 0.00876755, acc 1
2016-11-12T19:10:10.403696: step 3171, loss 0.00512673, acc 1
2016-11-12T19:10:10.461166: step 3172, loss 0.0207312, acc 1
2016-11-12T19:10:10.521124: step 3173, loss 0.0231548, acc 1
2016-11-12T19:10:10.581211: step 3174, loss 0.0103033, acc 1
2016-11-12T19:10:10.637264: step 3175, loss 0.0876511, acc 0.953125
2016-11-12T19:10:10.694269: step 3176, loss 0.00288311, acc 1
2016-11-12T19:10:10.751432: step 3177, loss 0.027135, acc 0.984375
2016-11-12T19:10:10.809768: step 3178, loss 0.0218258, acc 0.984375
2016-11-12T19:10:10.870236: step 3179, loss 0.0767316, acc 0.96875
2016-11-12T19:10:10.928762: step 3180, loss 0.0144989, acc 1
2016-11-12T19:10:10.984972: step 3181, loss 0.170029, acc 0.96875
2016-11-12T19:10:11.041754: step 3182, loss 0.0121657, acc 1
2016-11-12T19:10:11.098668: step 3183, loss 0.0129831, acc 1
2016-11-12T19:10:11.155327: step 3184, loss 0.0177988, acc 1
2016-11-12T19:10:11.212061: step 3185, loss 0.0117969, acc 1
2016-11-12T19:10:11.268742: step 3186, loss 0.0241188, acc 0.984375
2016-11-12T19:10:11.325773: step 3187, loss 0.00376474, acc 1
2016-11-12T19:10:11.381765: step 3188, loss 0.0606411, acc 0.953125
2016-11-12T19:10:11.438863: step 3189, loss 0.00436375, acc 1
2016-11-12T19:10:11.494605: step 3190, loss 0.0109109, acc 1
2016-11-12T19:10:11.553162: step 3191, loss 0.000694971, acc 1
2016-11-12T19:10:11.608524: step 3192, loss 0.00183196, acc 1
2016-11-12T19:10:11.665846: step 3193, loss 0.0109797, acc 1
2016-11-12T19:10:11.721644: step 3194, loss 0.0123722, acc 1
2016-11-12T19:10:11.758840: step 3195, loss 0.00929295, acc 1
2016-11-12T19:10:11.817252: step 3196, loss 0.0218216, acc 0.984375
2016-11-12T19:10:11.874348: step 3197, loss 0.0366383, acc 0.984375
2016-11-12T19:10:11.931133: step 3198, loss 0.0362055, acc 0.984375
2016-11-12T19:10:11.988699: step 3199, loss 0.0102823, acc 1
2016-11-12T19:10:12.045644: step 3200, loss 0.00732854, acc 1

Evaluation:
2016-11-12T19:10:12.116197: step 3200, loss 1.91908, acc 0.572

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-3200

2016-11-12T19:10:12.657210: step 3201, loss 0.0105133, acc 1
2016-11-12T19:10:12.715349: step 3202, loss 0.00498832, acc 1
2016-11-12T19:10:12.772549: step 3203, loss 0.0134116, acc 1
2016-11-12T19:10:12.829842: step 3204, loss 0.00432912, acc 1
2016-11-12T19:10:12.888795: step 3205, loss 0.0137203, acc 1
2016-11-12T19:10:12.946711: step 3206, loss 0.00432294, acc 1
2016-11-12T19:10:13.003488: step 3207, loss 0.104031, acc 0.984375
2016-11-12T19:10:13.063458: step 3208, loss 0.0107022, acc 1
2016-11-12T19:10:13.121672: step 3209, loss 0.0242267, acc 0.984375
2016-11-12T19:10:13.177691: step 3210, loss 0.00604644, acc 1
2016-11-12T19:10:13.234330: step 3211, loss 0.0104967, acc 1
2016-11-12T19:10:13.292637: step 3212, loss 0.00433237, acc 1
2016-11-12T19:10:13.348593: step 3213, loss 0.0221168, acc 0.984375
2016-11-12T19:10:13.404877: step 3214, loss 0.0157909, acc 1
2016-11-12T19:10:13.462423: step 3215, loss 0.014202, acc 1
2016-11-12T19:10:13.520506: step 3216, loss 0.0663765, acc 0.984375
2016-11-12T19:10:13.577304: step 3217, loss 0.0194983, acc 1
2016-11-12T19:10:13.635172: step 3218, loss 0.00399388, acc 1
2016-11-12T19:10:13.692073: step 3219, loss 0.0292506, acc 0.984375
2016-11-12T19:10:13.749703: step 3220, loss 0.0343106, acc 0.984375
2016-11-12T19:10:13.806213: step 3221, loss 0.01388, acc 1
2016-11-12T19:10:13.863053: step 3222, loss 0.00800759, acc 1
2016-11-12T19:10:13.919944: step 3223, loss 0.0678705, acc 0.984375
2016-11-12T19:10:13.976292: step 3224, loss 0.0034958, acc 1
2016-11-12T19:10:14.034270: step 3225, loss 0.0312895, acc 0.984375
2016-11-12T19:10:14.093458: step 3226, loss 0.0280396, acc 0.984375
2016-11-12T19:10:14.151265: step 3227, loss 0.00941261, acc 1
2016-11-12T19:10:14.208971: step 3228, loss 0.00594327, acc 1
2016-11-12T19:10:14.267093: step 3229, loss 0.052169, acc 0.984375
2016-11-12T19:10:14.325473: step 3230, loss 0.00414825, acc 1
2016-11-12T19:10:14.381592: step 3231, loss 0.003701, acc 1
2016-11-12T19:10:14.437255: step 3232, loss 0.0579739, acc 0.96875
2016-11-12T19:10:14.494530: step 3233, loss 0.025714, acc 0.96875
2016-11-12T19:10:14.553337: step 3234, loss 0.0413446, acc 0.96875
2016-11-12T19:10:14.609538: step 3235, loss 0.00558191, acc 1
2016-11-12T19:10:14.665339: step 3236, loss 0.0194016, acc 1
2016-11-12T19:10:14.724968: step 3237, loss 0.00947062, acc 1
2016-11-12T19:10:14.783637: step 3238, loss 0.00572896, acc 1
2016-11-12T19:10:14.839912: step 3239, loss 0.00591078, acc 1
2016-11-12T19:10:14.895710: step 3240, loss 0.00461918, acc 1
2016-11-12T19:10:14.952499: step 3241, loss 0.0272511, acc 0.984375
2016-11-12T19:10:15.009372: step 3242, loss 0.00742933, acc 1
2016-11-12T19:10:15.066618: step 3243, loss 0.00558394, acc 1
2016-11-12T19:10:15.122480: step 3244, loss 0.00725379, acc 1
2016-11-12T19:10:15.178402: step 3245, loss 0.0169018, acc 0.984375
2016-11-12T19:10:15.235695: step 3246, loss 0.00601008, acc 1
2016-11-12T19:10:15.293117: step 3247, loss 0.0247849, acc 0.984375
2016-11-12T19:10:15.349280: step 3248, loss 0.00428108, acc 1
2016-11-12T19:10:15.409145: step 3249, loss 0.0121852, acc 1
2016-11-12T19:10:15.466030: step 3250, loss 0.0262946, acc 0.984375
2016-11-12T19:10:15.523172: step 3251, loss 0.0420335, acc 0.984375
2016-11-12T19:10:15.579269: step 3252, loss 0.112182, acc 0.96875
2016-11-12T19:10:15.638050: step 3253, loss 0.0417906, acc 0.984375
2016-11-12T19:10:15.697268: step 3254, loss 0.00174795, acc 1
2016-11-12T19:10:15.753101: step 3255, loss 0.0428142, acc 0.984375
2016-11-12T19:10:15.810395: step 3256, loss 0.0263975, acc 0.984375
2016-11-12T19:10:15.865859: step 3257, loss 0.0623798, acc 0.96875
2016-11-12T19:10:15.923150: step 3258, loss 0.0209197, acc 1
2016-11-12T19:10:15.985406: step 3259, loss 0.0112065, acc 1
2016-11-12T19:10:16.041170: step 3260, loss 0.0686603, acc 0.96875
2016-11-12T19:10:16.099363: step 3261, loss 0.00620332, acc 1
2016-11-12T19:10:16.156527: step 3262, loss 0.0111449, acc 1
2016-11-12T19:10:16.213881: step 3263, loss 0.000813186, acc 1
2016-11-12T19:10:16.269592: step 3264, loss 0.046489, acc 0.984375
2016-11-12T19:10:16.329868: step 3265, loss 0.00174194, acc 1
2016-11-12T19:10:16.370648: step 3266, loss 0.00245798, acc 1
2016-11-12T19:10:16.429424: step 3267, loss 0.0565356, acc 0.984375
2016-11-12T19:10:16.491215: step 3268, loss 0.00565118, acc 1
2016-11-12T19:10:16.548854: step 3269, loss 0.00443579, acc 1
2016-11-12T19:10:16.606068: step 3270, loss 0.00407235, acc 1
2016-11-12T19:10:16.662363: step 3271, loss 0.0197983, acc 1
2016-11-12T19:10:16.721297: step 3272, loss 0.0666937, acc 0.96875
2016-11-12T19:10:16.781142: step 3273, loss 0.0589669, acc 0.96875
2016-11-12T19:10:16.837253: step 3274, loss 0.0281149, acc 1
2016-11-12T19:10:16.898218: step 3275, loss 0.0151524, acc 0.984375
2016-11-12T19:10:16.955269: step 3276, loss 0.0534117, acc 0.984375
2016-11-12T19:10:17.013479: step 3277, loss 0.00421732, acc 1
2016-11-12T19:10:17.071234: step 3278, loss 0.0403045, acc 0.984375
2016-11-12T19:10:17.130085: step 3279, loss 0.0238369, acc 1
2016-11-12T19:10:17.187892: step 3280, loss 0.0139258, acc 1
2016-11-12T19:10:17.245709: step 3281, loss 0.036129, acc 0.984375
2016-11-12T19:10:17.303554: step 3282, loss 0.00356816, acc 1
2016-11-12T19:10:17.361119: step 3283, loss 0.0518565, acc 0.96875
2016-11-12T19:10:17.419309: step 3284, loss 0.00549825, acc 1
2016-11-12T19:10:17.477734: step 3285, loss 0.00541993, acc 1
2016-11-12T19:10:17.533825: step 3286, loss 0.013312, acc 1
2016-11-12T19:10:17.590043: step 3287, loss 0.0130334, acc 1
2016-11-12T19:10:17.649224: step 3288, loss 0.00334643, acc 1
2016-11-12T19:10:17.705084: step 3289, loss 0.00233634, acc 1
2016-11-12T19:10:17.761108: step 3290, loss 0.0171125, acc 1
2016-11-12T19:10:17.817806: step 3291, loss 0.0395119, acc 0.984375
2016-11-12T19:10:17.873859: step 3292, loss 0.00857838, acc 1
2016-11-12T19:10:17.934890: step 3293, loss 0.00139271, acc 1
2016-11-12T19:10:17.990672: step 3294, loss 0.0530009, acc 0.984375
2016-11-12T19:10:18.049763: step 3295, loss 0.00315595, acc 1
2016-11-12T19:10:18.106417: step 3296, loss 0.00913316, acc 1
2016-11-12T19:10:18.162734: step 3297, loss 0.00861091, acc 1
2016-11-12T19:10:18.218569: step 3298, loss 0.00324251, acc 1
2016-11-12T19:10:18.274506: step 3299, loss 0.00302014, acc 1
2016-11-12T19:10:18.330110: step 3300, loss 0.0246667, acc 0.984375

Evaluation:
2016-11-12T19:10:18.400110: step 3300, loss 1.96735, acc 0.568

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-3300

2016-11-12T19:10:18.907191: step 3301, loss 0.00687008, acc 1
2016-11-12T19:10:18.966617: step 3302, loss 0.014156, acc 1
2016-11-12T19:10:19.023597: step 3303, loss 0.00459895, acc 1
2016-11-12T19:10:19.079539: step 3304, loss 0.00280633, acc 1
2016-11-12T19:10:19.137290: step 3305, loss 0.00119528, acc 1
2016-11-12T19:10:19.194508: step 3306, loss 0.0135603, acc 1
2016-11-12T19:10:19.251814: step 3307, loss 0.0195891, acc 0.984375
2016-11-12T19:10:19.311523: step 3308, loss 0.0080323, acc 1
2016-11-12T19:10:19.369244: step 3309, loss 0.00253997, acc 1
2016-11-12T19:10:19.425414: step 3310, loss 0.00502629, acc 1
2016-11-12T19:10:19.481194: step 3311, loss 0.0214638, acc 0.984375
2016-11-12T19:10:19.541372: step 3312, loss 0.000839094, acc 1
2016-11-12T19:10:19.598151: step 3313, loss 0.0143568, acc 1
2016-11-12T19:10:19.657242: step 3314, loss 0.0271191, acc 0.984375
2016-11-12T19:10:19.715900: step 3315, loss 0.00769654, acc 1
2016-11-12T19:10:19.773156: step 3316, loss 0.00759504, acc 1
2016-11-12T19:10:19.830566: step 3317, loss 0.0028453, acc 1
2016-11-12T19:10:19.886571: step 3318, loss 0.00369471, acc 1
2016-11-12T19:10:19.944639: step 3319, loss 0.0132307, acc 1
2016-11-12T19:10:20.004291: step 3320, loss 0.0166656, acc 1
2016-11-12T19:10:20.060946: step 3321, loss 0.00283148, acc 1
2016-11-12T19:10:20.117137: step 3322, loss 0.0103123, acc 1
2016-11-12T19:10:20.172636: step 3323, loss 0.0449102, acc 0.96875
2016-11-12T19:10:20.230301: step 3324, loss 0.0557305, acc 0.96875
2016-11-12T19:10:20.288514: step 3325, loss 0.00630287, acc 1
2016-11-12T19:10:20.345478: step 3326, loss 0.00367011, acc 1
2016-11-12T19:10:20.404853: step 3327, loss 0.0224821, acc 1
2016-11-12T19:10:20.462344: step 3328, loss 0.00947066, acc 1
2016-11-12T19:10:20.518564: step 3329, loss 0.019566, acc 0.984375
2016-11-12T19:10:20.574750: step 3330, loss 0.0133314, acc 1
2016-11-12T19:10:20.632788: step 3331, loss 0.0482888, acc 0.96875
2016-11-12T19:10:20.690818: step 3332, loss 0.0182886, acc 1
2016-11-12T19:10:20.749011: step 3333, loss 0.00508688, acc 1
2016-11-12T19:10:20.805974: step 3334, loss 0.00823949, acc 1
2016-11-12T19:10:20.862350: step 3335, loss 0.00861731, acc 1
2016-11-12T19:10:20.919362: step 3336, loss 0.00948624, acc 1
2016-11-12T19:10:20.958119: step 3337, loss 0.0796455, acc 0.95
2016-11-12T19:10:21.017262: step 3338, loss 0.0103671, acc 1
2016-11-12T19:10:21.073284: step 3339, loss 0.00261154, acc 1
2016-11-12T19:10:21.129856: step 3340, loss 0.00192293, acc 1
2016-11-12T19:10:21.185426: step 3341, loss 0.00163943, acc 1
2016-11-12T19:10:21.241577: step 3342, loss 0.0101472, acc 1
2016-11-12T19:10:21.297881: step 3343, loss 0.0286003, acc 0.984375
2016-11-12T19:10:21.354097: step 3344, loss 0.00983852, acc 1
2016-11-12T19:10:21.410366: step 3345, loss 0.0126258, acc 1
2016-11-12T19:10:21.469400: step 3346, loss 0.0259594, acc 0.984375
2016-11-12T19:10:21.526749: step 3347, loss 0.00256196, acc 1
2016-11-12T19:10:21.582754: step 3348, loss 0.00376664, acc 1
2016-11-12T19:10:21.638899: step 3349, loss 0.0370158, acc 0.984375
2016-11-12T19:10:21.697269: step 3350, loss 0.0221131, acc 0.984375
2016-11-12T19:10:21.753346: step 3351, loss 0.00368288, acc 1
2016-11-12T19:10:21.809426: step 3352, loss 0.0279747, acc 1
2016-11-12T19:10:21.865972: step 3353, loss 0.00393284, acc 1
2016-11-12T19:10:21.922524: step 3354, loss 0.0181705, acc 1
2016-11-12T19:10:21.981439: step 3355, loss 0.0173864, acc 1
2016-11-12T19:10:22.038842: step 3356, loss 0.0280441, acc 0.984375
2016-11-12T19:10:22.097638: step 3357, loss 0.0036828, acc 1
2016-11-12T19:10:22.152566: step 3358, loss 0.00354229, acc 1
2016-11-12T19:10:22.212177: step 3359, loss 0.00346478, acc 1
2016-11-12T19:10:22.269794: step 3360, loss 0.100229, acc 0.984375
2016-11-12T19:10:22.326933: step 3361, loss 0.00537698, acc 1
2016-11-12T19:10:22.383322: step 3362, loss 0.00367512, acc 1
2016-11-12T19:10:22.439793: step 3363, loss 0.0600597, acc 0.96875
2016-11-12T19:10:22.497649: step 3364, loss 0.0352907, acc 1
2016-11-12T19:10:22.553681: step 3365, loss 0.0165425, acc 0.984375
2016-11-12T19:10:22.610796: step 3366, loss 0.0265265, acc 0.984375
2016-11-12T19:10:22.668181: step 3367, loss 0.0209736, acc 0.984375
2016-11-12T19:10:22.725336: step 3368, loss 0.0212706, acc 0.984375
2016-11-12T19:10:22.781634: step 3369, loss 0.00547466, acc 1
2016-11-12T19:10:22.838480: step 3370, loss 0.00350701, acc 1
2016-11-12T19:10:22.896923: step 3371, loss 0.00330464, acc 1
2016-11-12T19:10:22.953903: step 3372, loss 0.00536289, acc 1
2016-11-12T19:10:23.010432: step 3373, loss 0.00250974, acc 1
2016-11-12T19:10:23.066118: step 3374, loss 0.00542461, acc 1
2016-11-12T19:10:23.122832: step 3375, loss 0.0970889, acc 0.96875
2016-11-12T19:10:23.179867: step 3376, loss 0.0150745, acc 1
2016-11-12T19:10:23.239476: step 3377, loss 0.0129152, acc 1
2016-11-12T19:10:23.297442: step 3378, loss 0.00684552, acc 1
2016-11-12T19:10:23.354614: step 3379, loss 0.0114175, acc 1
2016-11-12T19:10:23.413293: step 3380, loss 0.0658428, acc 0.96875
2016-11-12T19:10:23.473413: step 3381, loss 0.0276639, acc 0.984375
2016-11-12T19:10:23.541744: step 3382, loss 0.00871849, acc 1
2016-11-12T19:10:23.598037: step 3383, loss 0.00330462, acc 1
2016-11-12T19:10:23.654485: step 3384, loss 0.0111585, acc 1
2016-11-12T19:10:23.714503: step 3385, loss 0.0132819, acc 1
2016-11-12T19:10:23.770411: step 3386, loss 0.00737775, acc 1
2016-11-12T19:10:23.826271: step 3387, loss 0.0184981, acc 0.984375
2016-11-12T19:10:23.885262: step 3388, loss 0.00754979, acc 1
2016-11-12T19:10:23.944522: step 3389, loss 0.0084628, acc 1
2016-11-12T19:10:24.002015: step 3390, loss 0.0294303, acc 0.984375
2016-11-12T19:10:24.061134: step 3391, loss 0.0163386, acc 0.984375
2016-11-12T19:10:24.121528: step 3392, loss 0.00462059, acc 1
2016-11-12T19:10:24.180021: step 3393, loss 0.00384093, acc 1
2016-11-12T19:10:24.237235: step 3394, loss 0.00569816, acc 1
2016-11-12T19:10:24.293703: step 3395, loss 0.0047077, acc 1
2016-11-12T19:10:24.349999: step 3396, loss 0.0208158, acc 1
2016-11-12T19:10:24.408718: step 3397, loss 0.020155, acc 0.984375
2016-11-12T19:10:24.465675: step 3398, loss 0.00374805, acc 1
2016-11-12T19:10:24.523055: step 3399, loss 0.023102, acc 0.984375
2016-11-12T19:10:24.582411: step 3400, loss 0.0139689, acc 0.984375

Evaluation:
2016-11-12T19:10:24.653411: step 3400, loss 2.01852, acc 0.558

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-3400

2016-11-12T19:10:25.161969: step 3401, loss 0.00990463, acc 1
2016-11-12T19:10:25.221204: step 3402, loss 0.00666216, acc 1
2016-11-12T19:10:25.277615: step 3403, loss 0.00773354, acc 1
2016-11-12T19:10:25.334787: step 3404, loss 0.0113199, acc 1
2016-11-12T19:10:25.394462: step 3405, loss 0.0175132, acc 0.984375
2016-11-12T19:10:25.452190: step 3406, loss 0.0196848, acc 1
2016-11-12T19:10:25.508807: step 3407, loss 0.0123177, acc 1
2016-11-12T19:10:25.547774: step 3408, loss 0.00930766, acc 1
2016-11-12T19:10:25.606943: step 3409, loss 0.00191873, acc 1
2016-11-12T19:10:25.663150: step 3410, loss 0.00912799, acc 1
2016-11-12T19:10:25.723463: step 3411, loss 0.00635331, acc 1
2016-11-12T19:10:25.780135: step 3412, loss 0.00735164, acc 1
2016-11-12T19:10:25.838723: step 3413, loss 0.00125138, acc 1
2016-11-12T19:10:25.894432: step 3414, loss 0.012914, acc 0.984375
2016-11-12T19:10:25.950711: step 3415, loss 0.00267974, acc 1
2016-11-12T19:10:26.009081: step 3416, loss 0.00326182, acc 1
2016-11-12T19:10:26.066762: step 3417, loss 0.00263874, acc 1
2016-11-12T19:10:26.122500: step 3418, loss 0.0157908, acc 1
2016-11-12T19:10:26.179185: step 3419, loss 0.00415211, acc 1
2016-11-12T19:10:26.235335: step 3420, loss 0.0088561, acc 1
2016-11-12T19:10:26.292305: step 3421, loss 0.0026209, acc 1
2016-11-12T19:10:26.349080: step 3422, loss 0.0638019, acc 0.96875
2016-11-12T19:10:26.409005: step 3423, loss 0.00101213, acc 1
2016-11-12T19:10:26.465057: step 3424, loss 0.00492124, acc 1
2016-11-12T19:10:26.521457: step 3425, loss 0.010353, acc 1
2016-11-12T19:10:26.580901: step 3426, loss 0.0223098, acc 0.984375
2016-11-12T19:10:26.637798: step 3427, loss 0.0132305, acc 1
2016-11-12T19:10:26.698672: step 3428, loss 0.00219914, acc 1
2016-11-12T19:10:26.754835: step 3429, loss 0.00443183, acc 1
2016-11-12T19:10:26.813564: step 3430, loss 0.00228612, acc 1
2016-11-12T19:10:26.872993: step 3431, loss 0.0295957, acc 0.984375
2016-11-12T19:10:26.929371: step 3432, loss 0.0208691, acc 0.984375
2016-11-12T19:10:26.987182: step 3433, loss 0.00113284, acc 1
2016-11-12T19:10:27.043310: step 3434, loss 0.024067, acc 0.984375
2016-11-12T19:10:27.101914: step 3435, loss 0.0162918, acc 1
2016-11-12T19:10:27.162725: step 3436, loss 0.0387035, acc 0.96875
2016-11-12T19:10:27.221114: step 3437, loss 0.00275928, acc 1
2016-11-12T19:10:27.277192: step 3438, loss 0.00955254, acc 1
2016-11-12T19:10:27.333645: step 3439, loss 0.00912857, acc 1
2016-11-12T19:10:27.390744: step 3440, loss 0.00490983, acc 1
2016-11-12T19:10:27.448864: step 3441, loss 0.00131807, acc 1
2016-11-12T19:10:27.506092: step 3442, loss 0.0657699, acc 0.984375
2016-11-12T19:10:27.562751: step 3443, loss 0.0287573, acc 0.984375
2016-11-12T19:10:27.621633: step 3444, loss 0.0117773, acc 1
2016-11-12T19:10:27.678114: step 3445, loss 0.0211482, acc 0.984375
2016-11-12T19:10:27.735084: step 3446, loss 0.0180174, acc 0.984375
2016-11-12T19:10:27.793635: step 3447, loss 0.00562111, acc 1
2016-11-12T19:10:27.850005: step 3448, loss 0.00708836, acc 1
2016-11-12T19:10:27.907305: step 3449, loss 0.0116678, acc 1
2016-11-12T19:10:27.965365: step 3450, loss 0.000967914, acc 1
2016-11-12T19:10:28.022036: step 3451, loss 0.00244919, acc 1
2016-11-12T19:10:28.077934: step 3452, loss 0.00564592, acc 1
2016-11-12T19:10:28.137316: step 3453, loss 0.0483554, acc 0.984375
2016-11-12T19:10:28.197856: step 3454, loss 0.00807662, acc 1
2016-11-12T19:10:28.253539: step 3455, loss 0.00204374, acc 1
2016-11-12T19:10:28.309334: step 3456, loss 0.0347852, acc 0.984375
2016-11-12T19:10:28.365755: step 3457, loss 0.0501234, acc 0.984375
2016-11-12T19:10:28.425380: step 3458, loss 0.0113669, acc 1
2016-11-12T19:10:28.481680: step 3459, loss 0.00108164, acc 1
2016-11-12T19:10:28.536701: step 3460, loss 0.0191893, acc 1
2016-11-12T19:10:28.593891: step 3461, loss 0.00812327, acc 1
2016-11-12T19:10:28.653361: step 3462, loss 0.0190492, acc 0.984375
2016-11-12T19:10:28.709783: step 3463, loss 0.0144847, acc 1
2016-11-12T19:10:28.765638: step 3464, loss 0.0259139, acc 0.984375
2016-11-12T19:10:28.822246: step 3465, loss 0.00896252, acc 1
2016-11-12T19:10:28.880886: step 3466, loss 0.0071024, acc 1
2016-11-12T19:10:28.937762: step 3467, loss 0.0431131, acc 0.96875
2016-11-12T19:10:28.997378: step 3468, loss 0.00297596, acc 1
2016-11-12T19:10:29.054764: step 3469, loss 0.00439135, acc 1
2016-11-12T19:10:29.112988: step 3470, loss 0.0232064, acc 0.984375
2016-11-12T19:10:29.171072: step 3471, loss 0.0131348, acc 1
2016-11-12T19:10:29.228260: step 3472, loss 0.160281, acc 0.984375
2016-11-12T19:10:29.286957: step 3473, loss 0.0419017, acc 0.96875
2016-11-12T19:10:29.345595: step 3474, loss 0.00304515, acc 1
2016-11-12T19:10:29.402891: step 3475, loss 0.0140941, acc 1
2016-11-12T19:10:29.461584: step 3476, loss 0.0364729, acc 0.984375
2016-11-12T19:10:29.520478: step 3477, loss 0.0137378, acc 0.984375
2016-11-12T19:10:29.577683: step 3478, loss 0.0210873, acc 0.984375
2016-11-12T19:10:29.616050: step 3479, loss 0.00499137, acc 1
2016-11-12T19:10:29.675598: step 3480, loss 0.0463089, acc 0.96875
2016-11-12T19:10:29.732357: step 3481, loss 0.015732, acc 1
2016-11-12T19:10:29.787953: step 3482, loss 0.00324101, acc 1
2016-11-12T19:10:29.843923: step 3483, loss 0.0118183, acc 1
2016-11-12T19:10:29.900532: step 3484, loss 0.0364918, acc 0.984375
2016-11-12T19:10:29.958061: step 3485, loss 0.0178155, acc 0.984375
2016-11-12T19:10:30.015747: step 3486, loss 0.038974, acc 0.984375
2016-11-12T19:10:30.073325: step 3487, loss 0.0115369, acc 1
2016-11-12T19:10:30.128927: step 3488, loss 0.0151266, acc 1
2016-11-12T19:10:30.184839: step 3489, loss 0.018944, acc 0.984375
2016-11-12T19:10:30.241147: step 3490, loss 0.00460416, acc 1
2016-11-12T19:10:30.298790: step 3491, loss 0.00721028, acc 1
2016-11-12T19:10:30.356517: step 3492, loss 0.00253928, acc 1
2016-11-12T19:10:30.413277: step 3493, loss 0.0245522, acc 0.984375
2016-11-12T19:10:30.469224: step 3494, loss 0.00402463, acc 1
2016-11-12T19:10:30.525067: step 3495, loss 0.0291557, acc 0.984375
2016-11-12T19:10:30.582566: step 3496, loss 0.0253957, acc 0.984375
2016-11-12T19:10:30.640286: step 3497, loss 0.00558229, acc 1
2016-11-12T19:10:30.699901: step 3498, loss 0.0398216, acc 0.984375
2016-11-12T19:10:30.758891: step 3499, loss 0.0290811, acc 0.984375
2016-11-12T19:10:30.816831: step 3500, loss 0.00272986, acc 1

Evaluation:
2016-11-12T19:10:30.888083: step 3500, loss 2.06446, acc 0.556

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-3500

2016-11-12T19:10:31.393594: step 3501, loss 0.0283057, acc 0.984375
2016-11-12T19:10:31.450562: step 3502, loss 0.00412233, acc 1
2016-11-12T19:10:31.507462: step 3503, loss 0.0194103, acc 1
2016-11-12T19:10:31.565278: step 3504, loss 0.0182635, acc 1
2016-11-12T19:10:31.622745: step 3505, loss 0.019002, acc 0.984375
2016-11-12T19:10:31.680173: step 3506, loss 0.00757413, acc 1
2016-11-12T19:10:31.740384: step 3507, loss 0.00203888, acc 1
2016-11-12T19:10:31.795772: step 3508, loss 0.00368453, acc 1
2016-11-12T19:10:31.853604: step 3509, loss 0.0366405, acc 0.984375
2016-11-12T19:10:31.911824: step 3510, loss 0.00738475, acc 1
2016-11-12T19:10:31.967739: step 3511, loss 0.00633377, acc 1
2016-11-12T19:10:32.025291: step 3512, loss 0.00738725, acc 1
2016-11-12T19:10:32.080981: step 3513, loss 0.00521083, acc 1
2016-11-12T19:10:32.136569: step 3514, loss 0.0946001, acc 0.96875
2016-11-12T19:10:32.196093: step 3515, loss 0.0212364, acc 1
2016-11-12T19:10:32.256092: step 3516, loss 0.00714828, acc 1
2016-11-12T19:10:32.313439: step 3517, loss 0.00151456, acc 1
2016-11-12T19:10:32.370606: step 3518, loss 0.00656733, acc 1
2016-11-12T19:10:32.427183: step 3519, loss 0.0230689, acc 0.984375
2016-11-12T19:10:32.484219: step 3520, loss 0.0248808, acc 0.984375
2016-11-12T19:10:32.541796: step 3521, loss 0.012616, acc 0.984375
2016-11-12T19:10:32.601326: step 3522, loss 0.00857043, acc 1
2016-11-12T19:10:32.657579: step 3523, loss 0.00347992, acc 1
2016-11-12T19:10:32.717116: step 3524, loss 0.00851262, acc 1
2016-11-12T19:10:32.772982: step 3525, loss 0.0245123, acc 1
2016-11-12T19:10:32.829884: step 3526, loss 0.00706544, acc 1
2016-11-12T19:10:32.889256: step 3527, loss 0.00163831, acc 1
2016-11-12T19:10:32.945648: step 3528, loss 0.0216406, acc 0.984375
2016-11-12T19:10:33.003117: step 3529, loss 0.0122354, acc 1
2016-11-12T19:10:33.060025: step 3530, loss 0.00303921, acc 1
2016-11-12T19:10:33.116297: step 3531, loss 0.0082164, acc 1
2016-11-12T19:10:33.172957: step 3532, loss 0.00559738, acc 1
2016-11-12T19:10:33.230245: step 3533, loss 0.00609955, acc 1
2016-11-12T19:10:33.288197: step 3534, loss 0.00769098, acc 1
2016-11-12T19:10:33.345287: step 3535, loss 0.00533557, acc 1
2016-11-12T19:10:33.403206: step 3536, loss 0.100815, acc 0.984375
2016-11-12T19:10:33.460172: step 3537, loss 0.0172863, acc 0.984375
2016-11-12T19:10:33.516705: step 3538, loss 0.00313635, acc 1
2016-11-12T19:10:33.573339: step 3539, loss 0.0224029, acc 0.984375
2016-11-12T19:10:33.629475: step 3540, loss 0.0326602, acc 0.984375
2016-11-12T19:10:33.689564: step 3541, loss 0.00279459, acc 1
2016-11-12T19:10:33.745641: step 3542, loss 0.00162037, acc 1
2016-11-12T19:10:33.804187: step 3543, loss 0.00970072, acc 1
2016-11-12T19:10:33.862171: step 3544, loss 0.0325383, acc 0.984375
2016-11-12T19:10:33.918580: step 3545, loss 0.00859785, acc 1
2016-11-12T19:10:33.976869: step 3546, loss 0.00316823, acc 1
2016-11-12T19:10:34.032952: step 3547, loss 0.00601033, acc 1
2016-11-12T19:10:34.088891: step 3548, loss 0.00311037, acc 1
2016-11-12T19:10:34.150147: step 3549, loss 0.0370201, acc 0.984375
2016-11-12T19:10:34.188027: step 3550, loss 0.00182447, acc 1
2016-11-12T19:10:34.245285: step 3551, loss 0.00683248, acc 1
2016-11-12T19:10:34.302182: step 3552, loss 0.0562602, acc 0.984375
2016-11-12T19:10:34.360977: step 3553, loss 0.0284667, acc 0.984375
2016-11-12T19:10:34.420375: step 3554, loss 0.0040209, acc 1
2016-11-12T19:10:34.477309: step 3555, loss 0.00209619, acc 1
2016-11-12T19:10:34.534912: step 3556, loss 0.00713667, acc 1
2016-11-12T19:10:34.593112: step 3557, loss 0.0418482, acc 0.96875
2016-11-12T19:10:34.650170: step 3558, loss 0.0111029, acc 1
2016-11-12T19:10:34.709551: step 3559, loss 0.0196037, acc 0.984375
2016-11-12T19:10:34.767566: step 3560, loss 0.0238041, acc 0.984375
2016-11-12T19:10:34.825230: step 3561, loss 0.00695381, acc 1
2016-11-12T19:10:34.881877: step 3562, loss 0.00448158, acc 1
2016-11-12T19:10:34.937940: step 3563, loss 0.00849852, acc 1
2016-11-12T19:10:34.993777: step 3564, loss 0.0412104, acc 0.984375
2016-11-12T19:10:35.052688: step 3565, loss 0.00481188, acc 1
2016-11-12T19:10:35.112083: step 3566, loss 0.00257299, acc 1
2016-11-12T19:10:35.169188: step 3567, loss 0.0085021, acc 1
2016-11-12T19:10:35.225471: step 3568, loss 0.0345543, acc 0.984375
2016-11-12T19:10:35.281861: step 3569, loss 0.00198001, acc 1
2016-11-12T19:10:35.337878: step 3570, loss 0.0316344, acc 0.984375
2016-11-12T19:10:35.394735: step 3571, loss 0.0127202, acc 1
2016-11-12T19:10:35.451623: step 3572, loss 0.0100911, acc 1
2016-11-12T19:10:35.509271: step 3573, loss 0.00311293, acc 1
2016-11-12T19:10:35.566283: step 3574, loss 0.041687, acc 0.984375
2016-11-12T19:10:35.625567: step 3575, loss 0.0153081, acc 1
2016-11-12T19:10:35.683173: step 3576, loss 0.00572068, acc 1
2016-11-12T19:10:35.741620: step 3577, loss 0.0244604, acc 0.984375
2016-11-12T19:10:35.800493: step 3578, loss 0.0345389, acc 0.984375
2016-11-12T19:10:35.860259: step 3579, loss 0.00648873, acc 1
2016-11-12T19:10:35.916969: step 3580, loss 0.0103091, acc 1
2016-11-12T19:10:35.973797: step 3581, loss 0.0205875, acc 1
2016-11-12T19:10:36.030513: step 3582, loss 0.013291, acc 1
2016-11-12T19:10:36.087383: step 3583, loss 0.189373, acc 0.984375
2016-11-12T19:10:36.144700: step 3584, loss 0.00369153, acc 1
2016-11-12T19:10:36.200074: step 3585, loss 0.00786337, acc 1
2016-11-12T19:10:36.258665: step 3586, loss 0.007788, acc 1
2016-11-12T19:10:36.318395: step 3587, loss 0.00523939, acc 1
2016-11-12T19:10:36.377349: step 3588, loss 0.0167668, acc 0.984375
2016-11-12T19:10:36.433640: step 3589, loss 0.00568229, acc 1
2016-11-12T19:10:36.489411: step 3590, loss 0.00995838, acc 1
2016-11-12T19:10:36.546770: step 3591, loss 0.00571048, acc 1
2016-11-12T19:10:36.603671: step 3592, loss 0.00462759, acc 1
2016-11-12T19:10:36.660705: step 3593, loss 0.0147208, acc 0.984375
2016-11-12T19:10:36.716349: step 3594, loss 0.0241673, acc 0.984375
2016-11-12T19:10:36.773826: step 3595, loss 0.00848072, acc 1
2016-11-12T19:10:36.833609: step 3596, loss 0.0341448, acc 0.984375
2016-11-12T19:10:36.893497: step 3597, loss 0.0144168, acc 0.984375
2016-11-12T19:10:36.950777: step 3598, loss 0.00977152, acc 1
2016-11-12T19:10:37.007398: step 3599, loss 0.0191493, acc 1
2016-11-12T19:10:37.064022: step 3600, loss 0.00476815, acc 1

Evaluation:
2016-11-12T19:10:37.135176: step 3600, loss 2.09314, acc 0.556

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-3600

2016-11-12T19:10:37.640058: step 3601, loss 0.00965496, acc 1
2016-11-12T19:10:37.697387: step 3602, loss 0.0212938, acc 0.984375
2016-11-12T19:10:37.754951: step 3603, loss 0.0037012, acc 1
2016-11-12T19:10:37.813144: step 3604, loss 0.00264077, acc 1
2016-11-12T19:10:37.871513: step 3605, loss 0.00329466, acc 1
2016-11-12T19:10:37.928641: step 3606, loss 0.00200559, acc 1
2016-11-12T19:10:37.985876: step 3607, loss 0.0022362, acc 1
2016-11-12T19:10:38.041273: step 3608, loss 0.00658169, acc 1
2016-11-12T19:10:38.097121: step 3609, loss 0.00814246, acc 1
2016-11-12T19:10:38.154215: step 3610, loss 0.00321639, acc 1
2016-11-12T19:10:38.209878: step 3611, loss 0.00393134, acc 1
2016-11-12T19:10:38.266277: step 3612, loss 0.0111705, acc 1
2016-11-12T19:10:38.325739: step 3613, loss 0.0092957, acc 1
2016-11-12T19:10:38.383862: step 3614, loss 0.0129192, acc 1
2016-11-12T19:10:38.441482: step 3615, loss 0.0113885, acc 1
2016-11-12T19:10:38.500805: step 3616, loss 0.0115303, acc 1
2016-11-12T19:10:38.557398: step 3617, loss 0.0147396, acc 1
2016-11-12T19:10:38.616843: step 3618, loss 0.0105406, acc 1
2016-11-12T19:10:38.673419: step 3619, loss 0.0305613, acc 0.984375
2016-11-12T19:10:38.730886: step 3620, loss 0.0399712, acc 0.984375
2016-11-12T19:10:38.768865: step 3621, loss 0.0989851, acc 0.95
2016-11-12T19:10:38.826848: step 3622, loss 0.0600101, acc 0.984375
2016-11-12T19:10:38.885348: step 3623, loss 0.0153095, acc 0.984375
2016-11-12T19:10:38.942458: step 3624, loss 0.00863012, acc 1
2016-11-12T19:10:38.999989: step 3625, loss 0.0285947, acc 0.984375
2016-11-12T19:10:39.056743: step 3626, loss 0.00530314, acc 1
2016-11-12T19:10:39.113702: step 3627, loss 0.00744563, acc 1
2016-11-12T19:10:39.175584: step 3628, loss 0.020521, acc 0.984375
2016-11-12T19:10:39.233427: step 3629, loss 0.027296, acc 0.984375
2016-11-12T19:10:39.290645: step 3630, loss 0.00323645, acc 1
2016-11-12T19:10:39.348624: step 3631, loss 0.0103403, acc 1
2016-11-12T19:10:39.404725: step 3632, loss 0.00747997, acc 1
2016-11-12T19:10:39.461255: step 3633, loss 0.00622024, acc 1
2016-11-12T19:10:39.519543: step 3634, loss 0.0148239, acc 0.984375
2016-11-12T19:10:39.576261: step 3635, loss 0.0137849, acc 0.984375
2016-11-12T19:10:39.634724: step 3636, loss 0.0189908, acc 0.984375
2016-11-12T19:10:39.691424: step 3637, loss 0.000775137, acc 1
2016-11-12T19:10:39.749016: step 3638, loss 0.0122718, acc 1
2016-11-12T19:10:39.805573: step 3639, loss 0.00488501, acc 1
2016-11-12T19:10:39.865677: step 3640, loss 0.00475992, acc 1
2016-11-12T19:10:39.922522: step 3641, loss 0.00616862, acc 1
2016-11-12T19:10:39.981281: step 3642, loss 0.00482275, acc 1
2016-11-12T19:10:40.037563: step 3643, loss 0.0721964, acc 0.953125
2016-11-12T19:10:40.096659: step 3644, loss 0.00548755, acc 1
2016-11-12T19:10:40.153590: step 3645, loss 0.00461237, acc 1
2016-11-12T19:10:40.210868: step 3646, loss 0.00187798, acc 1
2016-11-12T19:10:40.266851: step 3647, loss 0.00524963, acc 1
2016-11-12T19:10:40.322823: step 3648, loss 0.00162522, acc 1
2016-11-12T19:10:40.378592: step 3649, loss 0.0273821, acc 0.96875
2016-11-12T19:10:40.435499: step 3650, loss 0.0782098, acc 0.984375
2016-11-12T19:10:40.492576: step 3651, loss 0.010663, acc 1
2016-11-12T19:10:40.549919: step 3652, loss 0.0221942, acc 0.984375
2016-11-12T19:10:40.607450: step 3653, loss 0.00324495, acc 1
2016-11-12T19:10:40.666222: step 3654, loss 0.0042093, acc 1
2016-11-12T19:10:40.722462: step 3655, loss 0.0130854, acc 1
2016-11-12T19:10:40.779945: step 3656, loss 0.00366803, acc 1
2016-11-12T19:10:40.837056: step 3657, loss 0.026431, acc 0.984375
2016-11-12T19:10:40.896019: step 3658, loss 0.00163639, acc 1
2016-11-12T19:10:40.952375: step 3659, loss 0.00335485, acc 1
2016-11-12T19:10:41.008377: step 3660, loss 0.03139, acc 0.984375
2016-11-12T19:10:41.064945: step 3661, loss 0.0953759, acc 0.984375
2016-11-12T19:10:41.122793: step 3662, loss 0.00255609, acc 1
2016-11-12T19:10:41.179048: step 3663, loss 0.0063891, acc 1
2016-11-12T19:10:41.235384: step 3664, loss 0.00404223, acc 1
2016-11-12T19:10:41.293562: step 3665, loss 0.00458744, acc 1
2016-11-12T19:10:41.350217: step 3666, loss 0.0375799, acc 0.984375
2016-11-12T19:10:41.408499: step 3667, loss 0.00236228, acc 1
2016-11-12T19:10:41.469902: step 3668, loss 0.0100781, acc 1
2016-11-12T19:10:41.529496: step 3669, loss 0.0251842, acc 0.984375
2016-11-12T19:10:41.589417: step 3670, loss 0.00753359, acc 1
2016-11-12T19:10:41.648456: step 3671, loss 0.00340234, acc 1
2016-11-12T19:10:41.704292: step 3672, loss 0.00434463, acc 1
2016-11-12T19:10:41.760244: step 3673, loss 0.00973494, acc 1
2016-11-12T19:10:41.817948: step 3674, loss 0.00360595, acc 1
2016-11-12T19:10:41.877715: step 3675, loss 0.00504579, acc 1
2016-11-12T19:10:41.936169: step 3676, loss 0.0295286, acc 0.984375
2016-11-12T19:10:41.993563: step 3677, loss 0.0118336, acc 1
2016-11-12T19:10:42.050269: step 3678, loss 0.0158373, acc 0.984375
2016-11-12T19:10:42.109314: step 3679, loss 0.0121962, acc 1
2016-11-12T19:10:42.165901: step 3680, loss 0.00814956, acc 1
2016-11-12T19:10:42.226271: step 3681, loss 0.0671334, acc 0.984375
2016-11-12T19:10:42.284648: step 3682, loss 0.000674097, acc 1
2016-11-12T19:10:42.342003: step 3683, loss 0.08911, acc 0.96875
2016-11-12T19:10:42.398682: step 3684, loss 0.00681367, acc 1
2016-11-12T19:10:42.455024: step 3685, loss 0.0683952, acc 0.96875
2016-11-12T19:10:42.511829: step 3686, loss 0.00546596, acc 1
2016-11-12T19:10:42.567882: step 3687, loss 0.0223454, acc 0.984375
2016-11-12T19:10:42.625562: step 3688, loss 0.00751944, acc 1
2016-11-12T19:10:42.683060: step 3689, loss 0.0172269, acc 0.984375
2016-11-12T19:10:42.741381: step 3690, loss 0.0128367, acc 0.984375
2016-11-12T19:10:42.801633: step 3691, loss 0.0131255, acc 1
2016-11-12T19:10:42.842297: step 3692, loss 0.0570423, acc 0.95
2016-11-12T19:10:42.901379: step 3693, loss 0.0224762, acc 1
2016-11-12T19:10:42.959075: step 3694, loss 0.0588464, acc 0.984375
2016-11-12T19:10:43.017218: step 3695, loss 0.020098, acc 0.984375
2016-11-12T19:10:43.080141: step 3696, loss 0.0074766, acc 1
2016-11-12T19:10:43.140060: step 3697, loss 0.0114955, acc 1
2016-11-12T19:10:43.197814: step 3698, loss 0.0301663, acc 0.984375
2016-11-12T19:10:43.255457: step 3699, loss 0.00678286, acc 1
2016-11-12T19:10:43.311076: step 3700, loss 0.0703864, acc 0.984375

Evaluation:
2016-11-12T19:10:43.382304: step 3700, loss 2.17521, acc 0.546

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-3700

2016-11-12T19:10:43.890723: step 3701, loss 0.00875583, acc 1
2016-11-12T19:10:43.946920: step 3702, loss 0.0095511, acc 1
2016-11-12T19:10:44.004606: step 3703, loss 0.00602359, acc 1
2016-11-12T19:10:44.061877: step 3704, loss 0.00828253, acc 1
2016-11-12T19:10:44.121662: step 3705, loss 0.00347342, acc 1
2016-11-12T19:10:44.178750: step 3706, loss 0.00934727, acc 1
2016-11-12T19:10:44.237406: step 3707, loss 0.00782702, acc 1
2016-11-12T19:10:44.294510: step 3708, loss 0.0124883, acc 1
2016-11-12T19:10:44.351607: step 3709, loss 0.0364197, acc 0.984375
2016-11-12T19:10:44.409921: step 3710, loss 0.0402788, acc 0.96875
2016-11-12T19:10:44.466760: step 3711, loss 0.0110988, acc 1
2016-11-12T19:10:44.525302: step 3712, loss 0.0209806, acc 0.984375
2016-11-12T19:10:44.583784: step 3713, loss 0.00188372, acc 1
2016-11-12T19:10:44.641248: step 3714, loss 0.00349942, acc 1
2016-11-12T19:10:44.697393: step 3715, loss 0.0338159, acc 0.984375
2016-11-12T19:10:44.756860: step 3716, loss 0.00754855, acc 1
2016-11-12T19:10:44.813592: step 3717, loss 0.0060952, acc 1
2016-11-12T19:10:44.870496: step 3718, loss 0.0100258, acc 1
2016-11-12T19:10:44.926541: step 3719, loss 0.00129274, acc 1
2016-11-12T19:10:44.985283: step 3720, loss 0.00181164, acc 1
2016-11-12T19:10:45.045551: step 3721, loss 0.00247909, acc 1
2016-11-12T19:10:45.103126: step 3722, loss 0.00633053, acc 1
2016-11-12T19:10:45.161065: step 3723, loss 0.0154923, acc 1
2016-11-12T19:10:45.219180: step 3724, loss 0.0097806, acc 1
2016-11-12T19:10:45.277562: step 3725, loss 0.0165971, acc 1
2016-11-12T19:10:45.334654: step 3726, loss 0.00983785, acc 1
2016-11-12T19:10:45.391729: step 3727, loss 0.0191106, acc 1
2016-11-12T19:10:45.448033: step 3728, loss 0.0161738, acc 1
2016-11-12T19:10:45.505316: step 3729, loss 0.0333236, acc 0.984375
2016-11-12T19:10:45.567276: step 3730, loss 0.00977136, acc 1
2016-11-12T19:10:45.625237: step 3731, loss 0.00795663, acc 1
2016-11-12T19:10:45.682655: step 3732, loss 0.00334448, acc 1
2016-11-12T19:10:45.741247: step 3733, loss 0.0307184, acc 0.984375
2016-11-12T19:10:45.799164: step 3734, loss 0.0688864, acc 0.984375
2016-11-12T19:10:45.855535: step 3735, loss 0.0258305, acc 1
2016-11-12T19:10:45.914069: step 3736, loss 0.0106649, acc 1
2016-11-12T19:10:45.972405: step 3737, loss 0.00697574, acc 1
2016-11-12T19:10:46.029127: step 3738, loss 0.00561141, acc 1
2016-11-12T19:10:46.088666: step 3739, loss 0.00810679, acc 1
2016-11-12T19:10:46.145345: step 3740, loss 0.0373545, acc 0.984375
2016-11-12T19:10:46.201594: step 3741, loss 0.00338061, acc 1
2016-11-12T19:10:46.260894: step 3742, loss 0.00399787, acc 1
2016-11-12T19:10:46.316793: step 3743, loss 0.0204447, acc 0.984375
2016-11-12T19:10:46.373816: step 3744, loss 0.0283176, acc 0.984375
2016-11-12T19:10:46.430338: step 3745, loss 0.000608997, acc 1
2016-11-12T19:10:46.488658: step 3746, loss 0.0150047, acc 1
2016-11-12T19:10:46.545689: step 3747, loss 0.001551, acc 1
2016-11-12T19:10:46.604886: step 3748, loss 0.00221036, acc 1
2016-11-12T19:10:46.661346: step 3749, loss 0.0187526, acc 0.984375
2016-11-12T19:10:46.718376: step 3750, loss 0.0224279, acc 0.984375
2016-11-12T19:10:46.777236: step 3751, loss 0.00225164, acc 1
2016-11-12T19:10:46.834299: step 3752, loss 0.00524935, acc 1
2016-11-12T19:10:46.890796: step 3753, loss 0.0247257, acc 0.984375
2016-11-12T19:10:46.947918: step 3754, loss 0.00601909, acc 1
2016-11-12T19:10:47.007129: step 3755, loss 0.002598, acc 1
2016-11-12T19:10:47.065070: step 3756, loss 0.00305179, acc 1
2016-11-12T19:10:47.121494: step 3757, loss 0.00651155, acc 1
2016-11-12T19:10:47.177524: step 3758, loss 0.0349312, acc 0.984375
2016-11-12T19:10:47.237535: step 3759, loss 0.0216096, acc 0.984375
2016-11-12T19:10:47.294564: step 3760, loss 0.00489582, acc 1
2016-11-12T19:10:47.353816: step 3761, loss 0.0893558, acc 0.984375
2016-11-12T19:10:47.411884: step 3762, loss 0.011955, acc 1
2016-11-12T19:10:47.449688: step 3763, loss 0.00117062, acc 1
2016-11-12T19:10:47.506507: step 3764, loss 0.011069, acc 1
2016-11-12T19:10:47.565321: step 3765, loss 0.0246755, acc 0.984375
2016-11-12T19:10:47.623458: step 3766, loss 0.00510653, acc 1
2016-11-12T19:10:47.681985: step 3767, loss 0.0280121, acc 0.984375
2016-11-12T19:10:47.739602: step 3768, loss 0.0236059, acc 0.984375
2016-11-12T19:10:47.797220: step 3769, loss 0.00255061, acc 1
2016-11-12T19:10:47.856582: step 3770, loss 0.0133591, acc 1
2016-11-12T19:10:47.917276: step 3771, loss 0.00289654, acc 1
2016-11-12T19:10:47.973349: step 3772, loss 0.0125253, acc 1
2016-11-12T19:10:48.032520: step 3773, loss 0.00313838, acc 1
2016-11-12T19:10:48.088815: step 3774, loss 0.00360451, acc 1
2016-11-12T19:10:48.145449: step 3775, loss 0.018378, acc 0.984375
2016-11-12T19:10:48.202825: step 3776, loss 0.0102427, acc 1
2016-11-12T19:10:48.261413: step 3777, loss 0.011979, acc 1
2016-11-12T19:10:48.318405: step 3778, loss 0.00169189, acc 1
2016-11-12T19:10:48.375855: step 3779, loss 0.00790942, acc 1
2016-11-12T19:10:48.433051: step 3780, loss 0.0101769, acc 1
2016-11-12T19:10:48.493624: step 3781, loss 0.0465023, acc 0.984375
2016-11-12T19:10:48.551401: step 3782, loss 0.011697, acc 1
2016-11-12T19:10:48.609032: step 3783, loss 0.0152964, acc 0.984375
2016-11-12T19:10:48.665392: step 3784, loss 0.055241, acc 0.984375
2016-11-12T19:10:48.722685: step 3785, loss 0.00905777, acc 1
2016-11-12T19:10:48.779160: step 3786, loss 0.0175777, acc 0.984375
2016-11-12T19:10:48.836503: step 3787, loss 0.00565604, acc 1
2016-11-12T19:10:48.897086: step 3788, loss 0.020246, acc 0.984375
2016-11-12T19:10:48.956105: step 3789, loss 0.00377782, acc 1
2016-11-12T19:10:49.012019: step 3790, loss 0.00347007, acc 1
2016-11-12T19:10:49.068438: step 3791, loss 0.0170574, acc 1
2016-11-12T19:10:49.125006: step 3792, loss 0.00193578, acc 1
2016-11-12T19:10:49.181663: step 3793, loss 0.0047094, acc 1
2016-11-12T19:10:49.239014: step 3794, loss 0.0475141, acc 0.984375
2016-11-12T19:10:49.296742: step 3795, loss 0.0227131, acc 0.984375
2016-11-12T19:10:49.354362: step 3796, loss 0.00192245, acc 1
2016-11-12T19:10:49.410834: step 3797, loss 0.00610871, acc 1
2016-11-12T19:10:49.467324: step 3798, loss 0.00754926, acc 1
2016-11-12T19:10:49.526395: step 3799, loss 0.0185588, acc 0.984375
2016-11-12T19:10:49.586543: step 3800, loss 0.00415223, acc 1

Evaluation:
2016-11-12T19:10:49.657233: step 3800, loss 2.21972, acc 0.558

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-3800

2016-11-12T19:10:50.159751: step 3801, loss 0.0223959, acc 1
2016-11-12T19:10:50.217524: step 3802, loss 0.00513507, acc 1
2016-11-12T19:10:50.273363: step 3803, loss 0.0175062, acc 0.984375
2016-11-12T19:10:50.330349: step 3804, loss 0.121587, acc 0.984375
2016-11-12T19:10:50.388444: step 3805, loss 0.00340362, acc 1
2016-11-12T19:10:50.446380: step 3806, loss 0.0401428, acc 0.96875
2016-11-12T19:10:50.504248: step 3807, loss 0.00386534, acc 1
2016-11-12T19:10:50.563331: step 3808, loss 0.00769467, acc 1
2016-11-12T19:10:50.620538: step 3809, loss 0.00799485, acc 1
2016-11-12T19:10:50.681809: step 3810, loss 0.0209007, acc 0.984375
2016-11-12T19:10:50.739209: step 3811, loss 0.00444002, acc 1
2016-11-12T19:10:50.797142: step 3812, loss 0.00182391, acc 1
2016-11-12T19:10:50.856260: step 3813, loss 0.0104567, acc 1
2016-11-12T19:10:50.913114: step 3814, loss 0.00700708, acc 1
2016-11-12T19:10:50.972545: step 3815, loss 0.00111067, acc 1
2016-11-12T19:10:51.029407: step 3816, loss 0.007132, acc 1
2016-11-12T19:10:51.088491: step 3817, loss 0.00586727, acc 1
2016-11-12T19:10:51.145202: step 3818, loss 0.0236783, acc 0.984375
2016-11-12T19:10:51.204801: step 3819, loss 0.00738642, acc 1
2016-11-12T19:10:51.265037: step 3820, loss 0.0266236, acc 0.984375
2016-11-12T19:10:51.322086: step 3821, loss 0.0112843, acc 1
2016-11-12T19:10:51.379115: step 3822, loss 0.0028046, acc 1
2016-11-12T19:10:51.434925: step 3823, loss 0.00707508, acc 1
2016-11-12T19:10:51.492446: step 3824, loss 0.00582674, acc 1
2016-11-12T19:10:51.548776: step 3825, loss 0.00227525, acc 1
2016-11-12T19:10:51.604586: step 3826, loss 0.0610918, acc 0.984375
2016-11-12T19:10:51.661907: step 3827, loss 0.0150293, acc 1
2016-11-12T19:10:51.718100: step 3828, loss 0.0221728, acc 0.984375
2016-11-12T19:10:51.777728: step 3829, loss 0.00962396, acc 1
2016-11-12T19:10:51.836471: step 3830, loss 0.00513733, acc 1
2016-11-12T19:10:51.893416: step 3831, loss 0.00287078, acc 1
2016-11-12T19:10:51.949712: step 3832, loss 0.0361587, acc 0.984375
2016-11-12T19:10:52.006714: step 3833, loss 0.00355097, acc 1
2016-11-12T19:10:52.044239: step 3834, loss 0.00598151, acc 1
2016-11-12T19:10:52.102706: step 3835, loss 0.0150202, acc 0.984375
2016-11-12T19:10:52.159654: step 3836, loss 0.00268112, acc 1
2016-11-12T19:10:52.215859: step 3837, loss 0.0168623, acc 0.984375
2016-11-12T19:10:52.274480: step 3838, loss 0.0532599, acc 0.953125
2016-11-12T19:10:52.330987: step 3839, loss 0.0105196, acc 1
2016-11-12T19:10:52.388737: step 3840, loss 0.00547954, acc 1
2016-11-12T19:10:52.445635: step 3841, loss 0.000698897, acc 1
2016-11-12T19:10:52.501739: step 3842, loss 0.00243088, acc 1
2016-11-12T19:10:52.557880: step 3843, loss 0.00458409, acc 1
2016-11-12T19:10:52.617404: step 3844, loss 0.00212009, acc 1
2016-11-12T19:10:52.673858: step 3845, loss 0.00212822, acc 1
2016-11-12T19:10:52.731196: step 3846, loss 0.0145243, acc 1
2016-11-12T19:10:52.789984: step 3847, loss 0.0697817, acc 0.96875
2016-11-12T19:10:52.846800: step 3848, loss 0.0138403, acc 0.984375
2016-11-12T19:10:52.904376: step 3849, loss 0.00291064, acc 1
2016-11-12T19:10:52.961106: step 3850, loss 0.00274098, acc 1
2016-11-12T19:10:53.020616: step 3851, loss 0.00075518, acc 1
2016-11-12T19:10:53.076716: step 3852, loss 0.0124595, acc 1
2016-11-12T19:10:53.134511: step 3853, loss 0.00926636, acc 1
2016-11-12T19:10:53.191806: step 3854, loss 0.0046387, acc 1
2016-11-12T19:10:53.248205: step 3855, loss 0.0547887, acc 0.984375
2016-11-12T19:10:53.310049: step 3856, loss 0.000527044, acc 1
2016-11-12T19:10:53.365433: step 3857, loss 0.0066501, acc 1
2016-11-12T19:10:53.424400: step 3858, loss 0.0133307, acc 1
2016-11-12T19:10:53.481657: step 3859, loss 0.00571204, acc 1
2016-11-12T19:10:53.538041: step 3860, loss 0.0134608, acc 0.984375
2016-11-12T19:10:53.596903: step 3861, loss 0.00186257, acc 1
2016-11-12T19:10:53.653615: step 3862, loss 0.0222996, acc 0.984375
2016-11-12T19:10:53.712656: step 3863, loss 0.00430828, acc 1
2016-11-12T19:10:53.768943: step 3864, loss 0.0113325, acc 1
2016-11-12T19:10:53.825089: step 3865, loss 0.005988, acc 1
2016-11-12T19:10:53.883324: step 3866, loss 0.000561791, acc 1
2016-11-12T19:10:53.941072: step 3867, loss 0.00143099, acc 1
2016-11-12T19:10:54.000443: step 3868, loss 0.00959832, acc 1
2016-11-12T19:10:54.056987: step 3869, loss 0.0118279, acc 1
2016-11-12T19:10:54.113791: step 3870, loss 0.0110533, acc 1
2016-11-12T19:10:54.172190: step 3871, loss 0.00325558, acc 1
2016-11-12T19:10:54.228089: step 3872, loss 0.0391652, acc 0.96875
2016-11-12T19:10:54.288863: step 3873, loss 0.0720331, acc 0.984375
2016-11-12T19:10:54.346463: step 3874, loss 0.00636636, acc 1
2016-11-12T19:10:54.405200: step 3875, loss 0.0230357, acc 0.984375
2016-11-12T19:10:54.461646: step 3876, loss 0.0100263, acc 1
2016-11-12T19:10:54.520842: step 3877, loss 0.00866684, acc 1
2016-11-12T19:10:54.578931: step 3878, loss 0.0315797, acc 0.984375
2016-11-12T19:10:54.637008: step 3879, loss 0.00581327, acc 1
2016-11-12T19:10:54.695004: step 3880, loss 0.00273848, acc 1
2016-11-12T19:10:54.753336: step 3881, loss 0.0315117, acc 0.984375
2016-11-12T19:10:54.809767: step 3882, loss 0.00661613, acc 1
2016-11-12T19:10:54.865992: step 3883, loss 0.00301027, acc 1
2016-11-12T19:10:54.925293: step 3884, loss 0.0417433, acc 0.984375
2016-11-12T19:10:54.983391: step 3885, loss 0.00396569, acc 1
2016-11-12T19:10:55.041470: step 3886, loss 0.0015174, acc 1
2016-11-12T19:10:55.096716: step 3887, loss 0.00613447, acc 1
2016-11-12T19:10:55.152692: step 3888, loss 0.00363945, acc 1
2016-11-12T19:10:55.209062: step 3889, loss 0.00227013, acc 1
2016-11-12T19:10:55.266662: step 3890, loss 0.00350726, acc 1
2016-11-12T19:10:55.323358: step 3891, loss 0.00246038, acc 1
2016-11-12T19:10:55.381630: step 3892, loss 0.00431804, acc 1
2016-11-12T19:10:55.440238: step 3893, loss 0.00302961, acc 1
2016-11-12T19:10:55.500905: step 3894, loss 0.0166225, acc 1
2016-11-12T19:10:55.560091: step 3895, loss 0.00714979, acc 1
2016-11-12T19:10:55.617977: step 3896, loss 0.112696, acc 0.984375
2016-11-12T19:10:55.675145: step 3897, loss 0.09609, acc 0.984375
2016-11-12T19:10:55.733242: step 3898, loss 0.094259, acc 0.984375
2016-11-12T19:10:55.789727: step 3899, loss 0.0611823, acc 0.984375
2016-11-12T19:10:55.847043: step 3900, loss 0.0135795, acc 0.984375

Evaluation:
2016-11-12T19:10:55.919193: step 3900, loss 2.3043, acc 0.544

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-3900

2016-11-12T19:10:56.423959: step 3901, loss 0.0054595, acc 1
2016-11-12T19:10:56.481065: step 3902, loss 0.00733771, acc 1
2016-11-12T19:10:56.541095: step 3903, loss 0.023958, acc 0.984375
2016-11-12T19:10:56.600211: step 3904, loss 0.0244717, acc 0.984375
2016-11-12T19:10:56.639375: step 3905, loss 0.00108769, acc 1
2016-11-12T19:10:56.697924: step 3906, loss 0.0364695, acc 0.984375
2016-11-12T19:10:56.757612: step 3907, loss 0.00771901, acc 1
2016-11-12T19:10:56.814084: step 3908, loss 0.00956456, acc 1
2016-11-12T19:10:56.874179: step 3909, loss 0.00122776, acc 1
2016-11-12T19:10:56.930212: step 3910, loss 0.0367653, acc 0.984375
2016-11-12T19:10:56.986610: step 3911, loss 0.0143924, acc 1
2016-11-12T19:10:57.045233: step 3912, loss 0.0211954, acc 0.984375
2016-11-12T19:10:57.103986: step 3913, loss 0.00435923, acc 1
2016-11-12T19:10:57.161567: step 3914, loss 0.0134883, acc 1
2016-11-12T19:10:57.219067: step 3915, loss 0.0060799, acc 1
2016-11-12T19:10:57.276685: step 3916, loss 0.0210412, acc 1
2016-11-12T19:10:57.336300: step 3917, loss 0.112018, acc 0.984375
2016-11-12T19:10:57.393367: step 3918, loss 0.0441307, acc 0.984375
2016-11-12T19:10:57.450515: step 3919, loss 0.0177994, acc 0.984375
2016-11-12T19:10:57.509603: step 3920, loss 0.00474891, acc 1
2016-11-12T19:10:57.565880: step 3921, loss 0.00206601, acc 1
2016-11-12T19:10:57.621864: step 3922, loss 0.00485763, acc 1
2016-11-12T19:10:57.678129: step 3923, loss 0.0286818, acc 0.984375
2016-11-12T19:10:57.737049: step 3924, loss 0.00522424, acc 1
2016-11-12T19:10:57.796352: step 3925, loss 0.0038688, acc 1
2016-11-12T19:10:57.853895: step 3926, loss 0.000319352, acc 1
2016-11-12T19:10:57.910293: step 3927, loss 0.00769203, acc 1
2016-11-12T19:10:57.969287: step 3928, loss 0.0032328, acc 1
2016-11-12T19:10:58.027367: step 3929, loss 0.00329083, acc 1
2016-11-12T19:10:58.084366: step 3930, loss 0.00806408, acc 1
2016-11-12T19:10:58.141106: step 3931, loss 0.00133937, acc 1
2016-11-12T19:10:58.197407: step 3932, loss 0.013778, acc 1
2016-11-12T19:10:58.255138: step 3933, loss 0.0875214, acc 0.96875
2016-11-12T19:10:58.313423: step 3934, loss 0.00243289, acc 1
2016-11-12T19:10:58.372484: step 3935, loss 0.00632049, acc 1
2016-11-12T19:10:58.429811: step 3936, loss 0.00664917, acc 1
2016-11-12T19:10:58.486414: step 3937, loss 0.00599291, acc 1
2016-11-12T19:10:58.542190: step 3938, loss 0.00494772, acc 1
2016-11-12T19:10:58.601189: step 3939, loss 0.0222197, acc 0.984375
2016-11-12T19:10:58.661500: step 3940, loss 0.00633747, acc 1
2016-11-12T19:10:58.721301: step 3941, loss 0.00295177, acc 1
2016-11-12T19:10:58.777518: step 3942, loss 0.0110497, acc 1
2016-11-12T19:10:58.834968: step 3943, loss 0.00434162, acc 1
2016-11-12T19:10:58.891185: step 3944, loss 0.00307166, acc 1
2016-11-12T19:10:58.949167: step 3945, loss 0.00396752, acc 1
2016-11-12T19:10:59.009373: step 3946, loss 0.00981848, acc 1
2016-11-12T19:10:59.065002: step 3947, loss 0.00696828, acc 1
2016-11-12T19:10:59.121381: step 3948, loss 0.0514287, acc 0.984375
2016-11-12T19:10:59.181762: step 3949, loss 0.00137791, acc 1
2016-11-12T19:10:59.237203: step 3950, loss 0.00464156, acc 1
2016-11-12T19:10:59.297403: step 3951, loss 0.0229405, acc 0.984375
2016-11-12T19:10:59.355694: step 3952, loss 0.0025634, acc 1
2016-11-12T19:10:59.412225: step 3953, loss 0.00762974, acc 1
2016-11-12T19:10:59.469325: step 3954, loss 0.00259065, acc 1
2016-11-12T19:10:59.527113: step 3955, loss 0.0013548, acc 1
2016-11-12T19:10:59.582925: step 3956, loss 0.000880714, acc 1
2016-11-12T19:10:59.642939: step 3957, loss 0.00268732, acc 1
2016-11-12T19:10:59.699146: step 3958, loss 0.00334225, acc 1
2016-11-12T19:10:59.757988: step 3959, loss 0.0388015, acc 0.984375
2016-11-12T19:10:59.814727: step 3960, loss 0.015982, acc 1
2016-11-12T19:10:59.871535: step 3961, loss 0.0172205, acc 0.984375
2016-11-12T19:10:59.929016: step 3962, loss 0.0023937, acc 1
2016-11-12T19:10:59.984415: step 3963, loss 0.0972927, acc 0.9375
2016-11-12T19:11:00.043339: step 3964, loss 0.00291476, acc 1
2016-11-12T19:11:00.101077: step 3965, loss 0.022034, acc 1
2016-11-12T19:11:00.157166: step 3966, loss 0.0156693, acc 0.984375
2016-11-12T19:11:00.214389: step 3967, loss 0.0123653, acc 1
2016-11-12T19:11:00.272979: step 3968, loss 0.0725577, acc 0.984375
2016-11-12T19:11:00.331008: step 3969, loss 0.00456125, acc 1
2016-11-12T19:11:00.387034: step 3970, loss 0.0151625, acc 0.984375
2016-11-12T19:11:00.445857: step 3971, loss 0.0367114, acc 0.984375
2016-11-12T19:11:00.505094: step 3972, loss 0.017636, acc 0.984375
2016-11-12T19:11:00.562081: step 3973, loss 0.00260048, acc 1
2016-11-12T19:11:00.617885: step 3974, loss 0.0296332, acc 0.984375
2016-11-12T19:11:00.675342: step 3975, loss 0.0105546, acc 1
2016-11-12T19:11:00.713233: step 3976, loss 0.00674978, acc 1
2016-11-12T19:11:00.770214: step 3977, loss 0.00272044, acc 1
2016-11-12T19:11:00.828231: step 3978, loss 0.00201152, acc 1
2016-11-12T19:11:00.885116: step 3979, loss 0.0108038, acc 1
2016-11-12T19:11:00.944139: step 3980, loss 0.00249055, acc 1
2016-11-12T19:11:01.000185: step 3981, loss 0.00566939, acc 1
2016-11-12T19:11:01.056249: step 3982, loss 0.00355374, acc 1
2016-11-12T19:11:01.111967: step 3983, loss 0.0585877, acc 0.984375
2016-11-12T19:11:01.169109: step 3984, loss 0.000522155, acc 1
2016-11-12T19:11:01.225127: step 3985, loss 0.0029979, acc 1
2016-11-12T19:11:01.281343: step 3986, loss 0.00324597, acc 1
2016-11-12T19:11:01.338492: step 3987, loss 0.0069185, acc 1
2016-11-12T19:11:01.396764: step 3988, loss 0.0133088, acc 1
2016-11-12T19:11:01.453387: step 3989, loss 0.00438142, acc 1
2016-11-12T19:11:01.513102: step 3990, loss 0.0276892, acc 0.984375
2016-11-12T19:11:01.570174: step 3991, loss 0.00873534, acc 1
2016-11-12T19:11:01.627995: step 3992, loss 0.0246387, acc 0.984375
2016-11-12T19:11:01.687118: step 3993, loss 0.00542989, acc 1
2016-11-12T19:11:01.745083: step 3994, loss 0.00159933, acc 1
2016-11-12T19:11:01.801201: step 3995, loss 0.00298094, acc 1
2016-11-12T19:11:01.857473: step 3996, loss 0.00702444, acc 1
2016-11-12T19:11:01.915953: step 3997, loss 0.00157698, acc 1
2016-11-12T19:11:01.971893: step 3998, loss 0.00276305, acc 1
2016-11-12T19:11:02.030270: step 3999, loss 0.0122933, acc 1
2016-11-12T19:11:02.089162: step 4000, loss 0.0464096, acc 0.984375

Evaluation:
2016-11-12T19:11:02.160636: step 4000, loss 2.32548, acc 0.538

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-4000

2016-11-12T19:11:02.664663: step 4001, loss 0.00641638, acc 1
2016-11-12T19:11:02.721746: step 4002, loss 0.0443973, acc 0.984375
2016-11-12T19:11:02.780161: step 4003, loss 0.0201052, acc 0.984375
2016-11-12T19:11:02.837038: step 4004, loss 0.0115235, acc 1
2016-11-12T19:11:02.893187: step 4005, loss 0.204212, acc 0.984375
2016-11-12T19:11:02.952254: step 4006, loss 0.0200855, acc 0.984375
2016-11-12T19:11:03.010062: step 4007, loss 0.00210711, acc 1
2016-11-12T19:11:03.066336: step 4008, loss 0.0416093, acc 0.984375
2016-11-12T19:11:03.125881: step 4009, loss 0.00645664, acc 1
2016-11-12T19:11:03.182562: step 4010, loss 0.0015196, acc 1
2016-11-12T19:11:03.238153: step 4011, loss 0.00426508, acc 1
2016-11-12T19:11:03.294368: step 4012, loss 0.00438874, acc 1
2016-11-12T19:11:03.353682: step 4013, loss 0.00570708, acc 1
2016-11-12T19:11:03.414414: step 4014, loss 0.0229049, acc 0.984375
2016-11-12T19:11:03.471462: step 4015, loss 0.00530201, acc 1
2016-11-12T19:11:03.527111: step 4016, loss 0.00832458, acc 1
2016-11-12T19:11:03.582781: step 4017, loss 0.0106634, acc 1
2016-11-12T19:11:03.641385: step 4018, loss 0.00321715, acc 1
2016-11-12T19:11:03.700559: step 4019, loss 0.0129563, acc 1
2016-11-12T19:11:03.757663: step 4020, loss 0.00153355, acc 1
2016-11-12T19:11:03.813472: step 4021, loss 0.00398746, acc 1
2016-11-12T19:11:03.870105: step 4022, loss 0.021735, acc 0.984375
2016-11-12T19:11:03.926613: step 4023, loss 0.00899809, acc 1
2016-11-12T19:11:03.985229: step 4024, loss 0.00227337, acc 1
2016-11-12T19:11:04.041649: step 4025, loss 0.0616884, acc 0.984375
2016-11-12T19:11:04.098706: step 4026, loss 0.0306236, acc 0.984375
2016-11-12T19:11:04.159460: step 4027, loss 0.00998404, acc 1
2016-11-12T19:11:04.218825: step 4028, loss 0.0477323, acc 0.984375
2016-11-12T19:11:04.275516: step 4029, loss 0.0544863, acc 0.96875
2016-11-12T19:11:04.333892: step 4030, loss 0.00513092, acc 1
2016-11-12T19:11:04.392048: step 4031, loss 0.00444223, acc 1
2016-11-12T19:11:04.448445: step 4032, loss 0.0414189, acc 0.984375
2016-11-12T19:11:04.506191: step 4033, loss 0.00830796, acc 1
2016-11-12T19:11:04.563858: step 4034, loss 0.00524054, acc 1
2016-11-12T19:11:04.619675: step 4035, loss 0.0045374, acc 1
2016-11-12T19:11:04.676653: step 4036, loss 0.00467712, acc 1
2016-11-12T19:11:04.735393: step 4037, loss 0.00243443, acc 1
2016-11-12T19:11:04.792215: step 4038, loss 0.0364836, acc 0.984375
2016-11-12T19:11:04.849133: step 4039, loss 0.00413581, acc 1
2016-11-12T19:11:04.909143: step 4040, loss 0.00284441, acc 1
2016-11-12T19:11:04.969676: step 4041, loss 0.0485937, acc 0.96875
2016-11-12T19:11:05.026579: step 4042, loss 0.00305086, acc 1
2016-11-12T19:11:05.082752: step 4043, loss 0.00770393, acc 1
2016-11-12T19:11:05.140161: step 4044, loss 0.0089896, acc 1
2016-11-12T19:11:05.200258: step 4045, loss 0.0108346, acc 1
2016-11-12T19:11:05.256901: step 4046, loss 0.00993002, acc 1
2016-11-12T19:11:05.295421: step 4047, loss 0.0077192, acc 1
2016-11-12T19:11:05.354507: step 4048, loss 0.00414397, acc 1
2016-11-12T19:11:05.411915: step 4049, loss 0.00181691, acc 1
2016-11-12T19:11:05.467556: step 4050, loss 0.0023999, acc 1
2016-11-12T19:11:05.523986: step 4051, loss 0.00563549, acc 1
2016-11-12T19:11:05.580755: step 4052, loss 0.00376984, acc 1
2016-11-12T19:11:05.637180: step 4053, loss 0.00102804, acc 1
2016-11-12T19:11:05.697009: step 4054, loss 0.0211694, acc 0.984375
2016-11-12T19:11:05.753655: step 4055, loss 0.00131667, acc 1
2016-11-12T19:11:05.809856: step 4056, loss 0.001327, acc 1
2016-11-12T19:11:05.866276: step 4057, loss 0.00240483, acc 1
2016-11-12T19:11:05.923227: step 4058, loss 0.0395403, acc 0.984375
2016-11-12T19:11:05.980674: step 4059, loss 0.0178039, acc 1
2016-11-12T19:11:06.037405: step 4060, loss 0.00317661, acc 1
2016-11-12T19:11:06.096584: step 4061, loss 0.00120781, acc 1
2016-11-12T19:11:06.153431: step 4062, loss 0.0155472, acc 1
2016-11-12T19:11:06.213675: step 4063, loss 0.00390211, acc 1
2016-11-12T19:11:06.269562: step 4064, loss 0.00518564, acc 1
2016-11-12T19:11:06.328218: step 4065, loss 0.034377, acc 0.984375
2016-11-12T19:11:06.385574: step 4066, loss 0.0207231, acc 0.984375
2016-11-12T19:11:06.441832: step 4067, loss 0.0291029, acc 0.984375
2016-11-12T19:11:06.501912: step 4068, loss 0.000719475, acc 1
2016-11-12T19:11:06.557222: step 4069, loss 0.033563, acc 0.984375
2016-11-12T19:11:06.615004: step 4070, loss 0.00377563, acc 1
2016-11-12T19:11:06.670640: step 4071, loss 0.00112745, acc 1
2016-11-12T19:11:06.727036: step 4072, loss 0.0123615, acc 0.984375
2016-11-12T19:11:06.783560: step 4073, loss 0.00313318, acc 1
2016-11-12T19:11:06.840608: step 4074, loss 0.0652675, acc 0.984375
2016-11-12T19:11:06.897099: step 4075, loss 0.0367715, acc 0.984375
2016-11-12T19:11:06.955013: step 4076, loss 0.0175077, acc 0.984375
2016-11-12T19:11:07.013558: step 4077, loss 0.0122805, acc 1
2016-11-12T19:11:07.069963: step 4078, loss 0.000130454, acc 1
2016-11-12T19:11:07.126535: step 4079, loss 0.0112346, acc 1
2016-11-12T19:11:07.185938: step 4080, loss 0.0146659, acc 1
2016-11-12T19:11:07.242307: step 4081, loss 0.0124537, acc 0.984375
2016-11-12T19:11:07.301478: step 4082, loss 0.0591762, acc 0.96875
2016-11-12T19:11:07.358718: step 4083, loss 0.0120864, acc 0.984375
2016-11-12T19:11:07.417103: step 4084, loss 0.0146833, acc 1
2016-11-12T19:11:07.474864: step 4085, loss 0.022612, acc 0.984375
2016-11-12T19:11:07.532829: step 4086, loss 0.0256747, acc 0.984375
2016-11-12T19:11:07.589921: step 4087, loss 0.00147009, acc 1
2016-11-12T19:11:07.648888: step 4088, loss 0.0552012, acc 0.984375
2016-11-12T19:11:07.709888: step 4089, loss 0.00446678, acc 1
2016-11-12T19:11:07.767622: step 4090, loss 0.00307701, acc 1
2016-11-12T19:11:07.826109: step 4091, loss 0.00982943, acc 1
2016-11-12T19:11:07.882622: step 4092, loss 0.122693, acc 0.984375
2016-11-12T19:11:07.940338: step 4093, loss 0.0090578, acc 1
2016-11-12T19:11:07.996433: step 4094, loss 0.00593494, acc 1
2016-11-12T19:11:08.053240: step 4095, loss 0.00613422, acc 1
2016-11-12T19:11:08.108983: step 4096, loss 0.00303657, acc 1
2016-11-12T19:11:08.165317: step 4097, loss 0.0105569, acc 1
2016-11-12T19:11:08.220700: step 4098, loss 0.00791128, acc 1
2016-11-12T19:11:08.280827: step 4099, loss 0.0130497, acc 1
2016-11-12T19:11:08.337359: step 4100, loss 0.01897, acc 0.984375

Evaluation:
2016-11-12T19:11:08.408091: step 4100, loss 2.40769, acc 0.544

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-4100

2016-11-12T19:11:08.912940: step 4101, loss 0.0521605, acc 0.984375
2016-11-12T19:11:08.970774: step 4102, loss 0.0177314, acc 0.984375
2016-11-12T19:11:09.028993: step 4103, loss 0.0467962, acc 0.984375
2016-11-12T19:11:09.088084: step 4104, loss 0.00806606, acc 1
2016-11-12T19:11:09.145640: step 4105, loss 0.0304278, acc 0.984375
2016-11-12T19:11:09.203326: step 4106, loss 0.022793, acc 0.984375
2016-11-12T19:11:09.260725: step 4107, loss 0.0245442, acc 0.984375
2016-11-12T19:11:09.317150: step 4108, loss 0.00742022, acc 1
2016-11-12T19:11:09.374750: step 4109, loss 0.0044143, acc 1
2016-11-12T19:11:09.430655: step 4110, loss 0.00303363, acc 1
2016-11-12T19:11:09.487622: step 4111, loss 0.0131886, acc 1
2016-11-12T19:11:09.546670: step 4112, loss 0.0129991, acc 1
2016-11-12T19:11:09.604261: step 4113, loss 0.0105404, acc 1
2016-11-12T19:11:09.663366: step 4114, loss 0.00273592, acc 1
2016-11-12T19:11:09.721025: step 4115, loss 0.010009, acc 1
2016-11-12T19:11:09.776856: step 4116, loss 0.0598628, acc 0.96875
2016-11-12T19:11:09.833393: step 4117, loss 0.00395279, acc 1
2016-11-12T19:11:09.875005: step 4118, loss 0.058007, acc 0.95
2016-11-12T19:11:09.936334: step 4119, loss 0.00253372, acc 1
2016-11-12T19:11:09.992338: step 4120, loss 0.00685595, acc 1
2016-11-12T19:11:10.049833: step 4121, loss 0.00153653, acc 1
2016-11-12T19:11:10.106299: step 4122, loss 0.00237888, acc 1
2016-11-12T19:11:10.164446: step 4123, loss 0.00533615, acc 1
2016-11-12T19:11:10.222137: step 4124, loss 0.0109984, acc 1
2016-11-12T19:11:10.279169: step 4125, loss 0.00307658, acc 1
2016-11-12T19:11:10.335552: step 4126, loss 0.00165404, acc 1
2016-11-12T19:11:10.394137: step 4127, loss 0.0511929, acc 0.984375
2016-11-12T19:11:10.452302: step 4128, loss 0.0112132, acc 1
2016-11-12T19:11:10.509149: step 4129, loss 0.00209377, acc 1
2016-11-12T19:11:10.564716: step 4130, loss 0.00870281, acc 1
2016-11-12T19:11:10.621312: step 4131, loss 0.00705361, acc 1
2016-11-12T19:11:10.678228: step 4132, loss 0.0531757, acc 0.984375
2016-11-12T19:11:10.737436: step 4133, loss 0.0151981, acc 1
2016-11-12T19:11:10.797891: step 4134, loss 0.0169318, acc 0.984375
2016-11-12T19:11:10.855382: step 4135, loss 0.00347567, acc 1
2016-11-12T19:11:10.913066: step 4136, loss 0.0290634, acc 0.984375
2016-11-12T19:11:10.971650: step 4137, loss 0.0298035, acc 0.984375
2016-11-12T19:11:11.028623: step 4138, loss 0.0846458, acc 0.984375
2016-11-12T19:11:11.085121: step 4139, loss 0.0131661, acc 1
2016-11-12T19:11:11.144719: step 4140, loss 0.0115962, acc 1
2016-11-12T19:11:11.201349: step 4141, loss 0.00403803, acc 1
2016-11-12T19:11:11.258794: step 4142, loss 0.00129937, acc 1
2016-11-12T19:11:11.314447: step 4143, loss 0.00747954, acc 1
2016-11-12T19:11:11.372927: step 4144, loss 0.0430395, acc 0.984375
2016-11-12T19:11:11.431854: step 4145, loss 0.0711837, acc 0.984375
2016-11-12T19:11:11.488636: step 4146, loss 0.0347266, acc 0.984375
2016-11-12T19:11:11.546275: step 4147, loss 0.00445286, acc 1
2016-11-12T19:11:11.603261: step 4148, loss 0.0141838, acc 1
2016-11-12T19:11:11.661872: step 4149, loss 0.0345097, acc 0.984375
2016-11-12T19:11:11.719465: step 4150, loss 0.00524574, acc 1
2016-11-12T19:11:11.775018: step 4151, loss 0.0329746, acc 0.984375
2016-11-12T19:11:11.834774: step 4152, loss 0.00157387, acc 1
2016-11-12T19:11:11.891720: step 4153, loss 0.0119671, acc 1
2016-11-12T19:11:11.948303: step 4154, loss 0.0356707, acc 1
2016-11-12T19:11:12.005559: step 4155, loss 0.00436573, acc 1
2016-11-12T19:11:12.063663: step 4156, loss 0.0966238, acc 0.96875
2016-11-12T19:11:12.121451: step 4157, loss 0.0384489, acc 0.984375
2016-11-12T19:11:12.179125: step 4158, loss 0.0022331, acc 1
2016-11-12T19:11:12.236536: step 4159, loss 0.00427193, acc 1
2016-11-12T19:11:12.296625: step 4160, loss 0.0199378, acc 1
2016-11-12T19:11:12.353035: step 4161, loss 0.0354587, acc 0.984375
2016-11-12T19:11:12.409431: step 4162, loss 0.0186469, acc 0.984375
2016-11-12T19:11:12.466619: step 4163, loss 0.00177259, acc 1
2016-11-12T19:11:12.522813: step 4164, loss 0.00764248, acc 1
2016-11-12T19:11:12.578822: step 4165, loss 0.0777475, acc 0.96875
2016-11-12T19:11:12.637076: step 4166, loss 0.0111334, acc 1
2016-11-12T19:11:12.696140: step 4167, loss 0.0022402, acc 1
2016-11-12T19:11:12.751611: step 4168, loss 0.00399618, acc 1
2016-11-12T19:11:12.810866: step 4169, loss 0.00112896, acc 1
2016-11-12T19:11:12.868057: step 4170, loss 0.0107139, acc 1
2016-11-12T19:11:12.925485: step 4171, loss 0.0574002, acc 0.984375
2016-11-12T19:11:12.984177: step 4172, loss 0.00577581, acc 1
2016-11-12T19:11:13.041369: step 4173, loss 0.00238554, acc 1
2016-11-12T19:11:13.096970: step 4174, loss 0.0268694, acc 0.984375
2016-11-12T19:11:13.153543: step 4175, loss 0.0259246, acc 0.984375
2016-11-12T19:11:13.210133: step 4176, loss 0.00693865, acc 1
2016-11-12T19:11:13.271081: step 4177, loss 0.00743423, acc 1
2016-11-12T19:11:13.328695: step 4178, loss 0.0388252, acc 0.984375
2016-11-12T19:11:13.388773: step 4179, loss 0.0179251, acc 0.984375
2016-11-12T19:11:13.448718: step 4180, loss 0.00230252, acc 1
2016-11-12T19:11:13.506226: step 4181, loss 0.010025, acc 1
2016-11-12T19:11:13.565010: step 4182, loss 0.0152504, acc 0.984375
2016-11-12T19:11:13.622524: step 4183, loss 0.00323275, acc 1
2016-11-12T19:11:13.682332: step 4184, loss 0.0762917, acc 0.96875
2016-11-12T19:11:13.739587: step 4185, loss 0.000792191, acc 1
2016-11-12T19:11:13.796203: step 4186, loss 0.294362, acc 0.96875
2016-11-12T19:11:13.853080: step 4187, loss 0.00132229, acc 1
2016-11-12T19:11:13.910079: step 4188, loss 0.0363792, acc 0.96875
2016-11-12T19:11:13.948657: step 4189, loss 8.87571e-05, acc 1
2016-11-12T19:11:14.004597: step 4190, loss 0.0228245, acc 0.984375
2016-11-12T19:11:14.062046: step 4191, loss 0.0725203, acc 0.96875
2016-11-12T19:11:14.121399: step 4192, loss 0.0120045, acc 1
2016-11-12T19:11:14.178087: step 4193, loss 0.00103945, acc 1
2016-11-12T19:11:14.236906: step 4194, loss 0.00554375, acc 1
2016-11-12T19:11:14.297616: step 4195, loss 0.038086, acc 0.96875
2016-11-12T19:11:14.354433: step 4196, loss 0.00089331, acc 1
2016-11-12T19:11:14.409641: step 4197, loss 0.222138, acc 0.984375
2016-11-12T19:11:14.467040: step 4198, loss 0.00175938, acc 1
2016-11-12T19:11:14.524104: step 4199, loss 0.0129865, acc 1
2016-11-12T19:11:14.581350: step 4200, loss 0.00432006, acc 1

Evaluation:
2016-11-12T19:11:14.652091: step 4200, loss 2.31382, acc 0.544

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-4200

2016-11-12T19:11:15.155005: step 4201, loss 0.0132555, acc 0.984375
2016-11-12T19:11:15.213492: step 4202, loss 0.00298063, acc 1
2016-11-12T19:11:15.270021: step 4203, loss 0.039011, acc 0.984375
2016-11-12T19:11:15.330374: step 4204, loss 0.0171669, acc 0.984375
2016-11-12T19:11:15.389153: step 4205, loss 0.0456591, acc 0.984375
2016-11-12T19:11:15.446309: step 4206, loss 0.0044426, acc 1
2016-11-12T19:11:15.502858: step 4207, loss 0.00403196, acc 1
2016-11-12T19:11:15.561545: step 4208, loss 0.012302, acc 1
2016-11-12T19:11:15.618282: step 4209, loss 0.00815989, acc 1
2016-11-12T19:11:15.676164: step 4210, loss 0.0202481, acc 0.984375
2016-11-12T19:11:15.734629: step 4211, loss 0.00502948, acc 1
2016-11-12T19:11:15.792937: step 4212, loss 0.00177716, acc 1
2016-11-12T19:11:15.849578: step 4213, loss 0.126177, acc 0.984375
2016-11-12T19:11:15.907999: step 4214, loss 0.0654925, acc 0.96875
2016-11-12T19:11:15.967864: step 4215, loss 0.00430148, acc 1
2016-11-12T19:11:16.024513: step 4216, loss 0.00259361, acc 1
2016-11-12T19:11:16.082980: step 4217, loss 0.0780407, acc 0.96875
2016-11-12T19:11:16.139778: step 4218, loss 0.0400128, acc 0.984375
2016-11-12T19:11:16.198518: step 4219, loss 0.0209143, acc 0.984375
2016-11-12T19:11:16.255722: step 4220, loss 0.00835086, acc 1
2016-11-12T19:11:16.312556: step 4221, loss 0.026133, acc 0.984375
2016-11-12T19:11:16.372992: step 4222, loss 0.000975318, acc 1
2016-11-12T19:11:16.429148: step 4223, loss 0.0141232, acc 0.984375
2016-11-12T19:11:16.489367: step 4224, loss 0.00463893, acc 1
2016-11-12T19:11:16.545817: step 4225, loss 0.0168319, acc 0.984375
2016-11-12T19:11:16.605293: step 4226, loss 0.0078175, acc 1
2016-11-12T19:11:16.664007: step 4227, loss 0.008338, acc 1
2016-11-12T19:11:16.719647: step 4228, loss 0.0162098, acc 1
2016-11-12T19:11:16.777003: step 4229, loss 0.00427823, acc 1
2016-11-12T19:11:16.834569: step 4230, loss 0.00633717, acc 1
2016-11-12T19:11:16.890886: step 4231, loss 0.00873806, acc 1
2016-11-12T19:11:16.949361: step 4232, loss 0.000980418, acc 1
2016-11-12T19:11:17.005225: step 4233, loss 0.0024316, acc 1
2016-11-12T19:11:17.061544: step 4234, loss 0.00296583, acc 1
2016-11-12T19:11:17.119566: step 4235, loss 0.00602937, acc 1
2016-11-12T19:11:17.179260: step 4236, loss 0.0011157, acc 1
2016-11-12T19:11:17.237969: step 4237, loss 0.0117768, acc 1
2016-11-12T19:11:17.297195: step 4238, loss 0.00291813, acc 1
2016-11-12T19:11:17.353701: step 4239, loss 0.0024004, acc 1
2016-11-12T19:11:17.410084: step 4240, loss 0.00259777, acc 1
2016-11-12T19:11:17.468294: step 4241, loss 0.0142569, acc 1
2016-11-12T19:11:17.525781: step 4242, loss 0.0197006, acc 0.984375
2016-11-12T19:11:17.584330: step 4243, loss 0.00364082, acc 1
2016-11-12T19:11:17.643145: step 4244, loss 0.00476543, acc 1
2016-11-12T19:11:17.701772: step 4245, loss 0.00128198, acc 1
2016-11-12T19:11:17.757822: step 4246, loss 0.011554, acc 0.984375
2016-11-12T19:11:17.816685: step 4247, loss 0.0227773, acc 0.984375
2016-11-12T19:11:17.881171: step 4248, loss 0.0541012, acc 0.96875
2016-11-12T19:11:17.938976: step 4249, loss 0.00386356, acc 1
2016-11-12T19:11:17.995194: step 4250, loss 0.00436149, acc 1
2016-11-12T19:11:18.051882: step 4251, loss 0.00968406, acc 1
2016-11-12T19:11:18.108432: step 4252, loss 0.0234999, acc 0.984375
2016-11-12T19:11:18.165443: step 4253, loss 0.00298665, acc 1
2016-11-12T19:11:18.221190: step 4254, loss 0.00513371, acc 1
2016-11-12T19:11:18.281287: step 4255, loss 0.00390639, acc 1
2016-11-12T19:11:18.337646: step 4256, loss 0.069636, acc 0.984375
2016-11-12T19:11:18.397317: step 4257, loss 0.0583373, acc 0.984375
2016-11-12T19:11:18.459526: step 4258, loss 0.0147181, acc 1
2016-11-12T19:11:18.519891: step 4259, loss 0.00775337, acc 1
2016-11-12T19:11:18.558927: step 4260, loss 0.000359173, acc 1
2016-11-12T19:11:18.616810: step 4261, loss 0.0265199, acc 0.984375
2016-11-12T19:11:18.674799: step 4262, loss 0.00245118, acc 1
2016-11-12T19:11:18.735622: step 4263, loss 0.0479706, acc 0.984375
2016-11-12T19:11:18.797057: step 4264, loss 0.00709982, acc 1
2016-11-12T19:11:18.853260: step 4265, loss 0.0110417, acc 1
2016-11-12T19:11:18.910058: step 4266, loss 0.0283667, acc 0.984375
2016-11-12T19:11:18.966751: step 4267, loss 0.00964032, acc 1
2016-11-12T19:11:19.024999: step 4268, loss 0.00477328, acc 1
2016-11-12T19:11:19.081592: step 4269, loss 0.00104322, acc 1
2016-11-12T19:11:19.137653: step 4270, loss 0.00365316, acc 1
2016-11-12T19:11:19.194442: step 4271, loss 0.0299013, acc 0.984375
2016-11-12T19:11:19.252983: step 4272, loss 0.00170651, acc 1
2016-11-12T19:11:19.310204: step 4273, loss 0.0540582, acc 0.96875
2016-11-12T19:11:19.369639: step 4274, loss 0.0454399, acc 0.984375
2016-11-12T19:11:19.427216: step 4275, loss 0.00214919, acc 1
2016-11-12T19:11:19.483674: step 4276, loss 0.00172188, acc 1
2016-11-12T19:11:19.541103: step 4277, loss 0.00520693, acc 1
2016-11-12T19:11:19.597481: step 4278, loss 0.0146852, acc 1
2016-11-12T19:11:19.657133: step 4279, loss 0.00554748, acc 1
2016-11-12T19:11:19.713863: step 4280, loss 0.00280511, acc 1
2016-11-12T19:11:19.773139: step 4281, loss 0.00285987, acc 1
2016-11-12T19:11:19.832674: step 4282, loss 0.0306305, acc 0.984375
2016-11-12T19:11:19.889779: step 4283, loss 0.010568, acc 1
2016-11-12T19:11:19.946269: step 4284, loss 0.00490605, acc 1
2016-11-12T19:11:20.003154: step 4285, loss 0.00449884, acc 1
2016-11-12T19:11:20.060846: step 4286, loss 0.00374504, acc 1
2016-11-12T19:11:20.117757: step 4287, loss 0.00525701, acc 1
2016-11-12T19:11:20.174931: step 4288, loss 0.00498435, acc 1
2016-11-12T19:11:20.231704: step 4289, loss 0.0061825, acc 1
2016-11-12T19:11:20.289052: step 4290, loss 0.0032159, acc 1
2016-11-12T19:11:20.348151: step 4291, loss 0.00765262, acc 1
2016-11-12T19:11:20.403968: step 4292, loss 0.00582248, acc 1
2016-11-12T19:11:20.461065: step 4293, loss 0.00677545, acc 1
2016-11-12T19:11:20.517420: step 4294, loss 0.012048, acc 1
2016-11-12T19:11:20.577342: step 4295, loss 0.00394921, acc 1
2016-11-12T19:11:20.633249: step 4296, loss 0.00138159, acc 1
2016-11-12T19:11:20.690802: step 4297, loss 0.0416733, acc 0.96875
2016-11-12T19:11:20.749049: step 4298, loss 0.00777319, acc 1
2016-11-12T19:11:20.806143: step 4299, loss 0.0355291, acc 0.984375
2016-11-12T19:11:20.864782: step 4300, loss 0.00338693, acc 1

Evaluation:
2016-11-12T19:11:20.935273: step 4300, loss 2.32515, acc 0.556

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-4300

2016-11-12T19:11:21.441758: step 4301, loss 0.00133134, acc 1
2016-11-12T19:11:21.497674: step 4302, loss 0.0398268, acc 0.984375
2016-11-12T19:11:21.557015: step 4303, loss 0.103812, acc 0.984375
2016-11-12T19:11:21.616419: step 4304, loss 0.0195077, acc 1
2016-11-12T19:11:21.675571: step 4305, loss 0.00380361, acc 1
2016-11-12T19:11:21.732019: step 4306, loss 0.00309244, acc 1
2016-11-12T19:11:21.787027: step 4307, loss 0.00391968, acc 1
2016-11-12T19:11:21.844682: step 4308, loss 0.00641165, acc 1
2016-11-12T19:11:21.901554: step 4309, loss 0.023878, acc 0.984375
2016-11-12T19:11:21.957833: step 4310, loss 0.0140213, acc 1
2016-11-12T19:11:22.014349: step 4311, loss 0.011718, acc 1
2016-11-12T19:11:22.072713: step 4312, loss 0.00585655, acc 1
2016-11-12T19:11:22.130389: step 4313, loss 0.00111796, acc 1
2016-11-12T19:11:22.189427: step 4314, loss 0.0430142, acc 0.984375
2016-11-12T19:11:22.246691: step 4315, loss 0.0290263, acc 0.984375
2016-11-12T19:11:22.304674: step 4316, loss 0.0159047, acc 1
2016-11-12T19:11:22.361742: step 4317, loss 0.100189, acc 0.984375
2016-11-12T19:11:22.419819: step 4318, loss 0.00270213, acc 1
2016-11-12T19:11:22.477668: step 4319, loss 0.0143334, acc 0.984375
2016-11-12T19:11:22.536931: step 4320, loss 0.0207235, acc 0.984375
2016-11-12T19:11:22.593884: step 4321, loss 0.0100198, acc 1
2016-11-12T19:11:22.650800: step 4322, loss 0.00472559, acc 1
2016-11-12T19:11:22.709121: step 4323, loss 0.00347672, acc 1
2016-11-12T19:11:22.765256: step 4324, loss 0.0229129, acc 0.984375
2016-11-12T19:11:22.823985: step 4325, loss 0.0297325, acc 0.984375
2016-11-12T19:11:22.880452: step 4326, loss 0.0292144, acc 0.984375
2016-11-12T19:11:22.937273: step 4327, loss 0.00537164, acc 1
2016-11-12T19:11:22.995171: step 4328, loss 0.00154058, acc 1
2016-11-12T19:11:23.053108: step 4329, loss 0.0533691, acc 0.984375
2016-11-12T19:11:23.111745: step 4330, loss 0.00954903, acc 1
2016-11-12T19:11:23.152314: step 4331, loss 0.00540724, acc 1
2016-11-12T19:11:23.210041: step 4332, loss 0.00251386, acc 1
2016-11-12T19:11:23.266162: step 4333, loss 0.00211652, acc 1
2016-11-12T19:11:23.322841: step 4334, loss 0.0019604, acc 1
2016-11-12T19:11:23.379197: step 4335, loss 0.00366259, acc 1
2016-11-12T19:11:23.436649: step 4336, loss 0.0335851, acc 0.984375
2016-11-12T19:11:23.496184: step 4337, loss 0.0114661, acc 1
2016-11-12T19:11:23.554516: step 4338, loss 0.0301334, acc 0.984375
2016-11-12T19:11:23.613142: step 4339, loss 0.00506648, acc 1
2016-11-12T19:11:23.671776: step 4340, loss 0.00344396, acc 1
2016-11-12T19:11:23.728420: step 4341, loss 0.0300421, acc 0.984375
2016-11-12T19:11:23.789078: step 4342, loss 0.017591, acc 1
2016-11-12T19:11:23.847570: step 4343, loss 0.042206, acc 0.984375
2016-11-12T19:11:23.905151: step 4344, loss 0.0135672, acc 1
2016-11-12T19:11:23.960817: step 4345, loss 0.00513545, acc 1
2016-11-12T19:11:24.022978: step 4346, loss 0.00359044, acc 1
2016-11-12T19:11:24.082536: step 4347, loss 0.0358223, acc 0.984375
2016-11-12T19:11:24.140141: step 4348, loss 0.00213624, acc 1
2016-11-12T19:11:24.195494: step 4349, loss 0.00360455, acc 1
2016-11-12T19:11:24.253304: step 4350, loss 0.0078264, acc 1
2016-11-12T19:11:24.310179: step 4351, loss 0.00195406, acc 1
2016-11-12T19:11:24.366241: step 4352, loss 0.0256818, acc 0.984375
2016-11-12T19:11:24.423842: step 4353, loss 0.0129179, acc 1
2016-11-12T19:11:24.480559: step 4354, loss 0.00254727, acc 1
2016-11-12T19:11:24.536665: step 4355, loss 0.00313786, acc 1
2016-11-12T19:11:24.592709: step 4356, loss 0.00221125, acc 1
2016-11-12T19:11:24.648541: step 4357, loss 0.065397, acc 0.96875
2016-11-12T19:11:24.705740: step 4358, loss 0.00380458, acc 1
2016-11-12T19:11:24.765375: step 4359, loss 0.0267513, acc 0.984375
2016-11-12T19:11:24.824789: step 4360, loss 0.00175891, acc 1
2016-11-12T19:11:24.882103: step 4361, loss 0.0769101, acc 0.96875
2016-11-12T19:11:24.940945: step 4362, loss 0.00107685, acc 1
2016-11-12T19:11:24.997573: step 4363, loss 0.00334091, acc 1
2016-11-12T19:11:25.053962: step 4364, loss 0.0447485, acc 0.96875
2016-11-12T19:11:25.110098: step 4365, loss 0.00969946, acc 1
2016-11-12T19:11:25.167902: step 4366, loss 0.0204782, acc 1
2016-11-12T19:11:25.224973: step 4367, loss 0.00258221, acc 1
2016-11-12T19:11:25.281721: step 4368, loss 0.0124803, acc 1
2016-11-12T19:11:25.341629: step 4369, loss 0.0592664, acc 0.984375
2016-11-12T19:11:25.402379: step 4370, loss 0.0357441, acc 0.984375
2016-11-12T19:11:25.460177: step 4371, loss 0.00654807, acc 1
2016-11-12T19:11:25.520485: step 4372, loss 0.00755912, acc 1
2016-11-12T19:11:25.577806: step 4373, loss 0.00460761, acc 1
2016-11-12T19:11:25.637250: step 4374, loss 0.00276506, acc 1
2016-11-12T19:11:25.692788: step 4375, loss 0.000687524, acc 1
2016-11-12T19:11:25.749006: step 4376, loss 0.0048266, acc 1
2016-11-12T19:11:25.805529: step 4377, loss 0.0343553, acc 0.96875
2016-11-12T19:11:25.864020: step 4378, loss 0.00511259, acc 1
2016-11-12T19:11:25.921076: step 4379, loss 0.00726465, acc 1
2016-11-12T19:11:25.977455: step 4380, loss 0.0628159, acc 0.984375
2016-11-12T19:11:26.036279: step 4381, loss 0.00381837, acc 1
2016-11-12T19:11:26.092468: step 4382, loss 0.00737329, acc 1
2016-11-12T19:11:26.151555: step 4383, loss 0.00495693, acc 1
2016-11-12T19:11:26.210794: step 4384, loss 0.00777415, acc 1
2016-11-12T19:11:26.269075: step 4385, loss 0.160646, acc 0.953125
2016-11-12T19:11:26.328175: step 4386, loss 0.00443912, acc 1
2016-11-12T19:11:26.385520: step 4387, loss 0.0166436, acc 1
2016-11-12T19:11:26.446107: step 4388, loss 0.0241375, acc 0.984375
2016-11-12T19:11:26.505429: step 4389, loss 0.00113799, acc 1
2016-11-12T19:11:26.562620: step 4390, loss 0.00106373, acc 1
2016-11-12T19:11:26.621423: step 4391, loss 0.00192112, acc 1
2016-11-12T19:11:26.679680: step 4392, loss 0.0182283, acc 0.984375
2016-11-12T19:11:26.738083: step 4393, loss 0.00760185, acc 1
2016-11-12T19:11:26.796547: step 4394, loss 0.0188953, acc 1
2016-11-12T19:11:26.852932: step 4395, loss 0.0164953, acc 1
2016-11-12T19:11:26.912928: step 4396, loss 0.0794439, acc 0.984375
2016-11-12T19:11:26.970656: step 4397, loss 0.00325456, acc 1
2016-11-12T19:11:27.027940: step 4398, loss 0.0138087, acc 0.984375
2016-11-12T19:11:27.084697: step 4399, loss 0.0101418, acc 1
2016-11-12T19:11:27.141013: step 4400, loss 0.0402611, acc 0.96875

Evaluation:
2016-11-12T19:11:27.211760: step 4400, loss 2.33123, acc 0.56

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-4400

2016-11-12T19:11:27.716201: step 4401, loss 0.00277164, acc 1
2016-11-12T19:11:27.753258: step 4402, loss 0.0029364, acc 1
2016-11-12T19:11:27.812046: step 4403, loss 0.00293283, acc 1
2016-11-12T19:11:27.869802: step 4404, loss 0.0192753, acc 0.984375
2016-11-12T19:11:27.928587: step 4405, loss 0.029307, acc 0.984375
2016-11-12T19:11:27.985738: step 4406, loss 0.0780115, acc 0.96875
2016-11-12T19:11:28.044188: step 4407, loss 0.00646019, acc 1
2016-11-12T19:11:28.100576: step 4408, loss 0.00818947, acc 1
2016-11-12T19:11:28.157160: step 4409, loss 0.0088555, acc 1
2016-11-12T19:11:28.214059: step 4410, loss 0.00461566, acc 1
2016-11-12T19:11:28.271452: step 4411, loss 0.0246755, acc 0.984375
2016-11-12T19:11:28.330197: step 4412, loss 0.00933981, acc 1
2016-11-12T19:11:28.389499: step 4413, loss 0.00914987, acc 1
2016-11-12T19:11:28.449298: step 4414, loss 0.00526116, acc 1
2016-11-12T19:11:28.506157: step 4415, loss 0.00857964, acc 1
2016-11-12T19:11:28.564401: step 4416, loss 0.00576992, acc 1
2016-11-12T19:11:28.621350: step 4417, loss 0.00911566, acc 1
2016-11-12T19:11:28.678787: step 4418, loss 0.0060799, acc 1
2016-11-12T19:11:28.734518: step 4419, loss 0.00379097, acc 1
2016-11-12T19:11:28.794302: step 4420, loss 0.0232636, acc 0.984375
2016-11-12T19:11:28.851947: step 4421, loss 0.0047353, acc 1
2016-11-12T19:11:28.909280: step 4422, loss 0.00416299, acc 1
2016-11-12T19:11:28.966433: step 4423, loss 0.00194787, acc 1
2016-11-12T19:11:29.025198: step 4424, loss 0.0193639, acc 0.984375
2016-11-12T19:11:29.081988: step 4425, loss 0.0146515, acc 0.984375
2016-11-12T19:11:29.139412: step 4426, loss 0.206608, acc 0.953125
2016-11-12T19:11:29.198330: step 4427, loss 0.00863816, acc 1
2016-11-12T19:11:29.256616: step 4428, loss 0.00178963, acc 1
2016-11-12T19:11:29.313471: step 4429, loss 0.00124977, acc 1
2016-11-12T19:11:29.372313: step 4430, loss 0.00249053, acc 1
2016-11-12T19:11:29.428307: step 4431, loss 0.0106539, acc 1
2016-11-12T19:11:29.485701: step 4432, loss 0.0302042, acc 0.984375
2016-11-12T19:11:29.543710: step 4433, loss 0.00877741, acc 1
2016-11-12T19:11:29.601572: step 4434, loss 0.000674102, acc 1
2016-11-12T19:11:29.657670: step 4435, loss 0.00311694, acc 1
2016-11-12T19:11:29.715992: step 4436, loss 0.0705551, acc 0.96875
2016-11-12T19:11:29.774066: step 4437, loss 0.00190369, acc 1
2016-11-12T19:11:29.829993: step 4438, loss 0.027315, acc 0.984375
2016-11-12T19:11:29.888260: step 4439, loss 0.00133817, acc 1
2016-11-12T19:11:29.944130: step 4440, loss 0.00946105, acc 1
2016-11-12T19:11:30.001310: step 4441, loss 0.00345609, acc 1
2016-11-12T19:11:30.057700: step 4442, loss 0.0141808, acc 1
2016-11-12T19:11:30.115697: step 4443, loss 0.00217059, acc 1
2016-11-12T19:11:30.173493: step 4444, loss 0.00258427, acc 1
2016-11-12T19:11:30.232273: step 4445, loss 0.000767622, acc 1
2016-11-12T19:11:30.287938: step 4446, loss 0.00538052, acc 1
2016-11-12T19:11:30.345982: step 4447, loss 0.0564721, acc 0.96875
2016-11-12T19:11:30.404235: step 4448, loss 0.00214202, acc 1
2016-11-12T19:11:30.460348: step 4449, loss 0.00980498, acc 1
2016-11-12T19:11:30.518186: step 4450, loss 0.00353246, acc 1
2016-11-12T19:11:30.577532: step 4451, loss 0.0487658, acc 0.96875
2016-11-12T19:11:30.635291: step 4452, loss 0.00218238, acc 1
2016-11-12T19:11:30.693650: step 4453, loss 0.00412533, acc 1
2016-11-12T19:11:30.752401: step 4454, loss 0.0392379, acc 0.96875
2016-11-12T19:11:30.811810: step 4455, loss 0.0152111, acc 0.984375
2016-11-12T19:11:30.869562: step 4456, loss 0.00408471, acc 1
2016-11-12T19:11:30.930480: step 4457, loss 0.00557763, acc 1
2016-11-12T19:11:30.988361: step 4458, loss 0.0016485, acc 1
2016-11-12T19:11:31.045337: step 4459, loss 0.0163865, acc 0.984375
2016-11-12T19:11:31.104122: step 4460, loss 0.0126023, acc 1
2016-11-12T19:11:31.160882: step 4461, loss 0.0175951, acc 1
2016-11-12T19:11:31.216393: step 4462, loss 0.00437019, acc 1
2016-11-12T19:11:31.273079: step 4463, loss 0.00987269, acc 1
2016-11-12T19:11:31.330167: step 4464, loss 0.00214098, acc 1
2016-11-12T19:11:31.388185: step 4465, loss 0.0156026, acc 1
2016-11-12T19:11:31.445076: step 4466, loss 0.0125726, acc 1
2016-11-12T19:11:31.500703: step 4467, loss 0.0102574, acc 1
2016-11-12T19:11:31.556860: step 4468, loss 0.00447839, acc 1
2016-11-12T19:11:31.613092: step 4469, loss 0.00635261, acc 1
2016-11-12T19:11:31.669748: step 4470, loss 0.00391428, acc 1
2016-11-12T19:11:31.728717: step 4471, loss 0.0080209, acc 1
2016-11-12T19:11:31.785406: step 4472, loss 0.00073482, acc 1
2016-11-12T19:11:31.823265: step 4473, loss 0.00513061, acc 1
2016-11-12T19:11:31.882937: step 4474, loss 0.00873411, acc 1
2016-11-12T19:11:31.941328: step 4475, loss 0.00692489, acc 1
2016-11-12T19:11:31.998015: step 4476, loss 0.0282642, acc 0.984375
2016-11-12T19:11:32.054689: step 4477, loss 0.00637238, acc 1
2016-11-12T19:11:32.112954: step 4478, loss 0.0216575, acc 0.984375
2016-11-12T19:11:32.173230: step 4479, loss 0.0564847, acc 0.984375
2016-11-12T19:11:32.230810: step 4480, loss 0.00380984, acc 1
2016-11-12T19:11:32.288712: step 4481, loss 0.00473193, acc 1
2016-11-12T19:11:32.348523: step 4482, loss 0.0526997, acc 0.984375
2016-11-12T19:11:32.405082: step 4483, loss 0.0144948, acc 1
2016-11-12T19:11:32.464575: step 4484, loss 0.00694613, acc 1
2016-11-12T19:11:32.522011: step 4485, loss 0.00587669, acc 1
2016-11-12T19:11:32.579378: step 4486, loss 0.0115106, acc 1
2016-11-12T19:11:32.637638: step 4487, loss 0.00302877, acc 1
2016-11-12T19:11:32.693607: step 4488, loss 0.00189516, acc 1
2016-11-12T19:11:32.750034: step 4489, loss 0.0252023, acc 0.984375
2016-11-12T19:11:32.808379: step 4490, loss 0.0363062, acc 0.984375
2016-11-12T19:11:32.866186: step 4491, loss 0.0183707, acc 1
2016-11-12T19:11:32.922412: step 4492, loss 0.0030063, acc 1
2016-11-12T19:11:32.978603: step 4493, loss 0.0137944, acc 1
2016-11-12T19:11:33.035612: step 4494, loss 0.143577, acc 0.984375
2016-11-12T19:11:33.093180: step 4495, loss 0.0186759, acc 0.984375
2016-11-12T19:11:33.188411: step 4496, loss 0.00149657, acc 1
2016-11-12T19:11:33.244701: step 4497, loss 0.00579952, acc 1
2016-11-12T19:11:33.301259: step 4498, loss 0.00478781, acc 1
2016-11-12T19:11:33.357140: step 4499, loss 0.0267035, acc 0.984375
2016-11-12T19:11:33.415597: step 4500, loss 0.00123421, acc 1

Evaluation:
2016-11-12T19:11:33.485496: step 4500, loss 2.42956, acc 0.546

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-4500

2016-11-12T19:11:33.987235: step 4501, loss 0.0212394, acc 0.984375
2016-11-12T19:11:34.045283: step 4502, loss 0.00265057, acc 1
2016-11-12T19:11:34.101624: step 4503, loss 0.020022, acc 0.984375
2016-11-12T19:11:34.157904: step 4504, loss 0.00470549, acc 1
2016-11-12T19:11:34.213973: step 4505, loss 0.010297, acc 1
2016-11-12T19:11:34.270366: step 4506, loss 0.00412512, acc 1
2016-11-12T19:11:34.329165: step 4507, loss 0.00182259, acc 1
2016-11-12T19:11:34.385949: step 4508, loss 0.00942667, acc 1
2016-11-12T19:11:34.444567: step 4509, loss 0.0226854, acc 1
2016-11-12T19:11:34.501662: step 4510, loss 0.0186598, acc 0.984375
2016-11-12T19:11:34.559195: step 4511, loss 0.0188822, acc 0.984375
2016-11-12T19:11:34.615776: step 4512, loss 0.00152975, acc 1
2016-11-12T19:11:34.674644: step 4513, loss 0.0412244, acc 0.984375
2016-11-12T19:11:34.732303: step 4514, loss 0.00188031, acc 1
2016-11-12T19:11:34.790889: step 4515, loss 0.0355829, acc 0.984375
2016-11-12T19:11:34.848130: step 4516, loss 0.00657837, acc 1
2016-11-12T19:11:34.904078: step 4517, loss 0.00323049, acc 1
2016-11-12T19:11:34.961293: step 4518, loss 0.00661094, acc 1
2016-11-12T19:11:35.018310: step 4519, loss 0.00366314, acc 1
2016-11-12T19:11:35.075638: step 4520, loss 0.000608097, acc 1
2016-11-12T19:11:35.132444: step 4521, loss 0.00431895, acc 1
2016-11-12T19:11:35.189128: step 4522, loss 0.00154134, acc 1
2016-11-12T19:11:35.245197: step 4523, loss 0.00761336, acc 1
2016-11-12T19:11:35.302368: step 4524, loss 0.00226509, acc 1
2016-11-12T19:11:35.361367: step 4525, loss 0.00896717, acc 1
2016-11-12T19:11:35.419387: step 4526, loss 0.0148539, acc 1
2016-11-12T19:11:35.476066: step 4527, loss 0.00805538, acc 1
2016-11-12T19:11:35.532974: step 4528, loss 0.0128381, acc 1
2016-11-12T19:11:35.590696: step 4529, loss 0.00150677, acc 1
2016-11-12T19:11:35.647060: step 4530, loss 0.000741825, acc 1
2016-11-12T19:11:35.705587: step 4531, loss 0.00790152, acc 1
2016-11-12T19:11:35.767447: step 4532, loss 0.0318623, acc 0.984375
2016-11-12T19:11:35.825415: step 4533, loss 0.000556499, acc 1
2016-11-12T19:11:35.881236: step 4534, loss 0.00183902, acc 1
2016-11-12T19:11:35.936822: step 4535, loss 0.0713341, acc 0.96875
2016-11-12T19:11:35.993916: step 4536, loss 0.00285956, acc 1
2016-11-12T19:11:36.049985: step 4537, loss 0.0167457, acc 0.984375
2016-11-12T19:11:36.109254: step 4538, loss 0.00703855, acc 1
2016-11-12T19:11:36.168221: step 4539, loss 0.0139458, acc 1
2016-11-12T19:11:36.227305: step 4540, loss 0.000493692, acc 1
2016-11-12T19:11:36.285702: step 4541, loss 0.0342243, acc 0.96875
2016-11-12T19:11:36.344776: step 4542, loss 0.076163, acc 0.96875
2016-11-12T19:11:36.408183: step 4543, loss 0.00343403, acc 1
2016-11-12T19:11:36.448487: step 4544, loss 0.00648839, acc 1
2016-11-12T19:11:36.509303: step 4545, loss 0.000269079, acc 1
2016-11-12T19:11:36.569092: step 4546, loss 0.00868826, acc 1
2016-11-12T19:11:36.627215: step 4547, loss 0.000475032, acc 1
2016-11-12T19:11:36.684654: step 4548, loss 0.019154, acc 0.984375
2016-11-12T19:11:36.741487: step 4549, loss 0.0700674, acc 0.984375
2016-11-12T19:11:36.798831: step 4550, loss 0.0384917, acc 0.984375
2016-11-12T19:11:36.855642: step 4551, loss 0.00408574, acc 1
2016-11-12T19:11:36.911654: step 4552, loss 0.00289525, acc 1
2016-11-12T19:11:36.967295: step 4553, loss 0.00570952, acc 1
2016-11-12T19:11:37.024620: step 4554, loss 0.0144559, acc 1
2016-11-12T19:11:37.085058: step 4555, loss 0.00404878, acc 1
2016-11-12T19:11:37.141959: step 4556, loss 0.00198892, acc 1
2016-11-12T19:11:37.198208: step 4557, loss 0.00526966, acc 1
2016-11-12T19:11:37.257128: step 4558, loss 0.0299094, acc 0.984375
2016-11-12T19:11:37.315084: step 4559, loss 0.0054494, acc 1
2016-11-12T19:11:37.373050: step 4560, loss 0.00442377, acc 1
2016-11-12T19:11:37.429805: step 4561, loss 0.000795395, acc 1
2016-11-12T19:11:37.489134: step 4562, loss 0.0018026, acc 1
2016-11-12T19:11:37.546975: step 4563, loss 0.00538544, acc 1
2016-11-12T19:11:37.603224: step 4564, loss 0.00463099, acc 1
2016-11-12T19:11:37.661317: step 4565, loss 0.00826342, acc 1
2016-11-12T19:11:37.717992: step 4566, loss 0.00567611, acc 1
2016-11-12T19:11:37.774567: step 4567, loss 0.0105917, acc 1
2016-11-12T19:11:37.832916: step 4568, loss 0.00685947, acc 1
2016-11-12T19:11:37.889151: step 4569, loss 0.0032294, acc 1
2016-11-12T19:11:37.945565: step 4570, loss 0.00894313, acc 1
2016-11-12T19:11:38.004571: step 4571, loss 0.00889919, acc 1
2016-11-12T19:11:38.063099: step 4572, loss 0.00599533, acc 1
2016-11-12T19:11:38.121040: step 4573, loss 0.00543928, acc 1
2016-11-12T19:11:38.178912: step 4574, loss 0.0214451, acc 0.984375
2016-11-12T19:11:38.236828: step 4575, loss 0.0207783, acc 1
2016-11-12T19:11:38.294380: step 4576, loss 0.00218413, acc 1
2016-11-12T19:11:38.353724: step 4577, loss 0.0197414, acc 0.984375
2016-11-12T19:11:38.414188: step 4578, loss 0.00182061, acc 1
2016-11-12T19:11:38.473080: step 4579, loss 0.00429504, acc 1
2016-11-12T19:11:38.529918: step 4580, loss 0.0429125, acc 0.984375
2016-11-12T19:11:38.589890: step 4581, loss 0.0574716, acc 0.96875
2016-11-12T19:11:38.648150: step 4582, loss 0.00308606, acc 1
2016-11-12T19:11:38.705551: step 4583, loss 0.00284929, acc 1
2016-11-12T19:11:38.761486: step 4584, loss 0.0149208, acc 0.984375
2016-11-12T19:11:38.820997: step 4585, loss 0.00132169, acc 1
2016-11-12T19:11:38.878115: step 4586, loss 0.00386738, acc 1
2016-11-12T19:11:38.936662: step 4587, loss 0.00533107, acc 1
2016-11-12T19:11:38.995282: step 4588, loss 0.00142886, acc 1
2016-11-12T19:11:39.051755: step 4589, loss 0.00178713, acc 1
2016-11-12T19:11:39.109246: step 4590, loss 0.00934102, acc 1
2016-11-12T19:11:39.169687: step 4591, loss 0.00494133, acc 1
2016-11-12T19:11:39.230080: step 4592, loss 0.036363, acc 0.984375
2016-11-12T19:11:39.289230: step 4593, loss 0.00114627, acc 1
2016-11-12T19:11:39.344968: step 4594, loss 0.0361211, acc 0.984375
2016-11-12T19:11:39.405297: step 4595, loss 0.0136537, acc 0.984375
2016-11-12T19:11:39.461927: step 4596, loss 0.135018, acc 0.96875
2016-11-12T19:11:39.520153: step 4597, loss 0.00174014, acc 1
2016-11-12T19:11:39.575981: step 4598, loss 0.00432805, acc 1
2016-11-12T19:11:39.634211: step 4599, loss 0.00383512, acc 1
2016-11-12T19:11:39.690760: step 4600, loss 0.00163591, acc 1

Evaluation:
2016-11-12T19:11:39.760795: step 4600, loss 2.38296, acc 0.56

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-4600

2016-11-12T19:11:40.264153: step 4601, loss 0.0161416, acc 1
2016-11-12T19:11:40.321114: step 4602, loss 0.00359886, acc 1
2016-11-12T19:11:40.381011: step 4603, loss 0.0070465, acc 1
2016-11-12T19:11:40.436770: step 4604, loss 0.0194574, acc 0.984375
2016-11-12T19:11:40.493542: step 4605, loss 0.0339655, acc 0.984375
2016-11-12T19:11:40.553431: step 4606, loss 0.0132216, acc 1
2016-11-12T19:11:40.609731: step 4607, loss 0.0457236, acc 0.984375
2016-11-12T19:11:40.669431: step 4608, loss 0.0258956, acc 0.984375
2016-11-12T19:11:40.729733: step 4609, loss 0.000639508, acc 1
2016-11-12T19:11:40.784766: step 4610, loss 0.0151688, acc 0.984375
2016-11-12T19:11:40.843717: step 4611, loss 0.00182266, acc 1
2016-11-12T19:11:40.901568: step 4612, loss 0.0119588, acc 1
2016-11-12T19:11:40.958265: step 4613, loss 0.00648022, acc 1
2016-11-12T19:11:41.016408: step 4614, loss 0.00263783, acc 1
2016-11-12T19:11:41.054946: step 4615, loss 0.000623788, acc 1
2016-11-12T19:11:41.113725: step 4616, loss 0.0010068, acc 1
2016-11-12T19:11:41.172317: step 4617, loss 0.000960936, acc 1
2016-11-12T19:11:41.231202: step 4618, loss 0.00127627, acc 1
2016-11-12T19:11:41.289036: step 4619, loss 0.000393864, acc 1
2016-11-12T19:11:41.345454: step 4620, loss 0.00122351, acc 1
2016-11-12T19:11:41.405531: step 4621, loss 0.0045315, acc 1
2016-11-12T19:11:41.461594: step 4622, loss 0.00303606, acc 1
2016-11-12T19:11:41.521332: step 4623, loss 0.0245007, acc 0.984375
2016-11-12T19:11:41.582033: step 4624, loss 0.0469303, acc 0.984375
2016-11-12T19:11:41.642381: step 4625, loss 0.0502489, acc 0.984375
2016-11-12T19:11:41.700168: step 4626, loss 0.00834423, acc 1
2016-11-12T19:11:41.756841: step 4627, loss 0.02158, acc 0.984375
2016-11-12T19:11:41.815995: step 4628, loss 0.00258513, acc 1
2016-11-12T19:11:41.872984: step 4629, loss 0.0186963, acc 0.984375
2016-11-12T19:11:41.929644: step 4630, loss 0.00219979, acc 1
2016-11-12T19:11:41.987820: step 4631, loss 0.00220008, acc 1
2016-11-12T19:11:42.045129: step 4632, loss 0.00755081, acc 1
2016-11-12T19:11:42.102055: step 4633, loss 0.0111873, acc 1
2016-11-12T19:11:42.158989: step 4634, loss 0.0137258, acc 1
2016-11-12T19:11:42.217882: step 4635, loss 0.00597145, acc 1
2016-11-12T19:11:42.275387: step 4636, loss 0.0452237, acc 0.984375
2016-11-12T19:11:42.332965: step 4637, loss 0.00221504, acc 1
2016-11-12T19:11:42.392157: step 4638, loss 0.00376792, acc 1
2016-11-12T19:11:42.450958: step 4639, loss 0.0302193, acc 0.96875
2016-11-12T19:11:42.509546: step 4640, loss 0.00325415, acc 1
2016-11-12T19:11:42.569159: step 4641, loss 0.00116861, acc 1
2016-11-12T19:11:42.630636: step 4642, loss 0.0239144, acc 0.984375
2016-11-12T19:11:42.687747: step 4643, loss 0.0036265, acc 1
2016-11-12T19:11:42.745556: step 4644, loss 0.00212163, acc 1
2016-11-12T19:11:42.802740: step 4645, loss 0.0473329, acc 0.96875
2016-11-12T19:11:42.861374: step 4646, loss 0.00247808, acc 1
2016-11-12T19:11:42.921733: step 4647, loss 0.00299583, acc 1
2016-11-12T19:11:42.980045: step 4648, loss 0.00308231, acc 1
2016-11-12T19:11:43.036680: step 4649, loss 0.0486173, acc 0.96875
2016-11-12T19:11:43.093149: step 4650, loss 0.0244837, acc 0.984375
2016-11-12T19:11:43.151052: step 4651, loss 0.00229384, acc 1
2016-11-12T19:11:43.208634: step 4652, loss 0.00247729, acc 1
2016-11-12T19:11:43.266067: step 4653, loss 0.00253293, acc 1
2016-11-12T19:11:43.322079: step 4654, loss 0.0159901, acc 0.984375
2016-11-12T19:11:43.380025: step 4655, loss 0.00439462, acc 1
2016-11-12T19:11:43.436790: step 4656, loss 0.00308308, acc 1
2016-11-12T19:11:43.493183: step 4657, loss 0.0228257, acc 1
2016-11-12T19:11:43.552299: step 4658, loss 0.00171085, acc 1
2016-11-12T19:11:43.608852: step 4659, loss 0.00833946, acc 1
2016-11-12T19:11:43.667520: step 4660, loss 0.0885481, acc 0.984375
2016-11-12T19:11:43.725449: step 4661, loss 0.0405408, acc 0.984375
2016-11-12T19:11:43.785279: step 4662, loss 0.0059627, acc 1
2016-11-12T19:11:43.842253: step 4663, loss 0.00502555, acc 1
2016-11-12T19:11:43.899259: step 4664, loss 0.0908887, acc 0.984375
2016-11-12T19:11:43.958055: step 4665, loss 0.00638963, acc 1
2016-11-12T19:11:44.013995: step 4666, loss 0.00224139, acc 1
2016-11-12T19:11:44.072835: step 4667, loss 0.0225991, acc 0.984375
2016-11-12T19:11:44.129978: step 4668, loss 0.00952683, acc 1
2016-11-12T19:11:44.189157: step 4669, loss 0.0701763, acc 0.96875
2016-11-12T19:11:44.246672: step 4670, loss 0.00236709, acc 1
2016-11-12T19:11:44.305246: step 4671, loss 0.00078407, acc 1
2016-11-12T19:11:44.362012: step 4672, loss 0.0116747, acc 1
2016-11-12T19:11:44.421225: step 4673, loss 0.00430283, acc 1
2016-11-12T19:11:44.478989: step 4674, loss 0.00515612, acc 1
2016-11-12T19:11:44.537345: step 4675, loss 0.0039242, acc 1
2016-11-12T19:11:44.597701: step 4676, loss 0.0817929, acc 0.984375
2016-11-12T19:11:44.656699: step 4677, loss 0.0281122, acc 0.984375
2016-11-12T19:11:44.713278: step 4678, loss 0.00753732, acc 1
2016-11-12T19:11:44.770729: step 4679, loss 0.000722245, acc 1
2016-11-12T19:11:44.826810: step 4680, loss 0.0291812, acc 0.984375
2016-11-12T19:11:44.885612: step 4681, loss 0.00694071, acc 1
2016-11-12T19:11:44.941346: step 4682, loss 0.00398726, acc 1
2016-11-12T19:11:44.999864: step 4683, loss 0.0164816, acc 0.984375
2016-11-12T19:11:45.056428: step 4684, loss 0.0489701, acc 0.96875
2016-11-12T19:11:45.114334: step 4685, loss 0.00545985, acc 1
2016-11-12T19:11:45.152365: step 4686, loss 0.00267509, acc 1
2016-11-12T19:11:45.211854: step 4687, loss 0.0152177, acc 0.984375
2016-11-12T19:11:45.270550: step 4688, loss 0.0620047, acc 0.984375
2016-11-12T19:11:45.329312: step 4689, loss 0.00697227, acc 1
2016-11-12T19:11:45.388367: step 4690, loss 0.00549849, acc 1
2016-11-12T19:11:45.445912: step 4691, loss 0.00742483, acc 1
2016-11-12T19:11:45.502590: step 4692, loss 0.0048534, acc 1
2016-11-12T19:11:45.560521: step 4693, loss 0.0108722, acc 1
2016-11-12T19:11:45.616866: step 4694, loss 0.00422614, acc 1
2016-11-12T19:11:45.674069: step 4695, loss 0.00365215, acc 1
2016-11-12T19:11:45.729903: step 4696, loss 0.0398501, acc 0.984375
2016-11-12T19:11:45.787321: step 4697, loss 0.0170205, acc 1
2016-11-12T19:11:45.844977: step 4698, loss 0.00214341, acc 1
2016-11-12T19:11:45.902428: step 4699, loss 0.0396795, acc 0.96875
2016-11-12T19:11:45.961557: step 4700, loss 0.012669, acc 1

Evaluation:
2016-11-12T19:11:46.033117: step 4700, loss 2.45838, acc 0.552

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-4700

2016-11-12T19:11:46.533517: step 4701, loss 0.0291287, acc 0.96875
2016-11-12T19:11:46.593960: step 4702, loss 0.00220237, acc 1
2016-11-12T19:11:46.654416: step 4703, loss 0.0361311, acc 0.984375
2016-11-12T19:11:46.712607: step 4704, loss 0.000271379, acc 1
2016-11-12T19:11:46.769146: step 4705, loss 0.00755749, acc 1
2016-11-12T19:11:46.826668: step 4706, loss 0.0537242, acc 0.984375
2016-11-12T19:11:46.884886: step 4707, loss 0.00212016, acc 1
2016-11-12T19:11:46.944180: step 4708, loss 0.00218285, acc 1
2016-11-12T19:11:47.000333: step 4709, loss 0.0155954, acc 0.984375
2016-11-12T19:11:47.057241: step 4710, loss 0.0112137, acc 1
2016-11-12T19:11:47.113767: step 4711, loss 0.00391442, acc 1
2016-11-12T19:11:47.171498: step 4712, loss 0.00477143, acc 1
2016-11-12T19:11:47.229054: step 4713, loss 0.0019229, acc 1
2016-11-12T19:11:47.285415: step 4714, loss 0.00412428, acc 1
2016-11-12T19:11:47.342321: step 4715, loss 0.0041853, acc 1
2016-11-12T19:11:47.401375: step 4716, loss 0.00804582, acc 1
2016-11-12T19:11:47.457697: step 4717, loss 0.00241543, acc 1
2016-11-12T19:11:47.517556: step 4718, loss 0.0152412, acc 0.984375
2016-11-12T19:11:47.574189: step 4719, loss 0.000716722, acc 1
2016-11-12T19:11:47.630049: step 4720, loss 0.0528158, acc 0.984375
2016-11-12T19:11:47.688771: step 4721, loss 0.0227402, acc 0.984375
2016-11-12T19:11:47.746282: step 4722, loss 0.040735, acc 0.984375
2016-11-12T19:11:47.804337: step 4723, loss 0.00408702, acc 1
2016-11-12T19:11:47.861457: step 4724, loss 0.0132706, acc 1
2016-11-12T19:11:47.919049: step 4725, loss 0.00583019, acc 1
2016-11-12T19:11:47.976089: step 4726, loss 0.00267859, acc 1
2016-11-12T19:11:48.032933: step 4727, loss 0.0249834, acc 0.984375
2016-11-12T19:11:48.091046: step 4728, loss 0.0022685, acc 1
2016-11-12T19:11:48.146941: step 4729, loss 0.00255021, acc 1
2016-11-12T19:11:48.205472: step 4730, loss 0.0331718, acc 0.96875
2016-11-12T19:11:48.265216: step 4731, loss 0.0102248, acc 1
2016-11-12T19:11:48.321801: step 4732, loss 0.0186977, acc 1
2016-11-12T19:11:48.378542: step 4733, loss 0.00781475, acc 1
2016-11-12T19:11:48.434949: step 4734, loss 0.0231966, acc 0.984375
2016-11-12T19:11:48.492953: step 4735, loss 0.0918821, acc 0.96875
2016-11-12T19:11:48.550883: step 4736, loss 0.00198143, acc 1
2016-11-12T19:11:48.609267: step 4737, loss 0.0015704, acc 1
2016-11-12T19:11:48.666323: step 4738, loss 0.0015639, acc 1
2016-11-12T19:11:48.725297: step 4739, loss 0.132785, acc 0.984375
2016-11-12T19:11:48.783162: step 4740, loss 0.000239969, acc 1
2016-11-12T19:11:48.839702: step 4741, loss 0.00907022, acc 1
2016-11-12T19:11:48.896701: step 4742, loss 0.00391096, acc 1
2016-11-12T19:11:48.953484: step 4743, loss 0.0043192, acc 1
2016-11-12T19:11:49.011808: step 4744, loss 0.0310056, acc 0.984375
2016-11-12T19:11:49.068851: step 4745, loss 0.00129103, acc 1
2016-11-12T19:11:49.125434: step 4746, loss 0.00536445, acc 1
2016-11-12T19:11:49.183060: step 4747, loss 0.00199939, acc 1
2016-11-12T19:11:49.241705: step 4748, loss 0.000794148, acc 1
2016-11-12T19:11:49.300079: step 4749, loss 0.00759901, acc 1
2016-11-12T19:11:49.357447: step 4750, loss 0.0066559, acc 1
2016-11-12T19:11:49.414780: step 4751, loss 0.00185243, acc 1
2016-11-12T19:11:49.470280: step 4752, loss 0.033983, acc 0.984375
2016-11-12T19:11:49.527345: step 4753, loss 0.00956574, acc 1
2016-11-12T19:11:49.583420: step 4754, loss 0.00851124, acc 1
2016-11-12T19:11:49.639619: step 4755, loss 0.000802335, acc 1
2016-11-12T19:11:49.696291: step 4756, loss 0.0721884, acc 0.984375
2016-11-12T19:11:49.734733: step 4757, loss 0.0104396, acc 1
2016-11-12T19:11:49.794083: step 4758, loss 0.00753038, acc 1
2016-11-12T19:11:49.852976: step 4759, loss 0.0514189, acc 0.984375
2016-11-12T19:11:49.910602: step 4760, loss 0.00212655, acc 1
2016-11-12T19:11:49.968486: step 4761, loss 0.00306409, acc 1
2016-11-12T19:11:50.025409: step 4762, loss 0.0416264, acc 0.984375
2016-11-12T19:11:50.084789: step 4763, loss 0.0137484, acc 0.984375
2016-11-12T19:11:50.142551: step 4764, loss 0.000784354, acc 1
2016-11-12T19:11:50.198027: step 4765, loss 0.00138425, acc 1
2016-11-12T19:11:50.256308: step 4766, loss 0.0170657, acc 1
2016-11-12T19:11:50.313280: step 4767, loss 0.00178477, acc 1
2016-11-12T19:11:50.371909: step 4768, loss 0.00254589, acc 1
2016-11-12T19:11:50.431540: step 4769, loss 0.00946984, acc 1
2016-11-12T19:11:50.487173: step 4770, loss 0.0153, acc 1
2016-11-12T19:11:50.543658: step 4771, loss 0.0375886, acc 0.984375
2016-11-12T19:11:50.600952: step 4772, loss 0.00217298, acc 1
2016-11-12T19:11:50.657374: step 4773, loss 0.0023808, acc 1
2016-11-12T19:11:50.715650: step 4774, loss 0.00352574, acc 1
2016-11-12T19:11:50.773672: step 4775, loss 0.0440564, acc 0.984375
2016-11-12T19:11:50.833673: step 4776, loss 0.0121967, acc 0.984375
2016-11-12T19:11:50.891701: step 4777, loss 0.0384265, acc 0.984375
2016-11-12T19:11:50.948685: step 4778, loss 0.00886478, acc 1
2016-11-12T19:11:51.006576: step 4779, loss 0.0154614, acc 0.984375
2016-11-12T19:11:51.065733: step 4780, loss 0.0577748, acc 0.984375
2016-11-12T19:11:51.123366: step 4781, loss 0.0195831, acc 1
2016-11-12T19:11:51.179253: step 4782, loss 0.0264046, acc 0.96875
2016-11-12T19:11:51.237194: step 4783, loss 0.219641, acc 0.984375
2016-11-12T19:11:51.297350: step 4784, loss 0.00321004, acc 1
2016-11-12T19:11:51.357178: step 4785, loss 0.00887611, acc 1
2016-11-12T19:11:51.414409: step 4786, loss 0.00339438, acc 1
2016-11-12T19:11:51.473413: step 4787, loss 0.00530071, acc 1
2016-11-12T19:11:51.531906: step 4788, loss 0.0161748, acc 0.984375
2016-11-12T19:11:51.589675: step 4789, loss 0.0373408, acc 0.984375
2016-11-12T19:11:51.650141: step 4790, loss 0.126475, acc 0.984375
2016-11-12T19:11:51.711608: step 4791, loss 0.0797805, acc 0.984375
2016-11-12T19:11:51.769457: step 4792, loss 0.000702583, acc 1
2016-11-12T19:11:51.825864: step 4793, loss 0.00178431, acc 1
2016-11-12T19:11:51.880579: step 4794, loss 0.00304084, acc 1
2016-11-12T19:11:51.936809: step 4795, loss 0.00606592, acc 1
2016-11-12T19:11:51.993077: step 4796, loss 0.000953711, acc 1
2016-11-12T19:11:52.049476: step 4797, loss 0.030729, acc 0.984375
2016-11-12T19:11:52.106231: step 4798, loss 0.0474464, acc 0.984375
2016-11-12T19:11:52.163487: step 4799, loss 0.00663349, acc 1
2016-11-12T19:11:52.221402: step 4800, loss 0.0110225, acc 1

Evaluation:
2016-11-12T19:11:52.293708: step 4800, loss 2.55317, acc 0.552

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-4800

2016-11-12T19:11:52.795318: step 4801, loss 0.00539346, acc 1
2016-11-12T19:11:52.853956: step 4802, loss 0.0412536, acc 0.96875
2016-11-12T19:11:52.912206: step 4803, loss 0.0323056, acc 0.984375
2016-11-12T19:11:52.969150: step 4804, loss 0.0166356, acc 0.984375
2016-11-12T19:11:53.027772: step 4805, loss 0.0156097, acc 0.984375
2016-11-12T19:11:53.086142: step 4806, loss 0.00685394, acc 1
2016-11-12T19:11:53.143738: step 4807, loss 0.00228145, acc 1
2016-11-12T19:11:53.201393: step 4808, loss 0.00108037, acc 1
2016-11-12T19:11:53.257537: step 4809, loss 0.000997706, acc 1
2016-11-12T19:11:53.313664: step 4810, loss 0.00944485, acc 1
2016-11-12T19:11:53.372283: step 4811, loss 0.0193698, acc 1
2016-11-12T19:11:53.428945: step 4812, loss 0.00235872, acc 1
2016-11-12T19:11:53.486115: step 4813, loss 0.00386634, acc 1
2016-11-12T19:11:53.544787: step 4814, loss 0.0182912, acc 0.984375
2016-11-12T19:11:53.605634: step 4815, loss 0.0241312, acc 0.984375
2016-11-12T19:11:53.662835: step 4816, loss 0.00389039, acc 1
2016-11-12T19:11:53.720226: step 4817, loss 0.00666674, acc 1
2016-11-12T19:11:53.776999: step 4818, loss 0.00169838, acc 1
2016-11-12T19:11:53.835390: step 4819, loss 0.0596903, acc 0.984375
2016-11-12T19:11:53.893445: step 4820, loss 0.0140834, acc 1
2016-11-12T19:11:53.952447: step 4821, loss 0.00367114, acc 1
2016-11-12T19:11:54.009075: step 4822, loss 0.0118115, acc 1
2016-11-12T19:11:54.067225: step 4823, loss 0.00823165, acc 1
2016-11-12T19:11:54.126994: step 4824, loss 0.00123174, acc 1
2016-11-12T19:11:54.183106: step 4825, loss 0.00516759, acc 1
2016-11-12T19:11:54.240724: step 4826, loss 0.000745349, acc 1
2016-11-12T19:11:54.298097: step 4827, loss 0.00057119, acc 1
2016-11-12T19:11:54.336720: step 4828, loss 0.00326274, acc 1
2016-11-12T19:11:54.394573: step 4829, loss 0.00451606, acc 1
2016-11-12T19:11:54.452222: step 4830, loss 0.00202695, acc 1
2016-11-12T19:11:54.511267: step 4831, loss 0.00390293, acc 1
2016-11-12T19:11:54.568492: step 4832, loss 0.0257808, acc 0.96875
2016-11-12T19:11:54.627868: step 4833, loss 0.00233264, acc 1
2016-11-12T19:11:54.684997: step 4834, loss 0.0161002, acc 0.984375
2016-11-12T19:11:54.741101: step 4835, loss 0.00296003, acc 1
2016-11-12T19:11:54.797218: step 4836, loss 0.00377329, acc 1
2016-11-12T19:11:54.853936: step 4837, loss 0.00969248, acc 1
2016-11-12T19:11:54.911752: step 4838, loss 0.00093663, acc 1
2016-11-12T19:11:54.969559: step 4839, loss 0.00356685, acc 1
2016-11-12T19:11:55.028373: step 4840, loss 0.00171481, acc 1
2016-11-12T19:11:55.085022: step 4841, loss 0.0596824, acc 0.984375
2016-11-12T19:11:55.143047: step 4842, loss 0.0103464, acc 1
2016-11-12T19:11:55.201188: step 4843, loss 0.00140301, acc 1
2016-11-12T19:11:55.261054: step 4844, loss 0.00239156, acc 1
2016-11-12T19:11:55.318355: step 4845, loss 0.00869334, acc 1
2016-11-12T19:11:55.375459: step 4846, loss 0.0135145, acc 0.984375
2016-11-12T19:11:55.434432: step 4847, loss 0.00525557, acc 1
2016-11-12T19:11:55.493195: step 4848, loss 0.0266969, acc 0.984375
2016-11-12T19:11:55.553935: step 4849, loss 0.0179978, acc 1
2016-11-12T19:11:55.613826: step 4850, loss 0.00758696, acc 1
2016-11-12T19:11:55.672216: step 4851, loss 0.00882171, acc 1
2016-11-12T19:11:55.729480: step 4852, loss 0.0568354, acc 0.984375
2016-11-12T19:11:55.786297: step 4853, loss 0.00257763, acc 1
2016-11-12T19:11:55.843184: step 4854, loss 0.0106314, acc 1
2016-11-12T19:11:55.905700: step 4855, loss 0.0167867, acc 0.984375
2016-11-12T19:11:55.963013: step 4856, loss 0.00332321, acc 1
2016-11-12T19:11:56.019973: step 4857, loss 0.0124224, acc 1
2016-11-12T19:11:56.079156: step 4858, loss 0.00314952, acc 1
2016-11-12T19:11:56.137172: step 4859, loss 0.000692297, acc 1
2016-11-12T19:11:56.195461: step 4860, loss 0.00182699, acc 1
2016-11-12T19:11:56.254234: step 4861, loss 0.00359069, acc 1
2016-11-12T19:11:56.312240: step 4862, loss 0.00180775, acc 1
2016-11-12T19:11:56.369585: step 4863, loss 0.00830246, acc 1
2016-11-12T19:11:56.427017: step 4864, loss 0.0229238, acc 0.984375
2016-11-12T19:11:56.484378: step 4865, loss 0.000593286, acc 1
2016-11-12T19:11:56.541376: step 4866, loss 0.00201584, acc 1
2016-11-12T19:11:56.597363: step 4867, loss 0.0030956, acc 1
2016-11-12T19:11:56.656987: step 4868, loss 0.213194, acc 0.953125
2016-11-12T19:11:56.717962: step 4869, loss 0.00222009, acc 1
2016-11-12T19:11:56.775490: step 4870, loss 0.000943877, acc 1
2016-11-12T19:11:56.831471: step 4871, loss 0.00197689, acc 1
2016-11-12T19:11:56.889185: step 4872, loss 0.00612156, acc 1
2016-11-12T19:11:56.948947: step 4873, loss 0.011127, acc 1
2016-11-12T19:11:57.005235: step 4874, loss 0.0345948, acc 0.984375
2016-11-12T19:11:57.063336: step 4875, loss 0.0302575, acc 0.984375
2016-11-12T19:11:57.119502: step 4876, loss 0.008687, acc 1
2016-11-12T19:11:57.176596: step 4877, loss 0.00374982, acc 1
2016-11-12T19:11:57.233385: step 4878, loss 0.0033239, acc 1
2016-11-12T19:11:57.289701: step 4879, loss 0.00072621, acc 1
2016-11-12T19:11:57.348644: step 4880, loss 0.00344889, acc 1
2016-11-12T19:11:57.404213: step 4881, loss 0.00202775, acc 1
2016-11-12T19:11:57.461944: step 4882, loss 0.00362398, acc 1
2016-11-12T19:11:57.518761: step 4883, loss 0.00297926, acc 1
2016-11-12T19:11:57.576447: step 4884, loss 0.00784568, acc 1
2016-11-12T19:11:57.636831: step 4885, loss 0.00103506, acc 1
2016-11-12T19:11:57.693796: step 4886, loss 0.00162918, acc 1
2016-11-12T19:11:57.750559: step 4887, loss 0.0374647, acc 0.984375
2016-11-12T19:11:57.809830: step 4888, loss 0.000871603, acc 1
2016-11-12T19:11:57.867366: step 4889, loss 0.00480878, acc 1
2016-11-12T19:11:57.925169: step 4890, loss 0.00986399, acc 1
2016-11-12T19:11:57.981442: step 4891, loss 0.00105696, acc 1
2016-11-12T19:11:58.037657: step 4892, loss 0.00171462, acc 1
2016-11-12T19:11:58.093734: step 4893, loss 0.0113939, acc 1
2016-11-12T19:11:58.151169: step 4894, loss 0.0314287, acc 0.984375
2016-11-12T19:11:58.208447: step 4895, loss 0.0176157, acc 0.984375
2016-11-12T19:11:58.266907: step 4896, loss 0.0132843, acc 1
2016-11-12T19:11:58.324789: step 4897, loss 0.00527389, acc 1
2016-11-12T19:11:58.381069: step 4898, loss 0.00551589, acc 1
2016-11-12T19:11:58.418762: step 4899, loss 0.00501697, acc 1
2016-11-12T19:11:58.477043: step 4900, loss 0.0115114, acc 1

Evaluation:
2016-11-12T19:11:58.548692: step 4900, loss 2.5296, acc 0.554

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-4900

2016-11-12T19:11:59.049676: step 4901, loss 0.00250022, acc 1
2016-11-12T19:11:59.109304: step 4902, loss 0.0267591, acc 0.984375
2016-11-12T19:11:59.168778: step 4903, loss 0.000961734, acc 1
2016-11-12T19:11:59.226514: step 4904, loss 0.0229217, acc 0.984375
2016-11-12T19:11:59.284453: step 4905, loss 0.0140468, acc 1
2016-11-12T19:11:59.341308: step 4906, loss 0.00460191, acc 1
2016-11-12T19:11:59.398440: step 4907, loss 0.0041487, acc 1
2016-11-12T19:11:59.455543: step 4908, loss 0.000939779, acc 1
2016-11-12T19:11:59.513019: step 4909, loss 0.0169222, acc 0.984375
2016-11-12T19:11:59.571738: step 4910, loss 0.0113134, acc 1
2016-11-12T19:11:59.629482: step 4911, loss 0.00125814, acc 1
2016-11-12T19:11:59.686213: step 4912, loss 0.0209276, acc 1
2016-11-12T19:11:59.744106: step 4913, loss 0.00618459, acc 1
2016-11-12T19:11:59.800109: step 4914, loss 0.00499973, acc 1
2016-11-12T19:11:59.856489: step 4915, loss 0.0142093, acc 0.984375
2016-11-12T19:11:59.912872: step 4916, loss 0.0291724, acc 0.984375
2016-11-12T19:11:59.969883: step 4917, loss 0.00464395, acc 1
2016-11-12T19:12:00.026281: step 4918, loss 0.00061524, acc 1
2016-11-12T19:12:00.082306: step 4919, loss 0.0117653, acc 0.984375
2016-11-12T19:12:00.141134: step 4920, loss 0.0026349, acc 1
2016-11-12T19:12:00.197718: step 4921, loss 0.00504706, acc 1
2016-11-12T19:12:00.254425: step 4922, loss 0.00274148, acc 1
2016-11-12T19:12:00.311932: step 4923, loss 0.0022655, acc 1
2016-11-12T19:12:00.368018: step 4924, loss 0.0217679, acc 0.984375
2016-11-12T19:12:00.426036: step 4925, loss 0.000877421, acc 1
2016-11-12T19:12:00.484591: step 4926, loss 0.00830172, acc 1
2016-11-12T19:12:00.540990: step 4927, loss 0.00259365, acc 1
2016-11-12T19:12:00.597344: step 4928, loss 0.052048, acc 0.984375
2016-11-12T19:12:00.657706: step 4929, loss 0.0324716, acc 0.96875
2016-11-12T19:12:00.714911: step 4930, loss 0.00253234, acc 1
2016-11-12T19:12:00.773329: step 4931, loss 0.00371136, acc 1
2016-11-12T19:12:00.829519: step 4932, loss 0.00084207, acc 1
2016-11-12T19:12:00.885909: step 4933, loss 0.00216483, acc 1
2016-11-12T19:12:00.942446: step 4934, loss 0.0144335, acc 1
2016-11-12T19:12:00.999171: step 4935, loss 0.0516835, acc 0.96875
2016-11-12T19:12:01.056847: step 4936, loss 0.00393025, acc 1
2016-11-12T19:12:01.114723: step 4937, loss 0.000538301, acc 1
2016-11-12T19:12:01.170869: step 4938, loss 0.00499039, acc 1
2016-11-12T19:12:01.227589: step 4939, loss 0.00483609, acc 1
2016-11-12T19:12:01.285897: step 4940, loss 0.0747514, acc 0.984375
2016-11-12T19:12:01.342426: step 4941, loss 0.0520734, acc 0.984375
2016-11-12T19:12:01.399213: step 4942, loss 0.0105593, acc 1
2016-11-12T19:12:01.458738: step 4943, loss 0.0413819, acc 0.984375
2016-11-12T19:12:01.516299: step 4944, loss 0.00727336, acc 1
2016-11-12T19:12:01.573489: step 4945, loss 0.00562185, acc 1
2016-11-12T19:12:01.630091: step 4946, loss 0.0104822, acc 1
2016-11-12T19:12:01.686032: step 4947, loss 0.0220897, acc 0.984375
2016-11-12T19:12:01.745379: step 4948, loss 0.00766942, acc 1
2016-11-12T19:12:01.804974: step 4949, loss 0.00147727, acc 1
2016-11-12T19:12:01.860881: step 4950, loss 0.00617961, acc 1
2016-11-12T19:12:01.919626: step 4951, loss 0.0161955, acc 1
2016-11-12T19:12:01.977912: step 4952, loss 0.0058131, acc 1
2016-11-12T19:12:02.036178: step 4953, loss 0.00390583, acc 1
2016-11-12T19:12:02.092369: step 4954, loss 0.0271395, acc 1
2016-11-12T19:12:02.149541: step 4955, loss 0.0551811, acc 0.96875
2016-11-12T19:12:02.207740: step 4956, loss 0.00788511, acc 1
2016-11-12T19:12:02.266354: step 4957, loss 0.0687659, acc 0.984375
2016-11-12T19:12:02.326433: step 4958, loss 0.0693121, acc 0.984375
2016-11-12T19:12:02.385151: step 4959, loss 0.0184447, acc 0.984375
2016-11-12T19:12:02.443211: step 4960, loss 0.0228739, acc 0.984375
2016-11-12T19:12:02.500661: step 4961, loss 0.00168671, acc 1
2016-11-12T19:12:02.558015: step 4962, loss 0.00293869, acc 1
2016-11-12T19:12:02.616707: step 4963, loss 0.00686572, acc 1
2016-11-12T19:12:02.672654: step 4964, loss 0.0214231, acc 1
2016-11-12T19:12:02.729420: step 4965, loss 0.00197664, acc 1
2016-11-12T19:12:02.785838: step 4966, loss 0.002547, acc 1
2016-11-12T19:12:02.842486: step 4967, loss 0.000358966, acc 1
2016-11-12T19:12:02.898767: step 4968, loss 0.00571232, acc 1
2016-11-12T19:12:02.957417: step 4969, loss 0.00123143, acc 1
2016-11-12T19:12:02.993848: step 4970, loss 0.00067614, acc 1
2016-11-12T19:12:03.052459: step 4971, loss 0.00415517, acc 1
2016-11-12T19:12:03.111165: step 4972, loss 0.0194806, acc 0.984375
2016-11-12T19:12:03.168987: step 4973, loss 0.013917, acc 0.984375
2016-11-12T19:12:03.225566: step 4974, loss 0.00374954, acc 1
2016-11-12T19:12:03.281735: step 4975, loss 0.00318333, acc 1
2016-11-12T19:12:03.341204: step 4976, loss 0.00149477, acc 1
2016-11-12T19:12:03.399619: step 4977, loss 0.00380024, acc 1
2016-11-12T19:12:03.457481: step 4978, loss 0.0164199, acc 0.984375
2016-11-12T19:12:03.516428: step 4979, loss 0.00218276, acc 1
2016-11-12T19:12:03.576154: step 4980, loss 0.0307575, acc 0.984375
2016-11-12T19:12:03.633206: step 4981, loss 0.036881, acc 0.984375
2016-11-12T19:12:03.691808: step 4982, loss 0.00733061, acc 1
2016-11-12T19:12:03.749177: step 4983, loss 0.000670225, acc 1
2016-11-12T19:12:03.805452: step 4984, loss 0.00532746, acc 1
2016-11-12T19:12:03.862844: step 4985, loss 0.016345, acc 0.984375
2016-11-12T19:12:03.921064: step 4986, loss 0.0293437, acc 0.984375
2016-11-12T19:12:03.977927: step 4987, loss 0.00112997, acc 1
2016-11-12T19:12:04.037235: step 4988, loss 0.00352469, acc 1
2016-11-12T19:12:04.094279: step 4989, loss 0.00251636, acc 1
2016-11-12T19:12:04.150282: step 4990, loss 0.0138418, acc 1
2016-11-12T19:12:04.209180: step 4991, loss 0.0125019, acc 1
2016-11-12T19:12:04.270870: step 4992, loss 0.0168962, acc 0.984375
2016-11-12T19:12:04.329194: step 4993, loss 0.000797904, acc 1
2016-11-12T19:12:04.384972: step 4994, loss 0.0286988, acc 0.984375
2016-11-12T19:12:04.443313: step 4995, loss 0.00578361, acc 1
2016-11-12T19:12:04.501312: step 4996, loss 0.00251851, acc 1
2016-11-12T19:12:04.559968: step 4997, loss 0.0271862, acc 0.96875
2016-11-12T19:12:04.616590: step 4998, loss 0.00127583, acc 1
2016-11-12T19:12:04.673388: step 4999, loss 0.0353122, acc 0.984375
2016-11-12T19:12:04.731418: step 5000, loss 0.00347291, acc 1

Evaluation:
2016-11-12T19:12:04.801972: step 5000, loss 2.54248, acc 0.574

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-5000

2016-11-12T19:12:05.298313: step 5001, loss 0.0074216, acc 1
2016-11-12T19:12:05.358567: step 5002, loss 0.000534524, acc 1
2016-11-12T19:12:05.416165: step 5003, loss 0.00064303, acc 1
2016-11-12T19:12:05.473329: step 5004, loss 0.0166901, acc 0.984375
2016-11-12T19:12:05.531638: step 5005, loss 0.013349, acc 1
2016-11-12T19:12:05.588763: step 5006, loss 0.00956371, acc 1
2016-11-12T19:12:05.645742: step 5007, loss 0.00751107, acc 1
2016-11-12T19:12:05.702616: step 5008, loss 0.0155977, acc 1
2016-11-12T19:12:05.759370: step 5009, loss 0.00113081, acc 1
2016-11-12T19:12:05.815053: step 5010, loss 0.0493306, acc 0.984375
2016-11-12T19:12:05.872573: step 5011, loss 0.00375282, acc 1
2016-11-12T19:12:05.928988: step 5012, loss 0.0049665, acc 1
2016-11-12T19:12:05.985745: step 5013, loss 0.000733403, acc 1
2016-11-12T19:12:06.044335: step 5014, loss 0.00453151, acc 1
2016-11-12T19:12:06.100669: step 5015, loss 0.00357969, acc 1
2016-11-12T19:12:06.156920: step 5016, loss 0.00531412, acc 1
2016-11-12T19:12:06.213936: step 5017, loss 0.00316558, acc 1
2016-11-12T19:12:06.271306: step 5018, loss 0.00619908, acc 1
2016-11-12T19:12:06.331342: step 5019, loss 0.000886676, acc 1
2016-11-12T19:12:06.389026: step 5020, loss 0.0135695, acc 1
2016-11-12T19:12:06.446187: step 5021, loss 0.00435048, acc 1
2016-11-12T19:12:06.505180: step 5022, loss 0.0900522, acc 0.984375
2016-11-12T19:12:06.562768: step 5023, loss 0.0280472, acc 0.984375
2016-11-12T19:12:06.621171: step 5024, loss 0.000342997, acc 1
2016-11-12T19:12:06.677016: step 5025, loss 0.000318208, acc 1
2016-11-12T19:12:06.732768: step 5026, loss 0.00172463, acc 1
2016-11-12T19:12:06.791608: step 5027, loss 0.0387382, acc 0.984375
2016-11-12T19:12:06.851386: step 5028, loss 0.00451373, acc 1
2016-11-12T19:12:06.908906: step 5029, loss 0.00177723, acc 1
2016-11-12T19:12:06.966461: step 5030, loss 0.011087, acc 1
2016-11-12T19:12:07.023251: step 5031, loss 0.0201508, acc 0.984375
2016-11-12T19:12:07.079763: step 5032, loss 0.0605796, acc 0.96875
2016-11-12T19:12:07.137801: step 5033, loss 0.0271353, acc 0.984375
2016-11-12T19:12:07.197044: step 5034, loss 0.0203393, acc 0.984375
2016-11-12T19:12:07.255448: step 5035, loss 0.00670867, acc 1
2016-11-12T19:12:07.311927: step 5036, loss 0.00237637, acc 1
2016-11-12T19:12:07.370607: step 5037, loss 0.000709299, acc 1
2016-11-12T19:12:07.426961: step 5038, loss 0.0645857, acc 0.984375
2016-11-12T19:12:07.485852: step 5039, loss 0.000334675, acc 1
2016-11-12T19:12:07.541175: step 5040, loss 0.00266643, acc 1
2016-11-12T19:12:07.580275: step 5041, loss 0.00285777, acc 1
2016-11-12T19:12:07.641048: step 5042, loss 0.00254281, acc 1
2016-11-12T19:12:07.697802: step 5043, loss 0.0109148, acc 1
2016-11-12T19:12:07.757297: step 5044, loss 0.00332316, acc 1
2016-11-12T19:12:07.814388: step 5045, loss 0.0149545, acc 1
2016-11-12T19:12:07.871273: step 5046, loss 0.0276522, acc 0.984375
2016-11-12T19:12:07.928141: step 5047, loss 0.0444373, acc 0.984375
2016-11-12T19:12:07.987931: step 5048, loss 0.03304, acc 0.984375
2016-11-12T19:12:08.045131: step 5049, loss 0.206607, acc 0.984375
2016-11-12T19:12:08.103211: step 5050, loss 0.0166386, acc 0.984375
2016-11-12T19:12:08.161253: step 5051, loss 0.0016163, acc 1
2016-11-12T19:12:08.217950: step 5052, loss 0.011979, acc 1
2016-11-12T19:12:08.275801: step 5053, loss 0.107005, acc 0.984375
2016-11-12T19:12:08.332738: step 5054, loss 0.00201697, acc 1
2016-11-12T19:12:08.390186: step 5055, loss 0.00154357, acc 1
2016-11-12T19:12:08.449661: step 5056, loss 0.00321914, acc 1
2016-11-12T19:12:08.509210: step 5057, loss 0.0207215, acc 0.984375
2016-11-12T19:12:08.567851: step 5058, loss 0.0116532, acc 1
2016-11-12T19:12:08.628664: step 5059, loss 0.0566534, acc 0.984375
2016-11-12T19:12:08.687556: step 5060, loss 0.0187066, acc 0.984375
2016-11-12T19:12:08.743885: step 5061, loss 0.00452716, acc 1
2016-11-12T19:12:08.800141: step 5062, loss 0.00151892, acc 1
2016-11-12T19:12:08.857552: step 5063, loss 0.0237045, acc 0.984375
2016-11-12T19:12:08.917210: step 5064, loss 0.00300317, acc 1
2016-11-12T19:12:08.976618: step 5065, loss 0.00199172, acc 1
2016-11-12T19:12:09.037181: step 5066, loss 0.00132051, acc 1
2016-11-12T19:12:09.093319: step 5067, loss 0.00634108, acc 1
2016-11-12T19:12:09.151670: step 5068, loss 0.00102098, acc 1
2016-11-12T19:12:09.209133: step 5069, loss 0.00372733, acc 1
2016-11-12T19:12:09.268745: step 5070, loss 0.0186833, acc 1
2016-11-12T19:12:09.328134: step 5071, loss 0.0232053, acc 0.984375
2016-11-12T19:12:09.385634: step 5072, loss 0.00276315, acc 1
2016-11-12T19:12:09.443965: step 5073, loss 0.00143221, acc 1
2016-11-12T19:12:09.501446: step 5074, loss 0.00411535, acc 1
2016-11-12T19:12:09.559223: step 5075, loss 0.00367343, acc 1
2016-11-12T19:12:09.616024: step 5076, loss 0.0095686, acc 1
2016-11-12T19:12:09.672496: step 5077, loss 0.00269788, acc 1
2016-11-12T19:12:09.729154: step 5078, loss 0.107483, acc 0.984375
2016-11-12T19:12:09.787184: step 5079, loss 0.000595709, acc 1
2016-11-12T19:12:09.845591: step 5080, loss 0.00140086, acc 1
2016-11-12T19:12:09.902552: step 5081, loss 0.00467535, acc 1
2016-11-12T19:12:09.960228: step 5082, loss 0.0033755, acc 1
2016-11-12T19:12:10.017960: step 5083, loss 0.00287631, acc 1
2016-11-12T19:12:10.075160: step 5084, loss 0.00662482, acc 1
2016-11-12T19:12:10.133158: step 5085, loss 0.00376944, acc 1
2016-11-12T19:12:10.193178: step 5086, loss 0.00187716, acc 1
2016-11-12T19:12:10.251217: step 5087, loss 0.000944761, acc 1
2016-11-12T19:12:10.307708: step 5088, loss 0.0191026, acc 0.984375
2016-11-12T19:12:10.363908: step 5089, loss 0.00150355, acc 1
2016-11-12T19:12:10.423825: step 5090, loss 0.014167, acc 0.984375
2016-11-12T19:12:10.482487: step 5091, loss 0.00906053, acc 1
2016-11-12T19:12:10.541202: step 5092, loss 0.072208, acc 0.984375
2016-11-12T19:12:10.599068: step 5093, loss 0.0282117, acc 0.984375
2016-11-12T19:12:10.655854: step 5094, loss 0.00126013, acc 1
2016-11-12T19:12:10.713203: step 5095, loss 0.00317394, acc 1
2016-11-12T19:12:10.772153: step 5096, loss 0.00151206, acc 1
2016-11-12T19:12:10.828934: step 5097, loss 0.0481322, acc 0.984375
2016-11-12T19:12:10.886778: step 5098, loss 0.00965451, acc 1
2016-11-12T19:12:10.945144: step 5099, loss 0.00113347, acc 1
2016-11-12T19:12:11.001491: step 5100, loss 0.00765076, acc 1

Evaluation:
2016-11-12T19:12:11.072168: step 5100, loss 2.52336, acc 0.574

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-5100

2016-11-12T19:12:11.570001: step 5101, loss 0.0031186, acc 1
2016-11-12T19:12:11.628653: step 5102, loss 0.0583719, acc 0.984375
2016-11-12T19:12:11.687800: step 5103, loss 0.00319821, acc 1
2016-11-12T19:12:11.744799: step 5104, loss 0.0179769, acc 1
2016-11-12T19:12:11.801725: step 5105, loss 0.00138536, acc 1
2016-11-12T19:12:11.858899: step 5106, loss 0.0237739, acc 0.984375
2016-11-12T19:12:11.916796: step 5107, loss 0.000273063, acc 1
2016-11-12T19:12:11.973343: step 5108, loss 0.0163871, acc 1
2016-11-12T19:12:12.032143: step 5109, loss 0.0532158, acc 0.984375
2016-11-12T19:12:12.092975: step 5110, loss 0.0116846, acc 1
2016-11-12T19:12:12.151446: step 5111, loss 0.00537456, acc 1
2016-11-12T19:12:12.188751: step 5112, loss 0.000555177, acc 1
2016-11-12T19:12:12.248501: step 5113, loss 0.0478776, acc 0.96875
2016-11-12T19:12:12.305426: step 5114, loss 0.00947262, acc 1
2016-11-12T19:12:12.365089: step 5115, loss 0.00475893, acc 1
2016-11-12T19:12:12.423686: step 5116, loss 0.0518649, acc 0.984375
2016-11-12T19:12:12.483344: step 5117, loss 0.0426945, acc 0.96875
2016-11-12T19:12:12.540843: step 5118, loss 0.00517225, acc 1
2016-11-12T19:12:12.599751: step 5119, loss 0.00117277, acc 1
2016-11-12T19:12:12.657032: step 5120, loss 0.00454505, acc 1
2016-11-12T19:12:12.715936: step 5121, loss 0.00246185, acc 1
2016-11-12T19:12:12.771785: step 5122, loss 0.0390276, acc 0.984375
2016-11-12T19:12:12.828984: step 5123, loss 0.00291297, acc 1
2016-11-12T19:12:12.885520: step 5124, loss 0.0012405, acc 1
2016-11-12T19:12:12.941601: step 5125, loss 0.0205908, acc 1
2016-11-12T19:12:13.001133: step 5126, loss 0.00141192, acc 1
2016-11-12T19:12:13.057377: step 5127, loss 0.0213572, acc 0.984375
2016-11-12T19:12:13.115823: step 5128, loss 0.125276, acc 0.96875
2016-11-12T19:12:13.174677: step 5129, loss 0.000567616, acc 1
2016-11-12T19:12:13.230839: step 5130, loss 0.00104999, acc 1
2016-11-12T19:12:13.287229: step 5131, loss 0.00164513, acc 1
2016-11-12T19:12:13.346071: step 5132, loss 0.0431846, acc 0.984375
2016-11-12T19:12:13.405849: step 5133, loss 0.0230867, acc 0.984375
2016-11-12T19:12:13.462808: step 5134, loss 0.000691892, acc 1
2016-11-12T19:12:13.521429: step 5135, loss 0.000709166, acc 1
2016-11-12T19:12:13.578811: step 5136, loss 0.0183446, acc 1
2016-11-12T19:12:13.635802: step 5137, loss 0.00244424, acc 1
2016-11-12T19:12:13.692043: step 5138, loss 0.0142767, acc 1
2016-11-12T19:12:13.748274: step 5139, loss 0.00215061, acc 1
2016-11-12T19:12:13.808050: step 5140, loss 0.00267709, acc 1
2016-11-12T19:12:13.865478: step 5141, loss 0.00203787, acc 1
2016-11-12T19:12:13.923662: step 5142, loss 0.00828523, acc 1
2016-11-12T19:12:13.981226: step 5143, loss 0.009587, acc 1
2016-11-12T19:12:14.040819: step 5144, loss 0.00106213, acc 1
2016-11-12T19:12:14.097776: step 5145, loss 0.0011182, acc 1
2016-11-12T19:12:14.157218: step 5146, loss 0.0721196, acc 0.984375
2016-11-12T19:12:14.215911: step 5147, loss 0.00714381, acc 1
2016-11-12T19:12:14.273358: step 5148, loss 0.00808924, acc 1
2016-11-12T19:12:14.329963: step 5149, loss 0.0181021, acc 0.984375
2016-11-12T19:12:14.387947: step 5150, loss 0.00965871, acc 1
2016-11-12T19:12:14.446403: step 5151, loss 0.20464, acc 0.96875
2016-11-12T19:12:14.504910: step 5152, loss 0.0173516, acc 0.984375
2016-11-12T19:12:14.562411: step 5153, loss 0.00627954, acc 1
2016-11-12T19:12:14.623239: step 5154, loss 0.00635318, acc 1
2016-11-12T19:12:14.680110: step 5155, loss 0.0141561, acc 1
2016-11-12T19:12:14.739116: step 5156, loss 0.09921, acc 0.984375
2016-11-12T19:12:14.798430: step 5157, loss 0.00312834, acc 1
2016-11-12T19:12:14.855225: step 5158, loss 0.0110688, acc 1
2016-11-12T19:12:14.912949: step 5159, loss 0.0671695, acc 0.953125
2016-11-12T19:12:14.971084: step 5160, loss 0.00151366, acc 1
2016-11-12T19:12:15.026428: step 5161, loss 0.0577945, acc 0.984375
2016-11-12T19:12:15.085859: step 5162, loss 0.00615394, acc 1
2016-11-12T19:12:15.143195: step 5163, loss 0.000662088, acc 1
2016-11-12T19:12:15.198655: step 5164, loss 0.000343555, acc 1
2016-11-12T19:12:15.254328: step 5165, loss 0.0186713, acc 0.984375
2016-11-12T19:12:15.311289: step 5166, loss 0.0343475, acc 0.96875
2016-11-12T19:12:15.368946: step 5167, loss 0.00959165, acc 1
2016-11-12T19:12:15.426168: step 5168, loss 0.00116335, acc 1
2016-11-12T19:12:15.483403: step 5169, loss 0.00379623, acc 1
2016-11-12T19:12:15.538959: step 5170, loss 0.045225, acc 0.984375
2016-11-12T19:12:15.596689: step 5171, loss 0.00339305, acc 1
2016-11-12T19:12:15.654026: step 5172, loss 0.0507229, acc 0.984375
2016-11-12T19:12:15.714194: step 5173, loss 0.00802816, acc 1
2016-11-12T19:12:15.772603: step 5174, loss 0.0062032, acc 1
2016-11-12T19:12:15.829952: step 5175, loss 0.0589063, acc 0.984375
2016-11-12T19:12:15.886955: step 5176, loss 0.000699165, acc 1
2016-11-12T19:12:15.942686: step 5177, loss 0.00798838, acc 1
2016-11-12T19:12:16.001401: step 5178, loss 0.0075048, acc 1
2016-11-12T19:12:16.058792: step 5179, loss 0.000491637, acc 1
2016-11-12T19:12:16.115197: step 5180, loss 0.0987604, acc 0.984375
2016-11-12T19:12:16.173850: step 5181, loss 0.0180031, acc 0.984375
2016-11-12T19:12:16.231564: step 5182, loss 0.0212028, acc 1
2016-11-12T19:12:16.272227: step 5183, loss 0.000298981, acc 1
2016-11-12T19:12:16.332657: step 5184, loss 0.00611676, acc 1
2016-11-12T19:12:16.390429: step 5185, loss 0.00511371, acc 1
2016-11-12T19:12:16.449142: step 5186, loss 0.0270609, acc 0.984375
2016-11-12T19:12:16.505792: step 5187, loss 0.0297552, acc 0.984375
2016-11-12T19:12:16.563431: step 5188, loss 0.000810106, acc 1
2016-11-12T19:12:16.621899: step 5189, loss 0.0190586, acc 0.984375
2016-11-12T19:12:16.682183: step 5190, loss 0.00170885, acc 1
2016-11-12T19:12:16.740797: step 5191, loss 0.0224547, acc 0.984375
2016-11-12T19:12:16.797003: step 5192, loss 0.000921226, acc 1
2016-11-12T19:12:16.853174: step 5193, loss 0.0244936, acc 0.984375
2016-11-12T19:12:16.911894: step 5194, loss 0.00304305, acc 1
2016-11-12T19:12:16.969070: step 5195, loss 0.00491917, acc 1
2016-11-12T19:12:17.025750: step 5196, loss 0.00094872, acc 1
2016-11-12T19:12:17.085685: step 5197, loss 0.00497107, acc 1
2016-11-12T19:12:17.142175: step 5198, loss 0.00653164, acc 1
2016-11-12T19:12:17.201046: step 5199, loss 0.227711, acc 0.984375
2016-11-12T19:12:17.258366: step 5200, loss 0.0196567, acc 0.984375

Evaluation:
2016-11-12T19:12:17.330217: step 5200, loss 2.54066, acc 0.566

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-5200

2016-11-12T19:12:17.828753: step 5201, loss 0.0180807, acc 0.984375
2016-11-12T19:12:17.886810: step 5202, loss 0.000687131, acc 1
2016-11-12T19:12:17.944499: step 5203, loss 0.000935044, acc 1
2016-11-12T19:12:18.004123: step 5204, loss 0.0654221, acc 0.96875
2016-11-12T19:12:18.062912: step 5205, loss 0.0252343, acc 0.984375
2016-11-12T19:12:18.120468: step 5206, loss 9.39872e-05, acc 1
2016-11-12T19:12:18.176115: step 5207, loss 0.00103066, acc 1
2016-11-12T19:12:18.236825: step 5208, loss 0.0340087, acc 0.96875
2016-11-12T19:12:18.294584: step 5209, loss 0.0192755, acc 0.984375
2016-11-12T19:12:18.354442: step 5210, loss 0.0161958, acc 0.984375
2016-11-12T19:12:18.412691: step 5211, loss 0.00781442, acc 1
2016-11-12T19:12:18.473186: step 5212, loss 0.00400416, acc 1
2016-11-12T19:12:18.533238: step 5213, loss 0.000983963, acc 1
2016-11-12T19:12:18.591145: step 5214, loss 0.000746478, acc 1
2016-11-12T19:12:18.647667: step 5215, loss 0.00125865, acc 1
2016-11-12T19:12:18.703073: step 5216, loss 0.00237168, acc 1
2016-11-12T19:12:18.759199: step 5217, loss 0.0119275, acc 1
2016-11-12T19:12:18.817813: step 5218, loss 0.0074628, acc 1
2016-11-12T19:12:18.876538: step 5219, loss 0.000401803, acc 1
2016-11-12T19:12:18.936090: step 5220, loss 0.0281855, acc 0.96875
2016-11-12T19:12:18.997472: step 5221, loss 0.000255672, acc 1
2016-11-12T19:12:19.053490: step 5222, loss 0.00141215, acc 1
2016-11-12T19:12:19.110192: step 5223, loss 0.0161188, acc 0.984375
2016-11-12T19:12:19.167576: step 5224, loss 0.0397163, acc 0.984375
2016-11-12T19:12:19.224168: step 5225, loss 0.00213804, acc 1
2016-11-12T19:12:19.280428: step 5226, loss 0.000708393, acc 1
2016-11-12T19:12:19.337754: step 5227, loss 0.00497814, acc 1
2016-11-12T19:12:19.395978: step 5228, loss 0.00399588, acc 1
2016-11-12T19:12:19.453114: step 5229, loss 0.009324, acc 1
2016-11-12T19:12:19.513227: step 5230, loss 0.00659018, acc 1
2016-11-12T19:12:19.572280: step 5231, loss 0.0193166, acc 0.984375
2016-11-12T19:12:19.631122: step 5232, loss 0.00229574, acc 1
2016-11-12T19:12:19.687677: step 5233, loss 0.00199194, acc 1
2016-11-12T19:12:19.744872: step 5234, loss 0.0109313, acc 1
2016-11-12T19:12:19.801454: step 5235, loss 0.00711915, acc 1
2016-11-12T19:12:19.857458: step 5236, loss 0.00137341, acc 1
2016-11-12T19:12:19.915755: step 5237, loss 0.00299217, acc 1
2016-11-12T19:12:19.973406: step 5238, loss 0.0453836, acc 0.984375
2016-11-12T19:12:20.034437: step 5239, loss 0.000740298, acc 1
2016-11-12T19:12:20.092157: step 5240, loss 0.00267724, acc 1
2016-11-12T19:12:20.150396: step 5241, loss 0.0446036, acc 0.96875
2016-11-12T19:12:20.208933: step 5242, loss 0.00199244, acc 1
2016-11-12T19:12:20.269070: step 5243, loss 0.00963513, acc 1
2016-11-12T19:12:20.327766: step 5244, loss 0.00758312, acc 1
2016-11-12T19:12:20.384805: step 5245, loss 0.000758238, acc 1
2016-11-12T19:12:20.441896: step 5246, loss 0.001712, acc 1
2016-11-12T19:12:20.504700: step 5247, loss 0.00308371, acc 1
2016-11-12T19:12:20.562332: step 5248, loss 0.0133072, acc 1
2016-11-12T19:12:20.620290: step 5249, loss 0.0644467, acc 0.984375
2016-11-12T19:12:20.677871: step 5250, loss 0.0407981, acc 0.984375
2016-11-12T19:12:20.737118: step 5251, loss 0.00367735, acc 1
2016-11-12T19:12:20.792906: step 5252, loss 0.00362754, acc 1
2016-11-12T19:12:20.853251: step 5253, loss 0.00404752, acc 1
2016-11-12T19:12:20.891129: step 5254, loss 0.000231563, acc 1
2016-11-12T19:12:20.949429: step 5255, loss 0.0105038, acc 1
2016-11-12T19:12:21.008408: step 5256, loss 0.0232803, acc 0.984375
2016-11-12T19:12:21.067183: step 5257, loss 0.0562338, acc 0.96875
2016-11-12T19:12:21.123905: step 5258, loss 0.0421852, acc 0.984375
2016-11-12T19:12:21.181973: step 5259, loss 0.00663325, acc 1
2016-11-12T19:12:21.239795: step 5260, loss 0.00480903, acc 1
2016-11-12T19:12:21.296624: step 5261, loss 0.00938839, acc 1
2016-11-12T19:12:21.356852: step 5262, loss 0.0019708, acc 1
2016-11-12T19:12:21.411969: step 5263, loss 0.0048326, acc 1
2016-11-12T19:12:21.469359: step 5264, loss 0.000747401, acc 1
2016-11-12T19:12:21.528089: step 5265, loss 0.00635385, acc 1
2016-11-12T19:12:21.584709: step 5266, loss 0.00225988, acc 1
2016-11-12T19:12:21.641694: step 5267, loss 0.000617659, acc 1
2016-11-12T19:12:21.698162: step 5268, loss 0.00167281, acc 1
2016-11-12T19:12:21.754488: step 5269, loss 0.00728668, acc 1
2016-11-12T19:12:21.812129: step 5270, loss 0.00489822, acc 1
2016-11-12T19:12:21.868661: step 5271, loss 0.000909103, acc 1
2016-11-12T19:12:21.927329: step 5272, loss 0.000915967, acc 1
2016-11-12T19:12:21.985556: step 5273, loss 0.0219959, acc 0.984375
2016-11-12T19:12:22.043239: step 5274, loss 0.00232492, acc 1
2016-11-12T19:12:22.099783: step 5275, loss 0.00165874, acc 1
2016-11-12T19:12:22.157244: step 5276, loss 0.00169705, acc 1
2016-11-12T19:12:22.216282: step 5277, loss 8.97894e-05, acc 1
2016-11-12T19:12:22.273152: step 5278, loss 0.00977953, acc 1
2016-11-12T19:12:22.333302: step 5279, loss 0.00345906, acc 1
2016-11-12T19:12:22.390002: step 5280, loss 0.0411944, acc 0.96875
2016-11-12T19:12:22.449014: step 5281, loss 0.0084849, acc 1
2016-11-12T19:12:22.508528: step 5282, loss 0.00200474, acc 1
2016-11-12T19:12:22.565305: step 5283, loss 0.0547228, acc 0.984375
2016-11-12T19:12:22.623864: step 5284, loss 0.000626769, acc 1
2016-11-12T19:12:22.681456: step 5285, loss 0.00832578, acc 1
2016-11-12T19:12:22.738835: step 5286, loss 0.0958788, acc 0.984375
2016-11-12T19:12:22.797579: step 5287, loss 0.0283793, acc 0.984375
2016-11-12T19:12:22.854813: step 5288, loss 0.0017819, acc 1
2016-11-12T19:12:22.912613: step 5289, loss 0.0010287, acc 1
2016-11-12T19:12:22.970235: step 5290, loss 0.0145643, acc 0.984375
2016-11-12T19:12:23.029039: step 5291, loss 0.000915997, acc 1
2016-11-12T19:12:23.086118: step 5292, loss 0.00504951, acc 1
2016-11-12T19:12:23.145469: step 5293, loss 0.000712749, acc 1
2016-11-12T19:12:23.200528: step 5294, loss 0.00235135, acc 1
2016-11-12T19:12:23.257522: step 5295, loss 0.0154228, acc 0.984375
2016-11-12T19:12:23.315928: step 5296, loss 0.00480581, acc 1
2016-11-12T19:12:23.372455: step 5297, loss 0.00746775, acc 1
2016-11-12T19:12:23.429478: step 5298, loss 0.00139673, acc 1
2016-11-12T19:12:23.485360: step 5299, loss 0.0339203, acc 0.984375
2016-11-12T19:12:23.545456: step 5300, loss 0.00394484, acc 1

Evaluation:
2016-11-12T19:12:23.615556: step 5300, loss 2.56881, acc 0.57

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-5300

2016-11-12T19:12:24.109256: step 5301, loss 0.0260677, acc 0.984375
2016-11-12T19:12:24.167279: step 5302, loss 0.0474139, acc 0.984375
2016-11-12T19:12:24.226686: step 5303, loss 0.00193348, acc 1
2016-11-12T19:12:24.284284: step 5304, loss 0.00131438, acc 1
2016-11-12T19:12:24.341065: step 5305, loss 0.00234208, acc 1
2016-11-12T19:12:24.398271: step 5306, loss 0.00528592, acc 1
2016-11-12T19:12:24.457348: step 5307, loss 0.0425373, acc 0.984375
2016-11-12T19:12:24.516045: step 5308, loss 0.00356322, acc 1
2016-11-12T19:12:24.572716: step 5309, loss 0.00256802, acc 1
2016-11-12T19:12:24.628898: step 5310, loss 0.0693337, acc 0.96875
2016-11-12T19:12:24.688956: step 5311, loss 0.00156702, acc 1
2016-11-12T19:12:24.748362: step 5312, loss 0.0141871, acc 0.984375
2016-11-12T19:12:24.808193: step 5313, loss 0.0260815, acc 0.984375
2016-11-12T19:12:24.865327: step 5314, loss 0.0461839, acc 0.984375
2016-11-12T19:12:24.921910: step 5315, loss 0.0273956, acc 0.984375
2016-11-12T19:12:24.981493: step 5316, loss 0.0199514, acc 0.984375
2016-11-12T19:12:25.038671: step 5317, loss 0.00449294, acc 1
2016-11-12T19:12:25.096925: step 5318, loss 0.0166604, acc 0.984375
2016-11-12T19:12:25.153688: step 5319, loss 0.0049248, acc 1
2016-11-12T19:12:25.211020: step 5320, loss 0.00205708, acc 1
2016-11-12T19:12:25.269525: step 5321, loss 0.00180708, acc 1
2016-11-12T19:12:25.325830: step 5322, loss 0.171526, acc 0.984375
2016-11-12T19:12:25.387844: step 5323, loss 0.0112867, acc 1
2016-11-12T19:12:25.445080: step 5324, loss 0.00267773, acc 1
2016-11-12T19:12:25.482677: step 5325, loss 0.00153581, acc 1
2016-11-12T19:12:25.540475: step 5326, loss 0.00057089, acc 1
2016-11-12T19:12:25.598224: step 5327, loss 0.00235534, acc 1
2016-11-12T19:12:25.655651: step 5328, loss 0.0262159, acc 0.984375
2016-11-12T19:12:25.713566: step 5329, loss 0.00397249, acc 1
2016-11-12T19:12:25.773060: step 5330, loss 0.000448727, acc 1
2016-11-12T19:12:25.828732: step 5331, loss 0.0113624, acc 1
2016-11-12T19:12:25.886635: step 5332, loss 0.00214465, acc 1
2016-11-12T19:12:25.943032: step 5333, loss 0.00137232, acc 1
2016-11-12T19:12:26.001611: step 5334, loss 0.000686996, acc 1
2016-11-12T19:12:26.057483: step 5335, loss 0.0103288, acc 1
2016-11-12T19:12:26.114788: step 5336, loss 0.00397135, acc 1
2016-11-12T19:12:26.171246: step 5337, loss 0.0159539, acc 0.984375
2016-11-12T19:12:26.230396: step 5338, loss 0.062557, acc 0.984375
2016-11-12T19:12:26.289135: step 5339, loss 0.0858581, acc 0.984375
2016-11-12T19:12:26.349663: step 5340, loss 0.00571991, acc 1
2016-11-12T19:12:26.408136: step 5341, loss 0.00304393, acc 1
2016-11-12T19:12:26.469087: step 5342, loss 0.002449, acc 1
2016-11-12T19:12:26.524794: step 5343, loss 0.0113922, acc 1
2016-11-12T19:12:26.583433: step 5344, loss 0.00176436, acc 1
2016-11-12T19:12:26.644612: step 5345, loss 0.00832077, acc 1
2016-11-12T19:12:26.701260: step 5346, loss 0.00162162, acc 1
2016-11-12T19:12:26.757944: step 5347, loss 0.0065886, acc 1
2016-11-12T19:12:26.814409: step 5348, loss 0.00400557, acc 1
2016-11-12T19:12:26.872849: step 5349, loss 0.00819854, acc 1
2016-11-12T19:12:26.930675: step 5350, loss 0.00525406, acc 1
2016-11-12T19:12:26.986513: step 5351, loss 0.00498699, acc 1
2016-11-12T19:12:27.042965: step 5352, loss 0.00069478, acc 1
2016-11-12T19:12:27.099091: step 5353, loss 0.0353777, acc 0.984375
2016-11-12T19:12:27.156345: step 5354, loss 0.00422514, acc 1
2016-11-12T19:12:27.213417: step 5355, loss 0.00161282, acc 1
2016-11-12T19:12:27.270324: step 5356, loss 0.0951533, acc 0.96875
2016-11-12T19:12:27.329487: step 5357, loss 0.00496636, acc 1
2016-11-12T19:12:27.392132: step 5358, loss 0.0532947, acc 0.984375
2016-11-12T19:12:27.452692: step 5359, loss 0.00112643, acc 1
2016-11-12T19:12:27.510711: step 5360, loss 0.0003196, acc 1
2016-11-12T19:12:27.567359: step 5361, loss 0.000598469, acc 1
2016-11-12T19:12:27.624482: step 5362, loss 0.0116156, acc 1
2016-11-12T19:12:27.684613: step 5363, loss 0.000822987, acc 1
2016-11-12T19:12:27.742464: step 5364, loss 0.00365741, acc 1
2016-11-12T19:12:27.801208: step 5365, loss 0.0203495, acc 0.984375
2016-11-12T19:12:27.860939: step 5366, loss 0.00500354, acc 1
2016-11-12T19:12:27.920244: step 5367, loss 0.00216088, acc 1
2016-11-12T19:12:27.977135: step 5368, loss 0.000900037, acc 1
2016-11-12T19:12:28.035033: step 5369, loss 0.00127917, acc 1
2016-11-12T19:12:28.094035: step 5370, loss 0.0091095, acc 1
2016-11-12T19:12:28.153289: step 5371, loss 0.00146947, acc 1
2016-11-12T19:12:28.213694: step 5372, loss 0.0276509, acc 0.984375
2016-11-12T19:12:28.272391: step 5373, loss 0.0156801, acc 0.984375
2016-11-12T19:12:28.329156: step 5374, loss 0.00200945, acc 1
2016-11-12T19:12:28.385867: step 5375, loss 0.0179573, acc 1
2016-11-12T19:12:28.442204: step 5376, loss 0.0999723, acc 0.96875
2016-11-12T19:12:28.501277: step 5377, loss 0.0184513, acc 0.984375
2016-11-12T19:12:28.559064: step 5378, loss 0.00364608, acc 1
2016-11-12T19:12:28.616200: step 5379, loss 0.00206431, acc 1
2016-11-12T19:12:28.673496: step 5380, loss 0.00367445, acc 1
2016-11-12T19:12:28.731650: step 5381, loss 0.00429742, acc 1
2016-11-12T19:12:28.789077: step 5382, loss 0.0322573, acc 0.984375
2016-11-12T19:12:28.847529: step 5383, loss 0.00300731, acc 1
2016-11-12T19:12:28.905371: step 5384, loss 0.00120123, acc 1
2016-11-12T19:12:28.962015: step 5385, loss 0.00121265, acc 1
2016-11-12T19:12:29.018778: step 5386, loss 0.0208809, acc 0.984375
2016-11-12T19:12:29.075722: step 5387, loss 0.0148267, acc 0.984375
2016-11-12T19:12:29.132922: step 5388, loss 0.0226229, acc 0.984375
2016-11-12T19:12:29.189493: step 5389, loss 0.0981845, acc 0.984375
2016-11-12T19:12:29.248299: step 5390, loss 0.0100375, acc 1
2016-11-12T19:12:29.304944: step 5391, loss 0.00262007, acc 1
2016-11-12T19:12:29.361299: step 5392, loss 0.0133469, acc 1
2016-11-12T19:12:29.418508: step 5393, loss 0.00187459, acc 1
2016-11-12T19:12:29.474074: step 5394, loss 0.0113387, acc 0.984375
2016-11-12T19:12:29.533333: step 5395, loss 0.015686, acc 0.984375
2016-11-12T19:12:29.572109: step 5396, loss 5.55848e-05, acc 1
2016-11-12T19:12:29.632756: step 5397, loss 0.000469926, acc 1
2016-11-12T19:12:29.689036: step 5398, loss 0.00632496, acc 1
2016-11-12T19:12:29.749236: step 5399, loss 0.015941, acc 0.984375
2016-11-12T19:12:29.809311: step 5400, loss 0.0227052, acc 0.984375

Evaluation:
2016-11-12T19:12:29.880003: step 5400, loss 2.63187, acc 0.556

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-5400

2016-11-12T19:12:30.380548: step 5401, loss 0.0106677, acc 1
2016-11-12T19:12:30.438380: step 5402, loss 0.0296914, acc 0.96875
2016-11-12T19:12:30.497481: step 5403, loss 0.025481, acc 0.984375
2016-11-12T19:12:30.555953: step 5404, loss 0.00188827, acc 1
2016-11-12T19:12:30.613641: step 5405, loss 0.00220589, acc 1
2016-11-12T19:12:30.672084: step 5406, loss 0.0017451, acc 1
2016-11-12T19:12:30.729722: step 5407, loss 0.00110677, acc 1
2016-11-12T19:12:30.787081: step 5408, loss 0.00461902, acc 1
2016-11-12T19:12:30.847713: step 5409, loss 0.0118177, acc 1
2016-11-12T19:12:30.904251: step 5410, loss 0.003894, acc 1
2016-11-12T19:12:30.960014: step 5411, loss 0.0141172, acc 0.984375
2016-11-12T19:12:31.017372: step 5412, loss 0.0032454, acc 1
2016-11-12T19:12:31.075765: step 5413, loss 0.00355333, acc 1
2016-11-12T19:12:31.132516: step 5414, loss 0.00116654, acc 1
2016-11-12T19:12:31.188498: step 5415, loss 0.00870914, acc 1
2016-11-12T19:12:31.245128: step 5416, loss 0.000701418, acc 1
2016-11-12T19:12:31.303250: step 5417, loss 0.0166194, acc 0.984375
2016-11-12T19:12:31.363857: step 5418, loss 0.00141604, acc 1
2016-11-12T19:12:31.421273: step 5419, loss 0.0456709, acc 0.96875
2016-11-12T19:12:31.484207: step 5420, loss 0.0340691, acc 0.984375
2016-11-12T19:12:31.541659: step 5421, loss 0.000360898, acc 1
2016-11-12T19:12:31.600191: step 5422, loss 0.00229084, acc 1
2016-11-12T19:12:31.658150: step 5423, loss 0.00290009, acc 1
2016-11-12T19:12:31.714799: step 5424, loss 0.00183474, acc 1
2016-11-12T19:12:31.771312: step 5425, loss 0.00903382, acc 1
2016-11-12T19:12:31.828001: step 5426, loss 0.00317305, acc 1
2016-11-12T19:12:31.889195: step 5427, loss 0.00142672, acc 1
2016-11-12T19:12:31.945836: step 5428, loss 0.0168167, acc 0.984375
2016-11-12T19:12:32.005582: step 5429, loss 0.00196665, acc 1
2016-11-12T19:12:32.061655: step 5430, loss 0.000444646, acc 1
2016-11-12T19:12:32.117972: step 5431, loss 0.0158755, acc 1
2016-11-12T19:12:32.177294: step 5432, loss 0.0221279, acc 0.984375
2016-11-12T19:12:32.236388: step 5433, loss 0.00765035, acc 1
2016-11-12T19:12:32.294809: step 5434, loss 0.0322084, acc 0.984375
2016-11-12T19:12:32.352135: step 5435, loss 0.0278467, acc 0.984375
2016-11-12T19:12:32.411413: step 5436, loss 0.000955887, acc 1
2016-11-12T19:12:32.470209: step 5437, loss 0.00311675, acc 1
2016-11-12T19:12:32.528889: step 5438, loss 0.0309853, acc 0.984375
2016-11-12T19:12:32.589207: step 5439, loss 0.27781, acc 0.953125
2016-11-12T19:12:32.652736: step 5440, loss 0.00642313, acc 1
2016-11-12T19:12:32.711460: step 5441, loss 0.000928446, acc 1
2016-11-12T19:12:32.770782: step 5442, loss 0.0287152, acc 0.984375
2016-11-12T19:12:32.827288: step 5443, loss 0.00138897, acc 1
2016-11-12T19:12:32.883701: step 5444, loss 0.0170186, acc 0.984375
2016-11-12T19:12:32.941152: step 5445, loss 0.00264304, acc 1
2016-11-12T19:12:32.997022: step 5446, loss 0.060882, acc 0.984375
2016-11-12T19:12:33.055902: step 5447, loss 0.00190212, acc 1
2016-11-12T19:12:33.113431: step 5448, loss 0.0373456, acc 0.984375
2016-11-12T19:12:33.173378: step 5449, loss 0.0840121, acc 0.96875
2016-11-12T19:12:33.232541: step 5450, loss 0.010659, acc 1
2016-11-12T19:12:33.289228: step 5451, loss 0.000455906, acc 1
2016-11-12T19:12:33.346814: step 5452, loss 0.00347659, acc 1
2016-11-12T19:12:33.404033: step 5453, loss 0.00268299, acc 1
2016-11-12T19:12:33.461750: step 5454, loss 0.00208513, acc 1
2016-11-12T19:12:33.520733: step 5455, loss 0.00377866, acc 1
2016-11-12T19:12:33.580292: step 5456, loss 0.0152725, acc 0.984375
2016-11-12T19:12:33.639521: step 5457, loss 0.0678458, acc 0.96875
2016-11-12T19:12:33.697260: step 5458, loss 0.00844573, acc 1
2016-11-12T19:12:33.756839: step 5459, loss 0.00432692, acc 1
2016-11-12T19:12:33.814651: step 5460, loss 0.0113863, acc 1
2016-11-12T19:12:33.873600: step 5461, loss 0.00519202, acc 1
2016-11-12T19:12:33.934217: step 5462, loss 0.003448, acc 1
2016-11-12T19:12:33.992083: step 5463, loss 0.00292309, acc 1
2016-11-12T19:12:34.049600: step 5464, loss 0.00376644, acc 1
2016-11-12T19:12:34.106272: step 5465, loss 0.00143637, acc 1
2016-11-12T19:12:34.162191: step 5466, loss 0.0585993, acc 0.96875
2016-11-12T19:12:34.200397: step 5467, loss 0.000506203, acc 1
2016-11-12T19:12:34.260715: step 5468, loss 0.00433539, acc 1
2016-11-12T19:12:34.317396: step 5469, loss 0.0276054, acc 0.984375
2016-11-12T19:12:34.376983: step 5470, loss 0.00152696, acc 1
2016-11-12T19:12:34.433583: step 5471, loss 0.0146424, acc 1
2016-11-12T19:12:34.491839: step 5472, loss 0.0308797, acc 0.984375
2016-11-12T19:12:34.548897: step 5473, loss 0.0274831, acc 0.984375
2016-11-12T19:12:34.606868: step 5474, loss 0.00145481, acc 1
2016-11-12T19:12:34.665543: step 5475, loss 0.00417231, acc 1
2016-11-12T19:12:34.722529: step 5476, loss 0.000902118, acc 1
2016-11-12T19:12:34.781268: step 5477, loss 0.000228713, acc 1
2016-11-12T19:12:34.840574: step 5478, loss 0.0022969, acc 1
2016-11-12T19:12:34.897447: step 5479, loss 0.0337183, acc 0.984375
2016-11-12T19:12:34.956376: step 5480, loss 0.00149656, acc 1
2016-11-12T19:12:35.014793: step 5481, loss 0.016326, acc 0.984375
2016-11-12T19:12:35.073355: step 5482, loss 0.00202669, acc 1
2016-11-12T19:12:35.132574: step 5483, loss 0.00629338, acc 1
2016-11-12T19:12:35.189096: step 5484, loss 0.00338199, acc 1
2016-11-12T19:12:35.247809: step 5485, loss 0.0010713, acc 1
2016-11-12T19:12:35.305861: step 5486, loss 0.00608092, acc 1
2016-11-12T19:12:35.365160: step 5487, loss 0.00280693, acc 1
2016-11-12T19:12:35.424373: step 5488, loss 0.00806343, acc 1
2016-11-12T19:12:35.480979: step 5489, loss 0.0274203, acc 0.984375
2016-11-12T19:12:35.540073: step 5490, loss 0.000560178, acc 1
2016-11-12T19:12:35.597484: step 5491, loss 0.0123354, acc 1
2016-11-12T19:12:35.657659: step 5492, loss 0.0407294, acc 0.984375
2016-11-12T19:12:35.717858: step 5493, loss 0.00179343, acc 1
2016-11-12T19:12:35.773667: step 5494, loss 0.0249258, acc 0.984375
2016-11-12T19:12:35.830486: step 5495, loss 0.00398719, acc 1
2016-11-12T19:12:35.888053: step 5496, loss 0.016431, acc 0.984375
2016-11-12T19:12:35.946522: step 5497, loss 0.00411035, acc 1
2016-11-12T19:12:36.005457: step 5498, loss 0.00154419, acc 1
2016-11-12T19:12:36.061403: step 5499, loss 0.00136592, acc 1
2016-11-12T19:12:36.118112: step 5500, loss 0.00319585, acc 1

Evaluation:
2016-11-12T19:12:36.194522: step 5500, loss 2.66329, acc 0.576

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-5500

2016-11-12T19:12:36.687143: step 5501, loss 0.000764584, acc 1
2016-11-12T19:12:36.745102: step 5502, loss 0.000968292, acc 1
2016-11-12T19:12:36.801063: step 5503, loss 0.000613389, acc 1
2016-11-12T19:12:36.857054: step 5504, loss 0.00691767, acc 1
2016-11-12T19:12:36.917096: step 5505, loss 0.110325, acc 0.984375
2016-11-12T19:12:36.976853: step 5506, loss 0.0021738, acc 1
2016-11-12T19:12:37.032827: step 5507, loss 0.00657663, acc 1
2016-11-12T19:12:37.089667: step 5508, loss 0.0813026, acc 0.984375
2016-11-12T19:12:37.149305: step 5509, loss 0.0513184, acc 0.984375
2016-11-12T19:12:37.208493: step 5510, loss 0.00457014, acc 1
2016-11-12T19:12:37.268367: step 5511, loss 0.0175714, acc 0.984375
2016-11-12T19:12:37.324983: step 5512, loss 0.0427821, acc 0.984375
2016-11-12T19:12:37.384947: step 5513, loss 0.0088582, acc 1
2016-11-12T19:12:37.441081: step 5514, loss 0.000538754, acc 1
2016-11-12T19:12:37.497084: step 5515, loss 0.00570285, acc 1
2016-11-12T19:12:37.555098: step 5516, loss 0.0549201, acc 0.984375
2016-11-12T19:12:37.613746: step 5517, loss 0.0110833, acc 1
2016-11-12T19:12:37.673437: step 5518, loss 0.00169217, acc 1
2016-11-12T19:12:37.730737: step 5519, loss 0.0141791, acc 0.984375
2016-11-12T19:12:37.788688: step 5520, loss 0.0181962, acc 0.984375
2016-11-12T19:12:37.848003: step 5521, loss 0.00370279, acc 1
2016-11-12T19:12:37.905110: step 5522, loss 0.00392834, acc 1
2016-11-12T19:12:37.961578: step 5523, loss 0.00390473, acc 1
2016-11-12T19:12:38.019057: step 5524, loss 0.0215328, acc 1
2016-11-12T19:12:38.078521: step 5525, loss 0.00226832, acc 1
2016-11-12T19:12:38.136510: step 5526, loss 0.0226834, acc 0.984375
2016-11-12T19:12:38.193838: step 5527, loss 0.00478169, acc 1
2016-11-12T19:12:38.250620: step 5528, loss 0.00470847, acc 1
2016-11-12T19:12:38.309001: step 5529, loss 0.0312101, acc 0.96875
2016-11-12T19:12:38.369171: step 5530, loss 0.00248786, acc 1
2016-11-12T19:12:38.425008: step 5531, loss 0.0056633, acc 1
2016-11-12T19:12:38.481769: step 5532, loss 0.00438158, acc 1
2016-11-12T19:12:38.541262: step 5533, loss 0.00149529, acc 1
2016-11-12T19:12:38.598008: step 5534, loss 0.00156014, acc 1
2016-11-12T19:12:38.657070: step 5535, loss 0.00576764, acc 1
2016-11-12T19:12:38.713713: step 5536, loss 0.00110935, acc 1
2016-11-12T19:12:38.770030: step 5537, loss 0.00176491, acc 1
2016-11-12T19:12:38.806772: step 5538, loss 0.00102032, acc 1
2016-11-12T19:12:38.865265: step 5539, loss 0.0584494, acc 0.984375
2016-11-12T19:12:38.923255: step 5540, loss 0.00110048, acc 1
2016-11-12T19:12:38.981420: step 5541, loss 0.017684, acc 1
2016-11-12T19:12:39.037793: step 5542, loss 0.0162156, acc 0.984375
2016-11-12T19:12:39.095830: step 5543, loss 0.00227651, acc 1
2016-11-12T19:12:39.152521: step 5544, loss 0.0494485, acc 0.984375
2016-11-12T19:12:39.211898: step 5545, loss 0.0137438, acc 1
2016-11-12T19:12:39.268920: step 5546, loss 0.0460105, acc 0.96875
2016-11-12T19:12:39.328850: step 5547, loss 0.00175848, acc 1
2016-11-12T19:12:39.385457: step 5548, loss 0.00319332, acc 1
2016-11-12T19:12:39.441761: step 5549, loss 0.00203227, acc 1
2016-11-12T19:12:39.497332: step 5550, loss 0.0275375, acc 0.984375
2016-11-12T19:12:39.556999: step 5551, loss 0.0057124, acc 1
2016-11-12T19:12:39.617187: step 5552, loss 0.0320633, acc 0.984375
2016-11-12T19:12:39.675390: step 5553, loss 0.00652663, acc 1
2016-11-12T19:12:39.733882: step 5554, loss 0.0145443, acc 1
2016-11-12T19:12:39.790779: step 5555, loss 0.0222664, acc 0.984375
2016-11-12T19:12:39.848908: step 5556, loss 0.0159572, acc 1
2016-11-12T19:12:39.909156: step 5557, loss 0.00439989, acc 1
2016-11-12T19:12:39.969407: step 5558, loss 0.0323657, acc 0.984375
2016-11-12T19:12:40.027796: step 5559, loss 0.0193202, acc 1
2016-11-12T19:12:40.085878: step 5560, loss 0.00766006, acc 1
2016-11-12T19:12:40.143420: step 5561, loss 0.0130022, acc 0.984375
2016-11-12T19:12:40.201231: step 5562, loss 0.000772374, acc 1
2016-11-12T19:12:40.257188: step 5563, loss 0.0230233, acc 0.984375
2016-11-12T19:12:40.317543: step 5564, loss 0.00368347, acc 1
2016-11-12T19:12:40.377242: step 5565, loss 0.00261032, acc 1
2016-11-12T19:12:40.433232: step 5566, loss 0.00214596, acc 1
2016-11-12T19:12:40.489357: step 5567, loss 0.00043204, acc 1
2016-11-12T19:12:40.545376: step 5568, loss 0.0153962, acc 0.984375
2016-11-12T19:12:40.601921: step 5569, loss 0.0144599, acc 1
2016-11-12T19:12:40.661341: step 5570, loss 0.0039035, acc 1
2016-11-12T19:12:40.719192: step 5571, loss 0.00774799, acc 1
2016-11-12T19:12:40.776884: step 5572, loss 0.00091088, acc 1
2016-11-12T19:12:40.832655: step 5573, loss 0.00209456, acc 1
2016-11-12T19:12:40.891242: step 5574, loss 0.00796194, acc 1
2016-11-12T19:12:40.949381: step 5575, loss 0.00286972, acc 1
2016-11-12T19:12:41.006465: step 5576, loss 0.00266159, acc 1
2016-11-12T19:12:41.064314: step 5577, loss 0.0448609, acc 0.984375
2016-11-12T19:12:41.122141: step 5578, loss 0.0720504, acc 0.984375
2016-11-12T19:12:41.182166: step 5579, loss 0.00152583, acc 1
2016-11-12T19:12:41.239446: step 5580, loss 0.000895802, acc 1
2016-11-12T19:12:41.297259: step 5581, loss 0.00368413, acc 1
2016-11-12T19:12:41.353535: step 5582, loss 0.0150658, acc 0.984375
2016-11-12T19:12:41.414127: step 5583, loss 0.177173, acc 0.984375
2016-11-12T19:12:41.473037: step 5584, loss 0.1141, acc 0.96875
2016-11-12T19:12:41.529315: step 5585, loss 0.00152877, acc 1
2016-11-12T19:12:41.585706: step 5586, loss 0.00193438, acc 1
2016-11-12T19:12:41.645324: step 5587, loss 0.0294055, acc 0.984375
2016-11-12T19:12:41.702285: step 5588, loss 0.0125043, acc 1
2016-11-12T19:12:41.758785: step 5589, loss 0.0238361, acc 0.984375
2016-11-12T19:12:41.816440: step 5590, loss 0.00185524, acc 1
2016-11-12T19:12:41.873165: step 5591, loss 0.0126047, acc 1
2016-11-12T19:12:41.930818: step 5592, loss 0.00596197, acc 1
2016-11-12T19:12:41.987577: step 5593, loss 0.00470875, acc 1
2016-11-12T19:12:42.045072: step 5594, loss 0.00564573, acc 1
2016-11-12T19:12:42.104646: step 5595, loss 0.00281197, acc 1
2016-11-12T19:12:42.162233: step 5596, loss 0.00506738, acc 1
2016-11-12T19:12:42.221854: step 5597, loss 0.0643895, acc 0.96875
2016-11-12T19:12:42.279713: step 5598, loss 0.0247031, acc 0.984375
2016-11-12T19:12:42.339636: step 5599, loss 0.000306608, acc 1
2016-11-12T19:12:42.395786: step 5600, loss 0.000995487, acc 1

Evaluation:
2016-11-12T19:12:42.465723: step 5600, loss 2.66431, acc 0.56

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-5600

2016-11-12T19:12:42.962791: step 5601, loss 0.0079053, acc 1
2016-11-12T19:12:43.020618: step 5602, loss 0.00294869, acc 1
2016-11-12T19:12:43.076121: step 5603, loss 0.00766764, acc 1
2016-11-12T19:12:43.134557: step 5604, loss 0.00354459, acc 1
2016-11-12T19:12:43.191244: step 5605, loss 0.000649495, acc 1
2016-11-12T19:12:43.249393: step 5606, loss 0.00613841, acc 1
2016-11-12T19:12:43.309148: step 5607, loss 0.0080984, acc 1
2016-11-12T19:12:43.368567: step 5608, loss 0.0194901, acc 0.984375
2016-11-12T19:12:43.407190: step 5609, loss 0.000364086, acc 1
2016-11-12T19:12:43.465066: step 5610, loss 0.0171846, acc 0.984375
2016-11-12T19:12:43.523627: step 5611, loss 0.00129393, acc 1
2016-11-12T19:12:43.580013: step 5612, loss 0.000993939, acc 1
2016-11-12T19:12:43.637815: step 5613, loss 0.00132952, acc 1
2016-11-12T19:12:43.696985: step 5614, loss 0.00191688, acc 1
2016-11-12T19:12:43.753234: step 5615, loss 0.00241921, acc 1
2016-11-12T19:12:43.809548: step 5616, loss 0.00145911, acc 1
2016-11-12T19:12:43.865813: step 5617, loss 0.0175002, acc 0.984375
2016-11-12T19:12:43.923212: step 5618, loss 0.0030526, acc 1
2016-11-12T19:12:43.979865: step 5619, loss 0.0275474, acc 0.984375
2016-11-12T19:12:44.036401: step 5620, loss 0.0819696, acc 0.96875
2016-11-12T19:12:44.094456: step 5621, loss 0.0071332, acc 1
2016-11-12T19:12:44.153209: step 5622, loss 0.000921018, acc 1
2016-11-12T19:12:44.210785: step 5623, loss 0.0061756, acc 1
2016-11-12T19:12:44.269388: step 5624, loss 0.0257474, acc 1
2016-11-12T19:12:44.327129: step 5625, loss 0.00230971, acc 1
2016-11-12T19:12:44.383891: step 5626, loss 0.00216396, acc 1
2016-11-12T19:12:44.441130: step 5627, loss 0.000587338, acc 1
2016-11-12T19:12:44.496284: step 5628, loss 0.00156986, acc 1
2016-11-12T19:12:44.553124: step 5629, loss 0.00091768, acc 1
2016-11-12T19:12:44.609535: step 5630, loss 0.0214537, acc 1
2016-11-12T19:12:44.668131: step 5631, loss 0.0112174, acc 1
2016-11-12T19:12:44.725941: step 5632, loss 0.00385668, acc 1
2016-11-12T19:12:44.782211: step 5633, loss 0.0085, acc 1
2016-11-12T19:12:44.839755: step 5634, loss 0.000223609, acc 1
2016-11-12T19:12:44.895563: step 5635, loss 0.00313725, acc 1
2016-11-12T19:12:44.954145: step 5636, loss 0.159557, acc 0.984375
2016-11-12T19:12:45.013040: step 5637, loss 0.00953475, acc 1
2016-11-12T19:12:45.069687: step 5638, loss 0.000668068, acc 1
2016-11-12T19:12:45.127579: step 5639, loss 0.00358797, acc 1
2016-11-12T19:12:45.185811: step 5640, loss 0.00187901, acc 1
2016-11-12T19:12:45.242122: step 5641, loss 0.0414088, acc 0.96875
2016-11-12T19:12:45.301461: step 5642, loss 0.00804792, acc 1
2016-11-12T19:12:45.361106: step 5643, loss 0.00840762, acc 1
2016-11-12T19:12:45.421350: step 5644, loss 0.00129478, acc 1
2016-11-12T19:12:45.479620: step 5645, loss 0.000485802, acc 1
2016-11-12T19:12:45.537444: step 5646, loss 0.0123018, acc 0.984375
2016-11-12T19:12:45.596231: step 5647, loss 0.0159575, acc 0.984375
2016-11-12T19:12:45.653489: step 5648, loss 0.0108391, acc 1
2016-11-12T19:12:45.712956: step 5649, loss 0.0022085, acc 1
2016-11-12T19:12:45.770769: step 5650, loss 0.00187185, acc 1
2016-11-12T19:12:45.826653: step 5651, loss 0.00833335, acc 1
2016-11-12T19:12:45.885191: step 5652, loss 0.0998622, acc 0.96875
2016-11-12T19:12:45.945760: step 5653, loss 0.000973059, acc 1
2016-11-12T19:12:46.001963: step 5654, loss 0.00411539, acc 1
2016-11-12T19:12:46.060831: step 5655, loss 0.00565497, acc 1
2016-11-12T19:12:46.120927: step 5656, loss 0.0113125, acc 1
2016-11-12T19:12:46.178222: step 5657, loss 0.0327558, acc 0.984375
2016-11-12T19:12:46.235951: step 5658, loss 0.00352423, acc 1
2016-11-12T19:12:46.296809: step 5659, loss 0.0104796, acc 1
2016-11-12T19:12:46.353063: step 5660, loss 0.000284612, acc 1
2016-11-12T19:12:46.412293: step 5661, loss 0.0585999, acc 0.984375
2016-11-12T19:12:46.469188: step 5662, loss 0.00087773, acc 1
2016-11-12T19:12:46.526111: step 5663, loss 0.0047831, acc 1
2016-11-12T19:12:46.585411: step 5664, loss 0.00127441, acc 1
2016-11-12T19:12:46.641821: step 5665, loss 0.0389998, acc 0.96875
2016-11-12T19:12:46.701996: step 5666, loss 0.185018, acc 0.984375
2016-11-12T19:12:46.762871: step 5667, loss 0.00577837, acc 1
2016-11-12T19:12:46.823865: step 5668, loss 0.014601, acc 0.984375
2016-11-12T19:12:46.882107: step 5669, loss 0.00126135, acc 1
2016-11-12T19:12:46.944634: step 5670, loss 0.0178658, acc 1
2016-11-12T19:12:47.002683: step 5671, loss 0.0201178, acc 0.984375
2016-11-12T19:12:47.064658: step 5672, loss 0.0179688, acc 0.984375
2016-11-12T19:12:47.123916: step 5673, loss 0.00738452, acc 1
2016-11-12T19:12:47.182624: step 5674, loss 0.00972323, acc 1
2016-11-12T19:12:47.241490: step 5675, loss 0.00799974, acc 1
2016-11-12T19:12:47.298210: step 5676, loss 0.0192527, acc 1
2016-11-12T19:12:47.357246: step 5677, loss 0.00190362, acc 1
2016-11-12T19:12:47.417382: step 5678, loss 0.00143401, acc 1
2016-11-12T19:12:47.474666: step 5679, loss 0.00376006, acc 1
2016-11-12T19:12:47.516179: step 5680, loss 0.00335272, acc 1
2016-11-12T19:12:47.575273: step 5681, loss 0.0370124, acc 0.96875
2016-11-12T19:12:47.632108: step 5682, loss 0.000527009, acc 1
2016-11-12T19:12:47.688062: step 5683, loss 0.00198504, acc 1
2016-11-12T19:12:47.745082: step 5684, loss 5.74219e-05, acc 1
2016-11-12T19:12:47.801902: step 5685, loss 0.00212599, acc 1
2016-11-12T19:12:47.859230: step 5686, loss 0.0274871, acc 0.984375
2016-11-12T19:12:47.920997: step 5687, loss 0.00814829, acc 1
2016-11-12T19:12:47.978538: step 5688, loss 0.00487854, acc 1
2016-11-12T19:12:48.036444: step 5689, loss 0.00091118, acc 1
2016-11-12T19:12:48.093311: step 5690, loss 0.0139115, acc 0.984375
2016-11-12T19:12:48.154978: step 5691, loss 0.00013833, acc 1
2016-11-12T19:12:48.212172: step 5692, loss 0.000920547, acc 1
2016-11-12T19:12:48.269283: step 5693, loss 0.00308633, acc 1
2016-11-12T19:12:48.327113: step 5694, loss 0.0025824, acc 1
2016-11-12T19:12:48.383562: step 5695, loss 0.000727866, acc 1
2016-11-12T19:12:48.439074: step 5696, loss 0.0642271, acc 0.96875
2016-11-12T19:12:48.495727: step 5697, loss 0.0545083, acc 0.984375
2016-11-12T19:12:48.553478: step 5698, loss 0.0141072, acc 1
2016-11-12T19:12:48.610212: step 5699, loss 0.00276616, acc 1
2016-11-12T19:12:48.669275: step 5700, loss 0.0468341, acc 0.984375

Evaluation:
2016-11-12T19:12:48.741059: step 5700, loss 2.69458, acc 0.564

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-5700

2016-11-12T19:12:49.233456: step 5701, loss 0.00977047, acc 1
2016-11-12T19:12:49.292955: step 5702, loss 0.0143181, acc 0.984375
2016-11-12T19:12:49.350366: step 5703, loss 0.000550956, acc 1
2016-11-12T19:12:49.408245: step 5704, loss 0.0207644, acc 0.984375
2016-11-12T19:12:49.469831: step 5705, loss 0.00300589, acc 1
2016-11-12T19:12:49.529955: step 5706, loss 0.00361895, acc 1
2016-11-12T19:12:49.586888: step 5707, loss 0.0446141, acc 0.984375
2016-11-12T19:12:49.648935: step 5708, loss 0.000777167, acc 1
2016-11-12T19:12:49.705409: step 5709, loss 0.00322292, acc 1
2016-11-12T19:12:49.762004: step 5710, loss 0.00208419, acc 1
2016-11-12T19:12:49.820958: step 5711, loss 0.0023643, acc 1
2016-11-12T19:12:49.877008: step 5712, loss 0.0353156, acc 0.984375
2016-11-12T19:12:49.935303: step 5713, loss 0.000235357, acc 1
2016-11-12T19:12:49.992345: step 5714, loss 0.00029122, acc 1
2016-11-12T19:12:50.048385: step 5715, loss 0.0163988, acc 0.984375
2016-11-12T19:12:50.106008: step 5716, loss 0.0204134, acc 0.984375
2016-11-12T19:12:50.163456: step 5717, loss 0.000681866, acc 1
2016-11-12T19:12:50.220169: step 5718, loss 0.000728388, acc 1
2016-11-12T19:12:50.276084: step 5719, loss 0.00101052, acc 1
2016-11-12T19:12:50.332918: step 5720, loss 0.000668351, acc 1
2016-11-12T19:12:50.391041: step 5721, loss 0.00135541, acc 1
2016-11-12T19:12:50.447923: step 5722, loss 0.000981844, acc 1
2016-11-12T19:12:50.505019: step 5723, loss 0.00155432, acc 1
2016-11-12T19:12:50.562340: step 5724, loss 0.00322334, acc 1
2016-11-12T19:12:50.620842: step 5725, loss 0.0254089, acc 0.984375
2016-11-12T19:12:50.680410: step 5726, loss 0.00293068, acc 1
2016-11-12T19:12:50.737299: step 5727, loss 0.00865353, acc 1
2016-11-12T19:12:50.794986: step 5728, loss 0.0108826, acc 1
2016-11-12T19:12:50.852029: step 5729, loss 0.00380223, acc 1
2016-11-12T19:12:50.910744: step 5730, loss 0.0117919, acc 1
2016-11-12T19:12:50.969225: step 5731, loss 0.00130695, acc 1
2016-11-12T19:12:51.026046: step 5732, loss 0.000658025, acc 1
2016-11-12T19:12:51.081977: step 5733, loss 0.00783032, acc 1
2016-11-12T19:12:51.138626: step 5734, loss 0.00574502, acc 1
2016-11-12T19:12:51.197566: step 5735, loss 0.0252523, acc 0.984375
2016-11-12T19:12:51.256796: step 5736, loss 0.095715, acc 0.96875
2016-11-12T19:12:51.315128: step 5737, loss 0.0330182, acc 0.984375
2016-11-12T19:12:51.373648: step 5738, loss 0.00124536, acc 1
2016-11-12T19:12:51.431364: step 5739, loss 0.00365369, acc 1
2016-11-12T19:12:51.491814: step 5740, loss 0.000704549, acc 1
2016-11-12T19:12:51.548928: step 5741, loss 0.00849774, acc 1
2016-11-12T19:12:51.605960: step 5742, loss 0.00628377, acc 1
2016-11-12T19:12:51.662945: step 5743, loss 0.000203195, acc 1
2016-11-12T19:12:51.720248: step 5744, loss 0.0215531, acc 0.984375
2016-11-12T19:12:51.777142: step 5745, loss 0.00240289, acc 1
2016-11-12T19:12:51.833093: step 5746, loss 0.00944613, acc 1
2016-11-12T19:12:51.892712: step 5747, loss 0.0048204, acc 1
2016-11-12T19:12:51.949560: step 5748, loss 0.0100459, acc 1
2016-11-12T19:12:52.008828: step 5749, loss 0.0319063, acc 0.984375
2016-11-12T19:12:52.066689: step 5750, loss 0.00120268, acc 1
2016-11-12T19:12:52.104442: step 5751, loss 0.0366005, acc 0.95
2016-11-12T19:12:52.165407: step 5752, loss 0.0214366, acc 0.984375
2016-11-12T19:12:52.222655: step 5753, loss 0.00117572, acc 1
2016-11-12T19:12:52.281277: step 5754, loss 0.00123232, acc 1
2016-11-12T19:12:52.337219: step 5755, loss 0.000945745, acc 1
2016-11-12T19:12:52.393147: step 5756, loss 0.0596566, acc 0.984375
2016-11-12T19:12:52.452025: step 5757, loss 0.00136038, acc 1
2016-11-12T19:12:52.508468: step 5758, loss 0.00670685, acc 1
2016-11-12T19:12:52.565007: step 5759, loss 0.00152639, acc 1
2016-11-12T19:12:52.621552: step 5760, loss 0.00177864, acc 1
2016-11-12T19:12:52.679049: step 5761, loss 0.0290561, acc 0.984375
2016-11-12T19:12:52.737232: step 5762, loss 0.000313542, acc 1
2016-11-12T19:12:52.794760: step 5763, loss 0.0020297, acc 1
2016-11-12T19:12:52.852157: step 5764, loss 0.00184347, acc 1
2016-11-12T19:12:52.910039: step 5765, loss 0.000410838, acc 1
2016-11-12T19:12:52.967172: step 5766, loss 0.001413, acc 1
2016-11-12T19:12:53.025170: step 5767, loss 0.000356218, acc 1
2016-11-12T19:12:53.081802: step 5768, loss 0.0149181, acc 0.984375
2016-11-12T19:12:53.139998: step 5769, loss 0.0122014, acc 0.984375
2016-11-12T19:12:53.197331: step 5770, loss 0.00402254, acc 1
2016-11-12T19:12:53.254041: step 5771, loss 0.0073426, acc 1
2016-11-12T19:12:53.311338: step 5772, loss 0.0235779, acc 0.984375
2016-11-12T19:12:53.369654: step 5773, loss 0.00176375, acc 1
2016-11-12T19:12:53.427994: step 5774, loss 0.0172365, acc 1
2016-11-12T19:12:53.489085: step 5775, loss 0.00506583, acc 1
2016-11-12T19:12:53.545099: step 5776, loss 0.000584327, acc 1
2016-11-12T19:12:53.600850: step 5777, loss 0.0103749, acc 1
2016-11-12T19:12:53.658315: step 5778, loss 0.00446662, acc 1
2016-11-12T19:12:53.715108: step 5779, loss 0.000961433, acc 1
2016-11-12T19:12:53.772295: step 5780, loss 0.000578431, acc 1
2016-11-12T19:12:53.830708: step 5781, loss 0.0228231, acc 0.984375
2016-11-12T19:12:53.889941: step 5782, loss 0.0210322, acc 0.984375
2016-11-12T19:12:53.947158: step 5783, loss 0.00904655, acc 1
2016-11-12T19:12:54.005048: step 5784, loss 0.0431676, acc 0.984375
2016-11-12T19:12:54.065388: step 5785, loss 0.00313643, acc 1
2016-11-12T19:12:54.125288: step 5786, loss 0.00284169, acc 1
2016-11-12T19:12:54.181757: step 5787, loss 0.00350408, acc 1
2016-11-12T19:12:54.238066: step 5788, loss 0.0071587, acc 1
2016-11-12T19:12:54.294967: step 5789, loss 0.000247906, acc 1
2016-11-12T19:12:54.351573: step 5790, loss 0.0163044, acc 0.984375
2016-11-12T19:12:54.409409: step 5791, loss 0.012818, acc 0.984375
2016-11-12T19:12:54.467624: step 5792, loss 0.00332336, acc 1
2016-11-12T19:12:54.525414: step 5793, loss 0.00321172, acc 1
2016-11-12T19:12:54.583814: step 5794, loss 0.000588883, acc 1
2016-11-12T19:12:54.644579: step 5795, loss 0.00879943, acc 1
2016-11-12T19:12:54.704936: step 5796, loss 0.00225683, acc 1
2016-11-12T19:12:54.762253: step 5797, loss 0.01808, acc 1
2016-11-12T19:12:54.820014: step 5798, loss 0.0288133, acc 0.984375
2016-11-12T19:12:54.876974: step 5799, loss 0.00699648, acc 1
2016-11-12T19:12:54.936024: step 5800, loss 0.0455609, acc 0.984375

Evaluation:
2016-11-12T19:12:55.006861: step 5800, loss 2.72558, acc 0.564

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-5800

2016-11-12T19:12:55.502048: step 5801, loss 0.00084611, acc 1
2016-11-12T19:12:55.561261: step 5802, loss 0.001915, acc 1
2016-11-12T19:12:55.619725: step 5803, loss 0.000970934, acc 1
2016-11-12T19:12:55.677766: step 5804, loss 0.0465666, acc 0.96875
2016-11-12T19:12:55.737276: step 5805, loss 0.00846861, acc 1
2016-11-12T19:12:55.796546: step 5806, loss 0.00840528, acc 1
2016-11-12T19:12:55.854485: step 5807, loss 0.0092567, acc 1
2016-11-12T19:12:55.911941: step 5808, loss 0.0609666, acc 0.96875
2016-11-12T19:12:55.969196: step 5809, loss 0.0081591, acc 1
2016-11-12T19:12:56.026257: step 5810, loss 0.0264118, acc 0.984375
2016-11-12T19:12:56.084844: step 5811, loss 0.0215644, acc 0.984375
2016-11-12T19:12:56.144724: step 5812, loss 0.166744, acc 0.984375
2016-11-12T19:12:56.206779: step 5813, loss 0.00338426, acc 1
2016-11-12T19:12:56.263478: step 5814, loss 0.00383658, acc 1
2016-11-12T19:12:56.320667: step 5815, loss 0.000689697, acc 1
2016-11-12T19:12:56.378037: step 5816, loss 0.00438847, acc 1
2016-11-12T19:12:56.434710: step 5817, loss 0.00428675, acc 1
2016-11-12T19:12:56.492004: step 5818, loss 0.00140155, acc 1
2016-11-12T19:12:56.549403: step 5819, loss 0.0016626, acc 1
2016-11-12T19:12:56.610014: step 5820, loss 0.0143422, acc 0.984375
2016-11-12T19:12:56.673271: step 5821, loss 0.00224142, acc 1
2016-11-12T19:12:56.710240: step 5822, loss 0.000342093, acc 1
2016-11-12T19:12:56.769163: step 5823, loss 0.0689567, acc 0.984375
2016-11-12T19:12:56.827327: step 5824, loss 0.00930359, acc 1
2016-11-12T19:12:56.885580: step 5825, loss 0.00150802, acc 1
2016-11-12T19:12:56.942333: step 5826, loss 0.000542134, acc 1
2016-11-12T19:12:56.999916: step 5827, loss 0.0425271, acc 0.96875
2016-11-12T19:12:57.058408: step 5828, loss 0.0342129, acc 0.984375
2016-11-12T19:12:57.117055: step 5829, loss 0.000623946, acc 1
2016-11-12T19:12:57.173666: step 5830, loss 0.00752136, acc 1
2016-11-12T19:12:57.230570: step 5831, loss 0.00110492, acc 1
2016-11-12T19:12:57.291241: step 5832, loss 0.00153369, acc 1
2016-11-12T19:12:57.348352: step 5833, loss 0.00073807, acc 1
2016-11-12T19:12:57.404258: step 5834, loss 0.0028214, acc 1
2016-11-12T19:12:57.463855: step 5835, loss 0.00110167, acc 1
2016-11-12T19:12:57.520461: step 5836, loss 0.0158183, acc 0.984375
2016-11-12T19:12:57.580248: step 5837, loss 0.000420409, acc 1
2016-11-12T19:12:57.636309: step 5838, loss 0.00156617, acc 1
2016-11-12T19:12:57.692293: step 5839, loss 0.00100503, acc 1
2016-11-12T19:12:57.751222: step 5840, loss 0.000919968, acc 1
2016-11-12T19:12:57.809441: step 5841, loss 0.0234355, acc 0.984375
2016-11-12T19:12:57.869477: step 5842, loss 0.0815073, acc 0.96875
2016-11-12T19:12:57.928166: step 5843, loss 0.00199696, acc 1
2016-11-12T19:12:57.987764: step 5844, loss 0.000947988, acc 1
2016-11-12T19:12:58.046725: step 5845, loss 0.013271, acc 0.984375
2016-11-12T19:12:58.104622: step 5846, loss 0.0186536, acc 1
2016-11-12T19:12:58.165172: step 5847, loss 0.00767669, acc 1
2016-11-12T19:12:58.222469: step 5848, loss 0.00604499, acc 1
2016-11-12T19:12:58.281605: step 5849, loss 0.00413049, acc 1
2016-11-12T19:12:58.341709: step 5850, loss 0.00746289, acc 1
2016-11-12T19:12:58.400674: step 5851, loss 0.000507006, acc 1
2016-11-12T19:12:58.457349: step 5852, loss 0.00381014, acc 1
2016-11-12T19:12:58.516731: step 5853, loss 0.000789905, acc 1
2016-11-12T19:12:58.576422: step 5854, loss 0.0230673, acc 0.984375
2016-11-12T19:12:58.634314: step 5855, loss 0.0060329, acc 1
2016-11-12T19:12:58.691795: step 5856, loss 0.00155458, acc 1
2016-11-12T19:12:58.749166: step 5857, loss 0.0131456, acc 0.984375
2016-11-12T19:12:58.808550: step 5858, loss 0.00110256, acc 1
2016-11-12T19:12:58.865160: step 5859, loss 0.011361, acc 1
2016-11-12T19:12:58.921753: step 5860, loss 0.00270843, acc 1
2016-11-12T19:12:58.981371: step 5861, loss 0.000868771, acc 1
2016-11-12T19:12:59.037085: step 5862, loss 0.00271228, acc 1
2016-11-12T19:12:59.095486: step 5863, loss 0.00344426, acc 1
2016-11-12T19:12:59.152151: step 5864, loss 0.0056093, acc 1
2016-11-12T19:12:59.208687: step 5865, loss 0.00031963, acc 1
2016-11-12T19:12:59.265451: step 5866, loss 0.000641729, acc 1
2016-11-12T19:12:59.322499: step 5867, loss 0.0115196, acc 1
2016-11-12T19:12:59.381544: step 5868, loss 0.000439511, acc 1
2016-11-12T19:12:59.440045: step 5869, loss 0.00133314, acc 1
2016-11-12T19:12:59.497054: step 5870, loss 0.0192235, acc 0.984375
2016-11-12T19:12:59.557520: step 5871, loss 0.00160281, acc 1
2016-11-12T19:12:59.613986: step 5872, loss 0.0157098, acc 0.984375
2016-11-12T19:12:59.673376: step 5873, loss 0.00500671, acc 1
2016-11-12T19:12:59.730266: step 5874, loss 0.0329613, acc 0.984375
2016-11-12T19:12:59.789024: step 5875, loss 0.038502, acc 0.96875
2016-11-12T19:12:59.846865: step 5876, loss 0.000741845, acc 1
2016-11-12T19:12:59.905118: step 5877, loss 0.0207452, acc 0.984375
2016-11-12T19:12:59.964313: step 5878, loss 0.00085173, acc 1
2016-11-12T19:13:00.020764: step 5879, loss 0.0156035, acc 0.984375
2016-11-12T19:13:00.079365: step 5880, loss 0.000493538, acc 1
2016-11-12T19:13:00.134856: step 5881, loss 0.0301624, acc 0.984375
2016-11-12T19:13:00.192311: step 5882, loss 0.00061755, acc 1
2016-11-12T19:13:00.250183: step 5883, loss 0.0185644, acc 0.984375
2016-11-12T19:13:00.309128: step 5884, loss 0.0127715, acc 0.984375
2016-11-12T19:13:00.366982: step 5885, loss 0.0485066, acc 0.984375
2016-11-12T19:13:00.424998: step 5886, loss 0.160566, acc 0.984375
2016-11-12T19:13:00.484731: step 5887, loss 0.0050978, acc 1
2016-11-12T19:13:00.541493: step 5888, loss 0.0036059, acc 1
2016-11-12T19:13:00.599111: step 5889, loss 0.0145003, acc 1
2016-11-12T19:13:00.655937: step 5890, loss 0.00315991, acc 1
2016-11-12T19:13:00.713351: step 5891, loss 0.00113378, acc 1
2016-11-12T19:13:00.770716: step 5892, loss 0.0169847, acc 0.984375
2016-11-12T19:13:00.808322: step 5893, loss 0.0036025, acc 1
2016-11-12T19:13:00.866882: step 5894, loss 0.00276156, acc 1
2016-11-12T19:13:00.924633: step 5895, loss 0.000270829, acc 1
2016-11-12T19:13:00.982614: step 5896, loss 0.000924064, acc 1
2016-11-12T19:13:01.040125: step 5897, loss 0.0015079, acc 1
2016-11-12T19:13:01.097373: step 5898, loss 0.0290966, acc 0.984375
2016-11-12T19:13:01.158196: step 5899, loss 0.0181041, acc 1
2016-11-12T19:13:01.215962: step 5900, loss 0.0212025, acc 1

Evaluation:
2016-11-12T19:13:01.287947: step 5900, loss 2.73831, acc 0.58

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-5900

2016-11-12T19:13:01.781260: step 5901, loss 0.010273, acc 1
2016-11-12T19:13:01.839222: step 5902, loss 0.0171281, acc 0.984375
2016-11-12T19:13:01.897246: step 5903, loss 0.000679186, acc 1
2016-11-12T19:13:01.955286: step 5904, loss 0.0120089, acc 1
2016-11-12T19:13:02.013903: step 5905, loss 0.00371218, acc 1
2016-11-12T19:13:02.070897: step 5906, loss 0.000355891, acc 1
2016-11-12T19:13:02.126595: step 5907, loss 0.00456829, acc 1
2016-11-12T19:13:02.185658: step 5908, loss 0.00606315, acc 1
2016-11-12T19:13:02.242162: step 5909, loss 0.0145607, acc 0.984375
2016-11-12T19:13:02.300983: step 5910, loss 0.00590342, acc 1
2016-11-12T19:13:02.358318: step 5911, loss 0.0151464, acc 0.984375
2016-11-12T19:13:02.417935: step 5912, loss 0.000949477, acc 1
2016-11-12T19:13:02.474318: step 5913, loss 0.00200828, acc 1
2016-11-12T19:13:02.533118: step 5914, loss 0.0430616, acc 0.984375
2016-11-12T19:13:02.590392: step 5915, loss 0.000969635, acc 1
2016-11-12T19:13:02.649316: step 5916, loss 0.0141986, acc 1
2016-11-12T19:13:02.705461: step 5917, loss 0.0332413, acc 0.96875
2016-11-12T19:13:02.764955: step 5918, loss 0.0375549, acc 0.96875
2016-11-12T19:13:02.825163: step 5919, loss 0.00588341, acc 1
2016-11-12T19:13:02.883105: step 5920, loss 0.0393222, acc 0.96875
2016-11-12T19:13:02.939715: step 5921, loss 0.00118954, acc 1
2016-11-12T19:13:02.997256: step 5922, loss 0.00350289, acc 1
2016-11-12T19:13:03.055352: step 5923, loss 0.0301592, acc 0.984375
2016-11-12T19:13:03.113047: step 5924, loss 0.0321653, acc 0.984375
2016-11-12T19:13:03.172299: step 5925, loss 0.00023897, acc 1
2016-11-12T19:13:03.230946: step 5926, loss 0.0102643, acc 1
2016-11-12T19:13:03.288862: step 5927, loss 0.0165519, acc 0.984375
2016-11-12T19:13:03.347505: step 5928, loss 0.0014685, acc 1
2016-11-12T19:13:03.405667: step 5929, loss 0.000547237, acc 1
2016-11-12T19:13:03.462081: step 5930, loss 0.000758338, acc 1
2016-11-12T19:13:03.518696: step 5931, loss 0.0378639, acc 0.984375
2016-11-12T19:13:03.576120: step 5932, loss 0.000880674, acc 1
2016-11-12T19:13:03.632947: step 5933, loss 0.0421819, acc 0.96875
2016-11-12T19:13:03.690563: step 5934, loss 0.00261247, acc 1
2016-11-12T19:13:03.748117: step 5935, loss 0.00285344, acc 1
2016-11-12T19:13:03.805431: step 5936, loss 0.00419074, acc 1
2016-11-12T19:13:03.869564: step 5937, loss 0.0100165, acc 1
2016-11-12T19:13:03.927526: step 5938, loss 0.0767125, acc 0.96875
2016-11-12T19:13:03.985413: step 5939, loss 0.00107475, acc 1
2016-11-12T19:13:04.043216: step 5940, loss 0.00162445, acc 1
2016-11-12T19:13:04.099629: step 5941, loss 0.000625873, acc 1
2016-11-12T19:13:04.155955: step 5942, loss 0.00265786, acc 1
2016-11-12T19:13:04.213214: step 5943, loss 0.00522018, acc 1
2016-11-12T19:13:04.270868: step 5944, loss 0.0395446, acc 0.984375
2016-11-12T19:13:04.329407: step 5945, loss 0.00341363, acc 1
2016-11-12T19:13:04.386627: step 5946, loss 0.0388259, acc 0.984375
2016-11-12T19:13:04.445255: step 5947, loss 0.0962398, acc 0.984375
2016-11-12T19:13:04.503113: step 5948, loss 0.0589828, acc 0.96875
2016-11-12T19:13:04.561462: step 5949, loss 0.00248753, acc 1
2016-11-12T19:13:04.620899: step 5950, loss 0.00203639, acc 1
2016-11-12T19:13:04.678846: step 5951, loss 0.000459546, acc 1
2016-11-12T19:13:04.737373: step 5952, loss 0.00100989, acc 1
2016-11-12T19:13:04.793478: step 5953, loss 0.00076186, acc 1
2016-11-12T19:13:04.850017: step 5954, loss 0.00642599, acc 1
2016-11-12T19:13:04.907956: step 5955, loss 0.00450623, acc 1
2016-11-12T19:13:04.965897: step 5956, loss 0.00589627, acc 1
2016-11-12T19:13:05.021827: step 5957, loss 0.00170827, acc 1
2016-11-12T19:13:05.081147: step 5958, loss 0.00240555, acc 1
2016-11-12T19:13:05.138588: step 5959, loss 0.00336459, acc 1
2016-11-12T19:13:05.196342: step 5960, loss 0.0013012, acc 1
2016-11-12T19:13:05.253999: step 5961, loss 0.00140485, acc 1
2016-11-12T19:13:05.312623: step 5962, loss 0.00850941, acc 1
2016-11-12T19:13:05.372322: step 5963, loss 0.0131089, acc 0.984375
2016-11-12T19:13:05.412780: step 5964, loss 0.000189256, acc 1
2016-11-12T19:13:05.471098: step 5965, loss 0.00100215, acc 1
2016-11-12T19:13:05.528359: step 5966, loss 0.0381461, acc 0.984375
2016-11-12T19:13:05.586546: step 5967, loss 0.00173247, acc 1
2016-11-12T19:13:05.645455: step 5968, loss 0.00410836, acc 1
2016-11-12T19:13:05.702438: step 5969, loss 0.00148025, acc 1
2016-11-12T19:13:05.760549: step 5970, loss 0.00260802, acc 1
2016-11-12T19:13:05.819128: step 5971, loss 0.0139909, acc 1
2016-11-12T19:13:05.876813: step 5972, loss 0.000840288, acc 1
2016-11-12T19:13:05.937364: step 5973, loss 0.000159633, acc 1
2016-11-12T19:13:05.993303: step 5974, loss 0.0338493, acc 0.984375
2016-11-12T19:13:06.050430: step 5975, loss 0.0210037, acc 0.984375
2016-11-12T19:13:06.109794: step 5976, loss 0.0266304, acc 0.984375
2016-11-12T19:13:06.168867: step 5977, loss 0.0061567, acc 1
2016-11-12T19:13:06.227417: step 5978, loss 0.00942433, acc 1
2016-11-12T19:13:06.285161: step 5979, loss 0.00886271, acc 1
2016-11-12T19:13:06.343007: step 5980, loss 0.0683281, acc 0.96875
2016-11-12T19:13:06.400330: step 5981, loss 0.00636601, acc 1
2016-11-12T19:13:06.457967: step 5982, loss 0.0806118, acc 0.984375
2016-11-12T19:13:06.517147: step 5983, loss 0.00712007, acc 1
2016-11-12T19:13:06.574841: step 5984, loss 0.013765, acc 1
2016-11-12T19:13:06.634270: step 5985, loss 0.00423737, acc 1
2016-11-12T19:13:06.692904: step 5986, loss 0.00772851, acc 1
2016-11-12T19:13:06.749827: step 5987, loss 0.00387184, acc 1
2016-11-12T19:13:06.806780: step 5988, loss 0.00128854, acc 1
2016-11-12T19:13:06.864010: step 5989, loss 0.0316065, acc 0.984375
2016-11-12T19:13:06.921318: step 5990, loss 0.00197561, acc 1
2016-11-12T19:13:06.981207: step 5991, loss 0.00488301, acc 1
2016-11-12T19:13:07.038839: step 5992, loss 0.0049066, acc 1
2016-11-12T19:13:07.097146: step 5993, loss 0.00154446, acc 1
2016-11-12T19:13:07.156570: step 5994, loss 0.000803598, acc 1
2016-11-12T19:13:07.213363: step 5995, loss 0.00800315, acc 1
2016-11-12T19:13:07.272471: step 5996, loss 0.0128087, acc 0.984375
2016-11-12T19:13:07.329395: step 5997, loss 0.036204, acc 0.984375
2016-11-12T19:13:07.390729: step 5998, loss 0.00076422, acc 1
2016-11-12T19:13:07.447201: step 5999, loss 0.000442467, acc 1
2016-11-12T19:13:07.504328: step 6000, loss 0.000554913, acc 1

Evaluation:
2016-11-12T19:13:07.574749: step 6000, loss 2.77657, acc 0.576

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-6000

2016-11-12T19:13:08.071362: step 6001, loss 0.0163106, acc 1
2016-11-12T19:13:08.142708: step 6002, loss 0.00124089, acc 1
2016-11-12T19:13:08.199186: step 6003, loss 0.0444379, acc 0.984375
2016-11-12T19:13:08.256652: step 6004, loss 0.0820033, acc 0.984375
2016-11-12T19:13:08.313479: step 6005, loss 0.000419734, acc 1
2016-11-12T19:13:08.369338: step 6006, loss 0.0035996, acc 1
2016-11-12T19:13:08.427913: step 6007, loss 5.85657e-05, acc 1
2016-11-12T19:13:08.484321: step 6008, loss 0.013221, acc 0.984375
2016-11-12T19:13:08.541333: step 6009, loss 0.014294, acc 0.984375
2016-11-12T19:13:08.601135: step 6010, loss 0.0106206, acc 1
2016-11-12T19:13:08.658960: step 6011, loss 0.00933485, acc 1
2016-11-12T19:13:08.716444: step 6012, loss 0.0138853, acc 0.984375
2016-11-12T19:13:08.775872: step 6013, loss 0.0130569, acc 1
2016-11-12T19:13:08.834440: step 6014, loss 0.000332228, acc 1
2016-11-12T19:13:08.891175: step 6015, loss 0.0188848, acc 0.984375
2016-11-12T19:13:08.949040: step 6016, loss 0.000630632, acc 1
2016-11-12T19:13:09.008390: step 6017, loss 0.385936, acc 0.984375
2016-11-12T19:13:09.066166: step 6018, loss 0.00652113, acc 1
2016-11-12T19:13:09.125104: step 6019, loss 0.000401181, acc 1
2016-11-12T19:13:09.181902: step 6020, loss 0.00064556, acc 1
2016-11-12T19:13:09.239194: step 6021, loss 0.0316872, acc 0.984375
2016-11-12T19:13:09.296895: step 6022, loss 0.000248886, acc 1
2016-11-12T19:13:09.351862: step 6023, loss 0.00173712, acc 1
2016-11-12T19:13:09.409308: step 6024, loss 0.0047479, acc 1
2016-11-12T19:13:09.468467: step 6025, loss 0.00164373, acc 1
2016-11-12T19:13:09.525547: step 6026, loss 0.0698471, acc 0.984375
2016-11-12T19:13:09.584743: step 6027, loss 0.00566588, acc 1
2016-11-12T19:13:09.643252: step 6028, loss 0.00262577, acc 1
2016-11-12T19:13:09.699252: step 6029, loss 0.0248945, acc 0.984375
2016-11-12T19:13:09.756742: step 6030, loss 0.00714411, acc 1
2016-11-12T19:13:09.812534: step 6031, loss 0.0339256, acc 0.984375
2016-11-12T19:13:09.873648: step 6032, loss 0.00556479, acc 1
2016-11-12T19:13:09.931218: step 6033, loss 0.012631, acc 1
2016-11-12T19:13:09.988508: step 6034, loss 0.0702964, acc 0.984375
2016-11-12T19:13:10.028858: step 6035, loss 0.00291312, acc 1
2016-11-12T19:13:10.089433: step 6036, loss 0.000422891, acc 1
2016-11-12T19:13:10.145611: step 6037, loss 0.0895096, acc 0.96875
2016-11-12T19:13:10.205425: step 6038, loss 7.61669e-05, acc 1
2016-11-12T19:13:10.264738: step 6039, loss 0.00179875, acc 1
2016-11-12T19:13:10.325926: step 6040, loss 0.0101489, acc 1
2016-11-12T19:13:10.384138: step 6041, loss 0.00357434, acc 1
2016-11-12T19:13:10.441046: step 6042, loss 0.00241742, acc 1
2016-11-12T19:13:10.499905: step 6043, loss 0.0149426, acc 0.984375
2016-11-12T19:13:10.561058: step 6044, loss 0.0204455, acc 0.984375
2016-11-12T19:13:10.621400: step 6045, loss 0.00205926, acc 1
2016-11-12T19:13:10.679917: step 6046, loss 0.00225388, acc 1
2016-11-12T19:13:10.740273: step 6047, loss 0.0239696, acc 0.96875
2016-11-12T19:13:10.797573: step 6048, loss 0.0027567, acc 1
2016-11-12T19:13:10.854086: step 6049, loss 0.0065358, acc 1
2016-11-12T19:13:10.911944: step 6050, loss 0.00160146, acc 1
2016-11-12T19:13:10.969287: step 6051, loss 0.00303278, acc 1
2016-11-12T19:13:11.029528: step 6052, loss 0.00206391, acc 1
2016-11-12T19:13:11.087572: step 6053, loss 0.000714088, acc 1
2016-11-12T19:13:11.144633: step 6054, loss 0.00045858, acc 1
2016-11-12T19:13:11.201147: step 6055, loss 0.00124176, acc 1
2016-11-12T19:13:11.258361: step 6056, loss 0.0094898, acc 1
2016-11-12T19:13:11.317862: step 6057, loss 0.00161001, acc 1
2016-11-12T19:13:11.375067: step 6058, loss 0.0018837, acc 1
2016-11-12T19:13:11.431796: step 6059, loss 0.000660641, acc 1
2016-11-12T19:13:11.489186: step 6060, loss 0.0211745, acc 0.984375
2016-11-12T19:13:11.548003: step 6061, loss 0.0239638, acc 0.984375
2016-11-12T19:13:11.606428: step 6062, loss 0.0526268, acc 0.96875
2016-11-12T19:13:11.666916: step 6063, loss 0.0262196, acc 0.984375
2016-11-12T19:13:11.725181: step 6064, loss 0.000983137, acc 1
2016-11-12T19:13:11.781390: step 6065, loss 0.0020096, acc 1
2016-11-12T19:13:11.840421: step 6066, loss 0.00989289, acc 1
2016-11-12T19:13:11.898111: step 6067, loss 0.0226489, acc 0.984375
2016-11-12T19:13:11.955399: step 6068, loss 0.0187195, acc 0.984375
2016-11-12T19:13:12.013378: step 6069, loss 0.00233365, acc 1
2016-11-12T19:13:12.070392: step 6070, loss 0.00166893, acc 1
2016-11-12T19:13:12.128857: step 6071, loss 0.000560878, acc 1
2016-11-12T19:13:12.187149: step 6072, loss 0.000661725, acc 1
2016-11-12T19:13:12.244066: step 6073, loss 0.00243877, acc 1
2016-11-12T19:13:12.301993: step 6074, loss 0.0685655, acc 0.96875
2016-11-12T19:13:12.360850: step 6075, loss 0.000713807, acc 1
2016-11-12T19:13:12.417494: step 6076, loss 0.0186046, acc 0.984375
2016-11-12T19:13:12.474390: step 6077, loss 0.0403643, acc 0.984375
2016-11-12T19:13:12.533810: step 6078, loss 0.00280803, acc 1
2016-11-12T19:13:12.590061: step 6079, loss 0.00100866, acc 1
2016-11-12T19:13:12.645215: step 6080, loss 0.000448992, acc 1
2016-11-12T19:13:12.701305: step 6081, loss 0.010951, acc 1
2016-11-12T19:13:12.760782: step 6082, loss 0.00315566, acc 1
2016-11-12T19:13:12.817487: step 6083, loss 0.0227335, acc 0.984375
2016-11-12T19:13:12.874346: step 6084, loss 0.0102503, acc 1
2016-11-12T19:13:12.931253: step 6085, loss 0.0012335, acc 1
2016-11-12T19:13:12.986969: step 6086, loss 0.00584135, acc 1
2016-11-12T19:13:13.043695: step 6087, loss 0.00294426, acc 1
2016-11-12T19:13:13.100415: step 6088, loss 0.00347456, acc 1
2016-11-12T19:13:13.157333: step 6089, loss 0.0181456, acc 0.984375
2016-11-12T19:13:13.216993: step 6090, loss 0.034311, acc 0.984375
2016-11-12T19:13:13.274265: step 6091, loss 0.00579291, acc 1
2016-11-12T19:13:13.332539: step 6092, loss 0.0221773, acc 0.984375
2016-11-12T19:13:13.389405: step 6093, loss 0.0227657, acc 0.984375
2016-11-12T19:13:13.446888: step 6094, loss 0.0168087, acc 0.984375
2016-11-12T19:13:13.503842: step 6095, loss 0.00280843, acc 1
2016-11-12T19:13:13.561249: step 6096, loss 0.0014076, acc 1
2016-11-12T19:13:13.618396: step 6097, loss 0.000679643, acc 1
2016-11-12T19:13:13.675209: step 6098, loss 0.0060103, acc 1
2016-11-12T19:13:13.731711: step 6099, loss 0.149077, acc 0.984375
2016-11-12T19:13:13.789358: step 6100, loss 0.00285626, acc 1

Evaluation:
2016-11-12T19:13:13.865050: step 6100, loss 2.78831, acc 0.568

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-6100

2016-11-12T19:13:14.357546: step 6101, loss 0.0147162, acc 0.984375
2016-11-12T19:13:14.416240: step 6102, loss 0.0289134, acc 0.984375
2016-11-12T19:13:14.474233: step 6103, loss 0.196353, acc 0.96875
2016-11-12T19:13:14.531766: step 6104, loss 0.0267151, acc 0.984375
2016-11-12T19:13:14.588196: step 6105, loss 0.0232186, acc 0.984375
2016-11-12T19:13:14.628211: step 6106, loss 0.000903525, acc 1
2016-11-12T19:13:14.686823: step 6107, loss 0.0258632, acc 0.984375
2016-11-12T19:13:14.743912: step 6108, loss 0.000796674, acc 1
2016-11-12T19:13:14.800516: step 6109, loss 0.0589079, acc 0.984375
2016-11-12T19:13:14.858317: step 6110, loss 0.000431735, acc 1
2016-11-12T19:13:14.916822: step 6111, loss 0.00448825, acc 1
2016-11-12T19:13:14.974861: step 6112, loss 0.00468635, acc 1
2016-11-12T19:13:15.031979: step 6113, loss 0.0105039, acc 1
2016-11-12T19:13:15.090116: step 6114, loss 0.0438181, acc 0.984375
2016-11-12T19:13:15.148270: step 6115, loss 0.00356407, acc 1
2016-11-12T19:13:15.209266: step 6116, loss 0.0109241, acc 1
2016-11-12T19:13:15.265538: step 6117, loss 0.0027019, acc 1
2016-11-12T19:13:15.322787: step 6118, loss 0.0114206, acc 1
2016-11-12T19:13:15.379629: step 6119, loss 0.0391169, acc 0.984375
2016-11-12T19:13:15.437889: step 6120, loss 0.0397836, acc 0.96875
2016-11-12T19:13:15.496444: step 6121, loss 0.012805, acc 0.984375
2016-11-12T19:13:15.553886: step 6122, loss 0.00377081, acc 1
2016-11-12T19:13:15.610705: step 6123, loss 0.000358494, acc 1
2016-11-12T19:13:15.669077: step 6124, loss 0.00205613, acc 1
2016-11-12T19:13:15.725183: step 6125, loss 0.0162225, acc 0.984375
2016-11-12T19:13:15.782091: step 6126, loss 0.0103854, acc 1
2016-11-12T19:13:15.841314: step 6127, loss 0.0119486, acc 1
2016-11-12T19:13:15.899226: step 6128, loss 0.000755743, acc 1
2016-11-12T19:13:15.955470: step 6129, loss 0.0518117, acc 0.984375
2016-11-12T19:13:16.013603: step 6130, loss 0.00155838, acc 1
2016-11-12T19:13:16.071194: step 6131, loss 0.00304485, acc 1
2016-11-12T19:13:16.130265: step 6132, loss 0.00104187, acc 1
2016-11-12T19:13:16.190434: step 6133, loss 0.0030709, acc 1
2016-11-12T19:13:16.246613: step 6134, loss 0.0224881, acc 0.984375
2016-11-12T19:13:16.303512: step 6135, loss 0.0385169, acc 0.984375
2016-11-12T19:13:16.360192: step 6136, loss 0.0063039, acc 1
2016-11-12T19:13:16.417659: step 6137, loss 0.00463833, acc 1
2016-11-12T19:13:16.474371: step 6138, loss 0.0100097, acc 1
2016-11-12T19:13:16.535574: step 6139, loss 0.00787882, acc 1
2016-11-12T19:13:16.593998: step 6140, loss 0.00511424, acc 1
2016-11-12T19:13:16.651064: step 6141, loss 0.0142198, acc 0.984375
2016-11-12T19:13:16.708823: step 6142, loss 0.00416663, acc 1
2016-11-12T19:13:16.766533: step 6143, loss 0.00959817, acc 1
2016-11-12T19:13:16.822992: step 6144, loss 0.000668678, acc 1
2016-11-12T19:13:16.880314: step 6145, loss 0.0017942, acc 1
2016-11-12T19:13:16.936814: step 6146, loss 0.00124219, acc 1
2016-11-12T19:13:16.996920: step 6147, loss 0.00228663, acc 1
2016-11-12T19:13:17.053863: step 6148, loss 0.000675807, acc 1
2016-11-12T19:13:17.112819: step 6149, loss 0.00531022, acc 1
2016-11-12T19:13:17.169644: step 6150, loss 0.00627295, acc 1
2016-11-12T19:13:17.226010: step 6151, loss 0.00032135, acc 1
2016-11-12T19:13:17.282379: step 6152, loss 0.00681654, acc 1
2016-11-12T19:13:17.340784: step 6153, loss 0.00196487, acc 1
2016-11-12T19:13:17.401804: step 6154, loss 0.00226126, acc 1
2016-11-12T19:13:17.460778: step 6155, loss 0.0534961, acc 0.984375
2016-11-12T19:13:17.518108: step 6156, loss 0.000693951, acc 1
2016-11-12T19:13:17.574215: step 6157, loss 0.0346497, acc 0.984375
2016-11-12T19:13:17.630572: step 6158, loss 0.0068942, acc 1
2016-11-12T19:13:17.688901: step 6159, loss 0.0150248, acc 1
2016-11-12T19:13:17.745784: step 6160, loss 0.000274099, acc 1
2016-11-12T19:13:17.805163: step 6161, loss 0.0066352, acc 1
2016-11-12T19:13:17.865232: step 6162, loss 0.0109799, acc 1
2016-11-12T19:13:17.922758: step 6163, loss 0.000896078, acc 1
2016-11-12T19:13:17.979129: step 6164, loss 0.00131717, acc 1
2016-11-12T19:13:18.037645: step 6165, loss 0.000857, acc 1
2016-11-12T19:13:18.097498: step 6166, loss 0.000781884, acc 1
2016-11-12T19:13:18.153808: step 6167, loss 0.00133244, acc 1
2016-11-12T19:13:18.209982: step 6168, loss 0.263589, acc 0.984375
2016-11-12T19:13:18.269480: step 6169, loss 0.033485, acc 0.96875
2016-11-12T19:13:18.330146: step 6170, loss 0.00155155, acc 1
2016-11-12T19:13:18.387230: step 6171, loss 0.0129593, acc 0.984375
2016-11-12T19:13:18.445128: step 6172, loss 0.00824506, acc 1
2016-11-12T19:13:18.501859: step 6173, loss 0.0015108, acc 1
2016-11-12T19:13:18.561375: step 6174, loss 0.0114752, acc 1
2016-11-12T19:13:18.622095: step 6175, loss 0.0198341, acc 0.984375
2016-11-12T19:13:18.679225: step 6176, loss 0.000693027, acc 1
2016-11-12T19:13:18.716473: step 6177, loss 0.0130394, acc 1
2016-11-12T19:13:18.775812: step 6178, loss 0.00012116, acc 1
2016-11-12T19:13:18.832667: step 6179, loss 0.0129422, acc 1
2016-11-12T19:13:18.889975: step 6180, loss 0.00781893, acc 1
2016-11-12T19:13:18.947109: step 6181, loss 0.000344046, acc 1
2016-11-12T19:13:19.004648: step 6182, loss 0.00146237, acc 1
2016-11-12T19:13:19.061248: step 6183, loss 0.00044033, acc 1
2016-11-12T19:13:19.117192: step 6184, loss 0.0107721, acc 1
2016-11-12T19:13:19.173857: step 6185, loss 0.0441572, acc 0.984375
2016-11-12T19:13:19.232474: step 6186, loss 0.000976941, acc 1
2016-11-12T19:13:19.289436: step 6187, loss 0.00482672, acc 1
2016-11-12T19:13:19.349005: step 6188, loss 0.00167835, acc 1
2016-11-12T19:13:19.405956: step 6189, loss 0.0113496, acc 1
2016-11-12T19:13:19.465054: step 6190, loss 0.00193132, acc 1
2016-11-12T19:13:19.525256: step 6191, loss 0.00263917, acc 1
2016-11-12T19:13:19.584698: step 6192, loss 0.000306792, acc 1
2016-11-12T19:13:19.644166: step 6193, loss 0.0981054, acc 0.96875
2016-11-12T19:13:19.703590: step 6194, loss 0.00277294, acc 1
2016-11-12T19:13:19.762084: step 6195, loss 0.014563, acc 0.984375
2016-11-12T19:13:19.821269: step 6196, loss 0.00231577, acc 1
2016-11-12T19:13:19.881398: step 6197, loss 0.00288742, acc 1
2016-11-12T19:13:19.938064: step 6198, loss 0.0013055, acc 1
2016-11-12T19:13:19.994548: step 6199, loss 0.000825632, acc 1
2016-11-12T19:13:20.051754: step 6200, loss 0.0272144, acc 0.984375

Evaluation:
2016-11-12T19:13:20.122525: step 6200, loss 2.88751, acc 0.552

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-6200

2016-11-12T19:13:20.615740: step 6201, loss 0.00487217, acc 1
2016-11-12T19:13:20.675412: step 6202, loss 0.000895253, acc 1
2016-11-12T19:13:20.731821: step 6203, loss 0.0533368, acc 0.984375
2016-11-12T19:13:20.789850: step 6204, loss 0.0108286, acc 1
2016-11-12T19:13:20.847721: step 6205, loss 0.0253998, acc 0.984375
2016-11-12T19:13:20.904668: step 6206, loss 0.0303238, acc 0.984375
2016-11-12T19:13:20.961915: step 6207, loss 0.000674586, acc 1
2016-11-12T19:13:21.018007: step 6208, loss 0.000237465, acc 1
2016-11-12T19:13:21.073631: step 6209, loss 0.000529545, acc 1
2016-11-12T19:13:21.133119: step 6210, loss 0.0116381, acc 0.984375
2016-11-12T19:13:21.191642: step 6211, loss 0.058894, acc 0.96875
2016-11-12T19:13:21.248434: step 6212, loss 0.00213396, acc 1
2016-11-12T19:13:21.305452: step 6213, loss 0.00217249, acc 1
2016-11-12T19:13:21.362684: step 6214, loss 0.000910617, acc 1
2016-11-12T19:13:21.419763: step 6215, loss 0.00417991, acc 1
2016-11-12T19:13:21.476529: step 6216, loss 0.00147329, acc 1
2016-11-12T19:13:21.532596: step 6217, loss 0.00147742, acc 1
2016-11-12T19:13:21.589137: step 6218, loss 0.0191911, acc 0.984375
2016-11-12T19:13:21.646435: step 6219, loss 0.00538983, acc 1
2016-11-12T19:13:21.702076: step 6220, loss 0.0057615, acc 1
2016-11-12T19:13:21.758835: step 6221, loss 0.00117067, acc 1
2016-11-12T19:13:21.817340: step 6222, loss 0.0386981, acc 0.984375
2016-11-12T19:13:21.874342: step 6223, loss 0.0197966, acc 0.984375
2016-11-12T19:13:21.930815: step 6224, loss 0.0257293, acc 0.984375
2016-11-12T19:13:21.989225: step 6225, loss 0.000789868, acc 1
2016-11-12T19:13:22.047308: step 6226, loss 0.000723251, acc 1
2016-11-12T19:13:22.105780: step 6227, loss 0.00178611, acc 1
2016-11-12T19:13:22.163162: step 6228, loss 0.00966549, acc 1
2016-11-12T19:13:22.221117: step 6229, loss 0.00434225, acc 1
2016-11-12T19:13:22.278927: step 6230, loss 0.0437423, acc 0.984375
2016-11-12T19:13:22.336522: step 6231, loss 0.00221923, acc 1
2016-11-12T19:13:22.393384: step 6232, loss 0.00164035, acc 1
2016-11-12T19:13:22.451962: step 6233, loss 0.00267364, acc 1
2016-11-12T19:13:22.508470: step 6234, loss 0.0019567, acc 1
2016-11-12T19:13:22.565357: step 6235, loss 0.0180979, acc 0.984375
2016-11-12T19:13:22.622741: step 6236, loss 0.00708271, acc 1
2016-11-12T19:13:22.680524: step 6237, loss 6.50817e-05, acc 1
2016-11-12T19:13:22.738948: step 6238, loss 0.00162054, acc 1
2016-11-12T19:13:22.796243: step 6239, loss 0.00160545, acc 1
2016-11-12T19:13:22.856885: step 6240, loss 0.0174265, acc 0.984375
2016-11-12T19:13:22.916676: step 6241, loss 0.00038917, acc 1
2016-11-12T19:13:22.973192: step 6242, loss 0.0118584, acc 1
2016-11-12T19:13:23.031797: step 6243, loss 0.0016123, acc 1
2016-11-12T19:13:23.088833: step 6244, loss 0.000252964, acc 1
2016-11-12T19:13:23.145822: step 6245, loss 0.051158, acc 0.984375
2016-11-12T19:13:23.206672: step 6246, loss 0.00109852, acc 1
2016-11-12T19:13:23.262633: step 6247, loss 0.0356361, acc 0.984375
2016-11-12T19:13:23.305224: step 6248, loss 0.00288564, acc 1
2016-11-12T19:13:23.363417: step 6249, loss 0.0507154, acc 0.984375
2016-11-12T19:13:23.420451: step 6250, loss 0.0115219, acc 1
2016-11-12T19:13:23.479817: step 6251, loss 0.000283898, acc 1
2016-11-12T19:13:23.536112: step 6252, loss 0.000454237, acc 1
2016-11-12T19:13:23.593377: step 6253, loss 0.029728, acc 0.984375
2016-11-12T19:13:23.653285: step 6254, loss 0.0158218, acc 0.984375
2016-11-12T19:13:23.713183: step 6255, loss 0.000170671, acc 1
2016-11-12T19:13:23.771664: step 6256, loss 0.016837, acc 0.984375
2016-11-12T19:13:23.829263: step 6257, loss 0.000158145, acc 1
2016-11-12T19:13:23.886883: step 6258, loss 0.00063, acc 1
2016-11-12T19:13:23.943218: step 6259, loss 0.0138684, acc 1
2016-11-12T19:13:24.001224: step 6260, loss 0.000721848, acc 1
2016-11-12T19:13:24.060837: step 6261, loss 0.00108887, acc 1
2016-11-12T19:13:24.117938: step 6262, loss 0.000558469, acc 1
2016-11-12T19:13:24.174770: step 6263, loss 0.000634674, acc 1
2016-11-12T19:13:24.232095: step 6264, loss 0.00129987, acc 1
2016-11-12T19:13:24.288392: step 6265, loss 0.00443675, acc 1
2016-11-12T19:13:24.345165: step 6266, loss 0.000864025, acc 1
2016-11-12T19:13:24.403940: step 6267, loss 0.000653116, acc 1
2016-11-12T19:13:24.460105: step 6268, loss 0.0138324, acc 0.984375
2016-11-12T19:13:24.520843: step 6269, loss 0.0055443, acc 1
2016-11-12T19:13:24.578547: step 6270, loss 0.0183189, acc 0.984375
2016-11-12T19:13:24.637297: step 6271, loss 0.000931155, acc 1
2016-11-12T19:13:24.693406: step 6272, loss 0.00146949, acc 1
2016-11-12T19:13:24.752289: step 6273, loss 0.00223415, acc 1
2016-11-12T19:13:24.809350: step 6274, loss 0.000927726, acc 1
2016-11-12T19:13:24.865934: step 6275, loss 0.0751567, acc 0.984375
2016-11-12T19:13:24.923550: step 6276, loss 0.00147007, acc 1
2016-11-12T19:13:24.981350: step 6277, loss 0.00113135, acc 1
2016-11-12T19:13:25.037682: step 6278, loss 0.00356496, acc 1
2016-11-12T19:13:25.094223: step 6279, loss 0.00472903, acc 1
2016-11-12T19:13:25.152587: step 6280, loss 0.0260477, acc 0.984375
2016-11-12T19:13:25.210314: step 6281, loss 0.00631113, acc 1
2016-11-12T19:13:25.267548: step 6282, loss 0.0152726, acc 0.984375
2016-11-12T19:13:25.324265: step 6283, loss 0.000413206, acc 1
2016-11-12T19:13:25.380276: step 6284, loss 0.00117823, acc 1
2016-11-12T19:13:25.437962: step 6285, loss 0.00033846, acc 1
2016-11-12T19:13:25.498075: step 6286, loss 0.00491994, acc 1
2016-11-12T19:13:25.555183: step 6287, loss 0.000892784, acc 1
2016-11-12T19:13:25.612941: step 6288, loss 0.00385751, acc 1
2016-11-12T19:13:25.674593: step 6289, loss 0.0599426, acc 0.96875
2016-11-12T19:13:25.733094: step 6290, loss 0.0116479, acc 1
2016-11-12T19:13:25.792799: step 6291, loss 0.00106644, acc 1
2016-11-12T19:13:25.851411: step 6292, loss 0.00368974, acc 1
2016-11-12T19:13:25.909805: step 6293, loss 0.00918789, acc 1
2016-11-12T19:13:25.968056: step 6294, loss 0.0152566, acc 0.984375
2016-11-12T19:13:26.026164: step 6295, loss 0.00190307, acc 1
2016-11-12T19:13:26.083311: step 6296, loss 0.00209277, acc 1
2016-11-12T19:13:26.141489: step 6297, loss 0.0171136, acc 0.984375
2016-11-12T19:13:26.200480: step 6298, loss 0.0031852, acc 1
2016-11-12T19:13:26.257060: step 6299, loss 0.000132153, acc 1
2016-11-12T19:13:26.315179: step 6300, loss 0.00418267, acc 1

Evaluation:
2016-11-12T19:13:26.387384: step 6300, loss 2.91688, acc 0.574

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-6300

2016-11-12T19:13:26.880356: step 6301, loss 0.0829365, acc 0.96875
2016-11-12T19:13:26.939709: step 6302, loss 0.00545325, acc 1
2016-11-12T19:13:26.998149: step 6303, loss 0.00286741, acc 1
2016-11-12T19:13:27.054856: step 6304, loss 0.0299824, acc 0.984375
2016-11-12T19:13:27.113533: step 6305, loss 0.00100667, acc 1
2016-11-12T19:13:27.170606: step 6306, loss 0.00881685, acc 1
2016-11-12T19:13:27.229392: step 6307, loss 0.0220387, acc 0.984375
2016-11-12T19:13:27.288384: step 6308, loss 0.00190641, acc 1
2016-11-12T19:13:27.348088: step 6309, loss 0.0021237, acc 1
2016-11-12T19:13:27.404594: step 6310, loss 0.00450498, acc 1
2016-11-12T19:13:27.462466: step 6311, loss 0.171554, acc 0.96875
2016-11-12T19:13:27.521146: step 6312, loss 0.00136973, acc 1
2016-11-12T19:13:27.578641: step 6313, loss 0.00021736, acc 1
2016-11-12T19:13:27.634301: step 6314, loss 0.00225009, acc 1
2016-11-12T19:13:27.692853: step 6315, loss 0.000346981, acc 1
2016-11-12T19:13:27.751007: step 6316, loss 0.00256241, acc 1
2016-11-12T19:13:27.808233: step 6317, loss 0.0113135, acc 1
2016-11-12T19:13:27.869177: step 6318, loss 0.00227434, acc 1
2016-11-12T19:13:27.908754: step 6319, loss 0.000163536, acc 1
2016-11-12T19:13:27.967622: step 6320, loss 0.00266545, acc 1
2016-11-12T19:13:28.025247: step 6321, loss 0.00183663, acc 1
2016-11-12T19:13:28.085983: step 6322, loss 0.00143899, acc 1
2016-11-12T19:13:28.141913: step 6323, loss 0.00107337, acc 1
2016-11-12T19:13:28.200638: step 6324, loss 0.0219426, acc 1
2016-11-12T19:13:28.262038: step 6325, loss 0.00149276, acc 1
2016-11-12T19:13:28.319648: step 6326, loss 0.00287791, acc 1
2016-11-12T19:13:28.377466: step 6327, loss 0.00146027, acc 1
2016-11-12T19:13:28.434284: step 6328, loss 0.00104867, acc 1
2016-11-12T19:13:28.491237: step 6329, loss 0.00537002, acc 1
2016-11-12T19:13:28.552064: step 6330, loss 0.000262889, acc 1
2016-11-12T19:13:28.607329: step 6331, loss 0.00534188, acc 1
2016-11-12T19:13:28.664902: step 6332, loss 0.00115504, acc 1
2016-11-12T19:13:28.722108: step 6333, loss 0.00736224, acc 1
2016-11-12T19:13:28.780917: step 6334, loss 0.0206435, acc 0.984375
2016-11-12T19:13:28.841407: step 6335, loss 0.000192049, acc 1
2016-11-12T19:13:28.897261: step 6336, loss 0.0156041, acc 0.984375
2016-11-12T19:13:28.955621: step 6337, loss 0.0199353, acc 0.984375
2016-11-12T19:13:29.013471: step 6338, loss 0.0179397, acc 1
2016-11-12T19:13:29.071028: step 6339, loss 0.02093, acc 1
2016-11-12T19:13:29.129164: step 6340, loss 0.0113661, acc 1
2016-11-12T19:13:29.189207: step 6341, loss 0.00150261, acc 1
2016-11-12T19:13:29.246424: step 6342, loss 0.00167851, acc 1
2016-11-12T19:13:29.304828: step 6343, loss 0.0189703, acc 0.984375
2016-11-12T19:13:29.365112: step 6344, loss 0.0306676, acc 0.984375
2016-11-12T19:13:29.424587: step 6345, loss 0.0151882, acc 1
2016-11-12T19:13:29.484355: step 6346, loss 0.00469291, acc 1
2016-11-12T19:13:29.541848: step 6347, loss 0.00152597, acc 1
2016-11-12T19:13:29.601345: step 6348, loss 0.00982008, acc 1
2016-11-12T19:13:29.659379: step 6349, loss 0.193719, acc 0.984375
2016-11-12T19:13:29.718612: step 6350, loss 0.000127153, acc 1
2016-11-12T19:13:29.776239: step 6351, loss 0.000255322, acc 1
2016-11-12T19:13:29.833840: step 6352, loss 0.00421309, acc 1
2016-11-12T19:13:29.890728: step 6353, loss 0.0104978, acc 1
2016-11-12T19:13:29.947405: step 6354, loss 0.00695174, acc 1
2016-11-12T19:13:30.003620: step 6355, loss 0.0119333, acc 1
2016-11-12T19:13:30.061175: step 6356, loss 0.000635629, acc 1
2016-11-12T19:13:30.119455: step 6357, loss 0.000581554, acc 1
2016-11-12T19:13:30.177880: step 6358, loss 0.00107459, acc 1
2016-11-12T19:13:30.234162: step 6359, loss 0.000322909, acc 1
2016-11-12T19:13:30.289809: step 6360, loss 0.0031642, acc 1
2016-11-12T19:13:30.348224: step 6361, loss 0.0148752, acc 0.984375
2016-11-12T19:13:30.408417: step 6362, loss 0.00211462, acc 1
2016-11-12T19:13:30.468566: step 6363, loss 0.00152684, acc 1
2016-11-12T19:13:30.524814: step 6364, loss 0.0466402, acc 0.984375
2016-11-12T19:13:30.584362: step 6365, loss 0.0136267, acc 0.984375
2016-11-12T19:13:30.641763: step 6366, loss 0.000149723, acc 1
2016-11-12T19:13:30.697691: step 6367, loss 0.0754468, acc 0.96875
2016-11-12T19:13:30.757264: step 6368, loss 0.000994402, acc 1
2016-11-12T19:13:30.813298: step 6369, loss 0.0314309, acc 0.984375
2016-11-12T19:13:30.872938: step 6370, loss 0.00379035, acc 1
2016-11-12T19:13:30.930297: step 6371, loss 0.0667876, acc 0.96875
2016-11-12T19:13:30.988585: step 6372, loss 0.31714, acc 0.984375
2016-11-12T19:13:31.047848: step 6373, loss 0.000134045, acc 1
2016-11-12T19:13:31.103582: step 6374, loss 0.00296646, acc 1
2016-11-12T19:13:31.161150: step 6375, loss 0.000274258, acc 1
2016-11-12T19:13:31.217479: step 6376, loss 0.0155299, acc 0.984375
2016-11-12T19:13:31.276662: step 6377, loss 0.00621999, acc 1
2016-11-12T19:13:31.334783: step 6378, loss 0.000882311, acc 1
2016-11-12T19:13:31.392561: step 6379, loss 0.00201758, acc 1
2016-11-12T19:13:31.449428: step 6380, loss 5.83697e-05, acc 1
2016-11-12T19:13:31.504537: step 6381, loss 0.00493228, acc 1
2016-11-12T19:13:31.564456: step 6382, loss 0.00480584, acc 1
2016-11-12T19:13:31.620581: step 6383, loss 0.0260962, acc 0.984375
2016-11-12T19:13:31.681586: step 6384, loss 0.00533628, acc 1
2016-11-12T19:13:31.739313: step 6385, loss 0.00373544, acc 1
2016-11-12T19:13:31.798590: step 6386, loss 0.0153192, acc 0.984375
2016-11-12T19:13:31.856151: step 6387, loss 0.000478288, acc 1
2016-11-12T19:13:31.914711: step 6388, loss 0.0115623, acc 0.984375
2016-11-12T19:13:31.972078: step 6389, loss 0.135127, acc 0.984375
2016-11-12T19:13:32.013699: step 6390, loss 0.00682805, acc 1
2016-11-12T19:13:32.077146: step 6391, loss 0.000960212, acc 1
2016-11-12T19:13:32.133979: step 6392, loss 0.00472848, acc 1
2016-11-12T19:13:32.192798: step 6393, loss 0.0104033, acc 1
2016-11-12T19:13:32.249545: step 6394, loss 0.0325072, acc 0.984375
2016-11-12T19:13:32.309234: step 6395, loss 0.0173746, acc 0.984375
2016-11-12T19:13:32.367116: step 6396, loss 0.00105591, acc 1
2016-11-12T19:13:32.424199: step 6397, loss 0.01243, acc 1
2016-11-12T19:13:32.481405: step 6398, loss 0.00591805, acc 1
2016-11-12T19:13:32.538709: step 6399, loss 0.0130679, acc 0.984375
2016-11-12T19:13:32.596684: step 6400, loss 0.000650443, acc 1

Evaluation:
2016-11-12T19:13:32.667160: step 6400, loss 2.86011, acc 0.57

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-6400

2016-11-12T19:13:33.160165: step 6401, loss 0.000705755, acc 1
2016-11-12T19:13:33.216982: step 6402, loss 0.00484491, acc 1
2016-11-12T19:13:33.276464: step 6403, loss 0.0231181, acc 0.984375
2016-11-12T19:13:33.334645: step 6404, loss 0.00150047, acc 1
2016-11-12T19:13:33.391197: step 6405, loss 0.0136173, acc 1
2016-11-12T19:13:33.449243: step 6406, loss 0.019753, acc 1
2016-11-12T19:13:33.509884: step 6407, loss 0.00494971, acc 1
2016-11-12T19:13:33.567369: step 6408, loss 0.000235711, acc 1
2016-11-12T19:13:33.626166: step 6409, loss 0.00196254, acc 1
2016-11-12T19:13:33.682537: step 6410, loss 0.000275898, acc 1
2016-11-12T19:13:33.738610: step 6411, loss 0.0160846, acc 0.984375
2016-11-12T19:13:33.797613: step 6412, loss 0.000123225, acc 1
2016-11-12T19:13:33.853012: step 6413, loss 0.00953421, acc 1
2016-11-12T19:13:33.909745: step 6414, loss 0.01619, acc 0.984375
2016-11-12T19:13:33.967926: step 6415, loss 0.0102935, acc 1
2016-11-12T19:13:34.024515: step 6416, loss 0.00360568, acc 1
2016-11-12T19:13:34.080421: step 6417, loss 0.00233893, acc 1
2016-11-12T19:13:34.136593: step 6418, loss 0.00353714, acc 1
2016-11-12T19:13:34.194109: step 6419, loss 0.0272225, acc 0.984375
2016-11-12T19:13:34.251345: step 6420, loss 0.0021305, acc 1
2016-11-12T19:13:34.312051: step 6421, loss 0.0592941, acc 0.984375
2016-11-12T19:13:34.368568: step 6422, loss 0.00159661, acc 1
2016-11-12T19:13:34.425438: step 6423, loss 0.00134445, acc 1
2016-11-12T19:13:34.482109: step 6424, loss 0.000325155, acc 1
2016-11-12T19:13:34.540364: step 6425, loss 0.0176857, acc 0.984375
2016-11-12T19:13:34.599761: step 6426, loss 0.0345865, acc 0.984375
2016-11-12T19:13:34.665610: step 6427, loss 0.00131117, acc 1
2016-11-12T19:13:34.723730: step 6428, loss 0.027169, acc 0.984375
2016-11-12T19:13:34.780663: step 6429, loss 0.0106354, acc 1
2016-11-12T19:13:34.838916: step 6430, loss 0.0104333, acc 1
2016-11-12T19:13:34.897445: step 6431, loss 0.000764699, acc 1
2016-11-12T19:13:34.954654: step 6432, loss 0.00179705, acc 1
2016-11-12T19:13:35.012854: step 6433, loss 0.00108341, acc 1
2016-11-12T19:13:35.069236: step 6434, loss 0.0012909, acc 1
2016-11-12T19:13:35.125906: step 6435, loss 0.0027417, acc 1
2016-11-12T19:13:35.182966: step 6436, loss 0.000803744, acc 1
2016-11-12T19:13:35.238989: step 6437, loss 0.000458405, acc 1
2016-11-12T19:13:35.295510: step 6438, loss 0.000800468, acc 1
2016-11-12T19:13:35.351908: step 6439, loss 0.000408706, acc 1
2016-11-12T19:13:35.410257: step 6440, loss 0.00116386, acc 1
2016-11-12T19:13:35.470959: step 6441, loss 0.000147421, acc 1
2016-11-12T19:13:35.528987: step 6442, loss 0.00445904, acc 1
2016-11-12T19:13:35.585699: step 6443, loss 0.00231204, acc 1
2016-11-12T19:13:35.644293: step 6444, loss 0.0367439, acc 0.984375
2016-11-12T19:13:35.704326: step 6445, loss 0.00396139, acc 1
2016-11-12T19:13:35.761800: step 6446, loss 0.000439098, acc 1
2016-11-12T19:13:35.818240: step 6447, loss 0.000647107, acc 1
2016-11-12T19:13:35.874596: step 6448, loss 0.0922808, acc 0.984375
2016-11-12T19:13:35.932689: step 6449, loss 0.00338874, acc 1
2016-11-12T19:13:35.991051: step 6450, loss 0.00115822, acc 1
2016-11-12T19:13:36.049091: step 6451, loss 0.000385583, acc 1
2016-11-12T19:13:36.106019: step 6452, loss 0.0102037, acc 1
2016-11-12T19:13:36.165176: step 6453, loss 0.00221127, acc 1
2016-11-12T19:13:36.222626: step 6454, loss 0.000580829, acc 1
2016-11-12T19:13:36.280568: step 6455, loss 0.0030609, acc 1
2016-11-12T19:13:36.337746: step 6456, loss 0.000526368, acc 1
2016-11-12T19:13:36.395105: step 6457, loss 0.0246732, acc 0.984375
2016-11-12T19:13:36.453024: step 6458, loss 0.000578968, acc 1
2016-11-12T19:13:36.509881: step 6459, loss 0.00145452, acc 1
2016-11-12T19:13:36.566816: step 6460, loss 0.0247357, acc 0.984375
2016-11-12T19:13:36.605814: step 6461, loss 0.000668252, acc 1
2016-11-12T19:13:36.665929: step 6462, loss 0.0129659, acc 0.984375
2016-11-12T19:13:36.726998: step 6463, loss 0.00620191, acc 1
2016-11-12T19:13:36.786108: step 6464, loss 0.00159253, acc 1
2016-11-12T19:13:36.846048: step 6465, loss 0.000374695, acc 1
2016-11-12T19:13:36.902667: step 6466, loss 0.029974, acc 0.984375
2016-11-12T19:13:36.960909: step 6467, loss 0.00223167, acc 1
2016-11-12T19:13:37.021258: step 6468, loss 0.000266543, acc 1
2016-11-12T19:13:37.079450: step 6469, loss 0.0199302, acc 1
2016-11-12T19:13:37.137041: step 6470, loss 0.00237431, acc 1
2016-11-12T19:13:37.196100: step 6471, loss 0.0172476, acc 0.984375
2016-11-12T19:13:37.253912: step 6472, loss 0.00926466, acc 1
2016-11-12T19:13:37.312130: step 6473, loss 0.000219344, acc 1
2016-11-12T19:13:37.367522: step 6474, loss 0.000667533, acc 1
2016-11-12T19:13:37.426591: step 6475, loss 0.00713605, acc 1
2016-11-12T19:13:37.484990: step 6476, loss 0.00831103, acc 1
2016-11-12T19:13:37.541258: step 6477, loss 0.000450658, acc 1
2016-11-12T19:13:37.600049: step 6478, loss 0.132323, acc 0.984375
2016-11-12T19:13:37.658882: step 6479, loss 0.0135679, acc 0.984375
2016-11-12T19:13:37.717039: step 6480, loss 0.000546616, acc 1
2016-11-12T19:13:37.776192: step 6481, loss 0.113299, acc 0.953125
2016-11-12T19:13:37.833777: step 6482, loss 0.000665319, acc 1
2016-11-12T19:13:37.890315: step 6483, loss 0.0157997, acc 0.984375
2016-11-12T19:13:37.947500: step 6484, loss 0.00624723, acc 1
2016-11-12T19:13:38.008000: step 6485, loss 0.000464498, acc 1
2016-11-12T19:13:38.065071: step 6486, loss 0.012136, acc 1
2016-11-12T19:13:38.124585: step 6487, loss 0.00018963, acc 1
2016-11-12T19:13:38.185251: step 6488, loss 0.000551184, acc 1
2016-11-12T19:13:38.244816: step 6489, loss 0.025436, acc 0.984375
2016-11-12T19:13:38.302618: step 6490, loss 0.000513521, acc 1
2016-11-12T19:13:38.358735: step 6491, loss 0.00373462, acc 1
2016-11-12T19:13:38.415090: step 6492, loss 0.016598, acc 1
2016-11-12T19:13:38.473775: step 6493, loss 0.000367944, acc 1
2016-11-12T19:13:38.529321: step 6494, loss 0.000244742, acc 1
2016-11-12T19:13:38.585581: step 6495, loss 0.00114553, acc 1
2016-11-12T19:13:38.644935: step 6496, loss 0.00310369, acc 1
2016-11-12T19:13:38.701720: step 6497, loss 0.00153079, acc 1
2016-11-12T19:13:38.758486: step 6498, loss 0.000843461, acc 1
2016-11-12T19:13:38.816892: step 6499, loss 0.0013277, acc 1
2016-11-12T19:13:38.875997: step 6500, loss 0.0127123, acc 0.984375

Evaluation:
2016-11-12T19:13:38.950445: step 6500, loss 2.88999, acc 0.572

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-6500

2016-11-12T19:13:39.541132: step 6501, loss 0.000961268, acc 1
2016-11-12T19:13:39.600087: step 6502, loss 0.00347669, acc 1
2016-11-12T19:13:39.657847: step 6503, loss 0.00321438, acc 1
2016-11-12T19:13:39.716963: step 6504, loss 0.00217952, acc 1
2016-11-12T19:13:39.777249: step 6505, loss 0.00612008, acc 1
2016-11-12T19:13:39.838132: step 6506, loss 0.000980594, acc 1
2016-11-12T19:13:39.894552: step 6507, loss 0.0284655, acc 0.96875
2016-11-12T19:13:39.954633: step 6508, loss 0.000508267, acc 1
2016-11-12T19:13:40.013222: step 6509, loss 0.00473972, acc 1
2016-11-12T19:13:40.070528: step 6510, loss 0.00102637, acc 1
2016-11-12T19:13:40.126098: step 6511, loss 0.0207468, acc 0.984375
2016-11-12T19:13:40.191335: step 6512, loss 0.000729638, acc 1
2016-11-12T19:13:40.252421: step 6513, loss 0.000184357, acc 1
2016-11-12T19:13:40.311683: step 6514, loss 0.0243244, acc 0.984375
2016-11-12T19:13:40.369047: step 6515, loss 0.0365344, acc 0.984375
2016-11-12T19:13:40.428220: step 6516, loss 0.00867109, acc 1
2016-11-12T19:13:40.485634: step 6517, loss 0.0240041, acc 0.984375
2016-11-12T19:13:40.543236: step 6518, loss 0.00111338, acc 1
2016-11-12T19:13:40.600640: step 6519, loss 0.00630645, acc 1
2016-11-12T19:13:40.658340: step 6520, loss 0.106187, acc 0.984375
2016-11-12T19:13:40.716323: step 6521, loss 0.00182244, acc 1
2016-11-12T19:13:40.773325: step 6522, loss 0.0030346, acc 1
2016-11-12T19:13:40.830189: step 6523, loss 0.0105123, acc 1
2016-11-12T19:13:40.888711: step 6524, loss 0.0782606, acc 0.96875
2016-11-12T19:13:40.947828: step 6525, loss 0.00693947, acc 1
2016-11-12T19:13:41.004581: step 6526, loss 0.0214046, acc 0.984375
2016-11-12T19:13:41.065140: step 6527, loss 0.000440977, acc 1
2016-11-12T19:13:41.122718: step 6528, loss 0.000564061, acc 1
2016-11-12T19:13:41.179486: step 6529, loss 0.000578263, acc 1
2016-11-12T19:13:41.236885: step 6530, loss 0.00132406, acc 1
2016-11-12T19:13:41.293371: step 6531, loss 0.00235004, acc 1
2016-11-12T19:13:41.332720: step 6532, loss 0.00019132, acc 1
2016-11-12T19:13:41.389722: step 6533, loss 0.0421457, acc 0.984375
2016-11-12T19:13:41.447576: step 6534, loss 0.0142015, acc 0.984375
2016-11-12T19:13:41.504139: step 6535, loss 0.00854474, acc 1
2016-11-12T19:13:41.561644: step 6536, loss 0.0050408, acc 1
2016-11-12T19:13:41.619319: step 6537, loss 0.000299153, acc 1
2016-11-12T19:13:41.678663: step 6538, loss 0.00751568, acc 1
2016-11-12T19:13:41.737899: step 6539, loss 0.00229124, acc 1
2016-11-12T19:13:41.794409: step 6540, loss 0.00437021, acc 1
2016-11-12T19:13:41.851405: step 6541, loss 0.00286123, acc 1
2016-11-12T19:13:41.911414: step 6542, loss 0.00781536, acc 1
2016-11-12T19:13:41.968594: step 6543, loss 0.000922768, acc 1
2016-11-12T19:13:42.026074: step 6544, loss 0.000683285, acc 1
2016-11-12T19:13:42.085014: step 6545, loss 0.000789225, acc 1
2016-11-12T19:13:42.143792: step 6546, loss 0.000931226, acc 1
2016-11-12T19:13:42.201328: step 6547, loss 0.000498133, acc 1
2016-11-12T19:13:42.257679: step 6548, loss 0.028573, acc 0.984375
2016-11-12T19:13:42.314523: step 6549, loss 0.00237563, acc 1
2016-11-12T19:13:42.372611: step 6550, loss 0.000154253, acc 1
2016-11-12T19:13:42.427496: step 6551, loss 0.0406211, acc 0.96875
2016-11-12T19:13:42.487035: step 6552, loss 0.00411966, acc 1
2016-11-12T19:13:42.543071: step 6553, loss 0.0017518, acc 1
2016-11-12T19:13:42.600008: step 6554, loss 0.0231269, acc 0.984375
2016-11-12T19:13:42.658444: step 6555, loss 0.000429289, acc 1
2016-11-12T19:13:42.716889: step 6556, loss 0.000821447, acc 1
2016-11-12T19:13:42.774035: step 6557, loss 0.00089368, acc 1
2016-11-12T19:13:42.829721: step 6558, loss 0.0742684, acc 0.96875
2016-11-12T19:13:42.889177: step 6559, loss 0.000332198, acc 1
2016-11-12T19:13:42.947836: step 6560, loss 0.00229227, acc 1
2016-11-12T19:13:43.004760: step 6561, loss 0.00185249, acc 1
2016-11-12T19:13:43.061271: step 6562, loss 0.0641005, acc 0.96875
2016-11-12T19:13:43.120177: step 6563, loss 0.0155418, acc 1
2016-11-12T19:13:43.177519: step 6564, loss 0.00188626, acc 1
2016-11-12T19:13:43.233423: step 6565, loss 0.0453162, acc 0.96875
2016-11-12T19:13:43.292972: step 6566, loss 0.000420677, acc 1
2016-11-12T19:13:43.348929: step 6567, loss 0.014474, acc 0.984375
2016-11-12T19:13:43.408546: step 6568, loss 0.000912456, acc 1
2016-11-12T19:13:43.466883: step 6569, loss 0.00119181, acc 1
2016-11-12T19:13:43.525266: step 6570, loss 0.039985, acc 0.984375
2016-11-12T19:13:43.585187: step 6571, loss 0.0782081, acc 0.984375
2016-11-12T19:13:43.643541: step 6572, loss 0.000326058, acc 1
2016-11-12T19:13:43.701324: step 6573, loss 0.000146833, acc 1
2016-11-12T19:13:43.758072: step 6574, loss 0.0142788, acc 1
2016-11-12T19:13:43.815445: step 6575, loss 0.00165804, acc 1
2016-11-12T19:13:43.873952: step 6576, loss 0.00328909, acc 1
2016-11-12T19:13:43.932780: step 6577, loss 0.00170221, acc 1
2016-11-12T19:13:43.992882: step 6578, loss 0.00277643, acc 1
2016-11-12T19:13:44.050166: step 6579, loss 0.00617614, acc 1
2016-11-12T19:13:44.112098: step 6580, loss 0.000760318, acc 1
2016-11-12T19:13:44.169297: step 6581, loss 0.0697427, acc 0.953125
2016-11-12T19:13:44.230437: step 6582, loss 0.0147856, acc 1
2016-11-12T19:13:44.289939: step 6583, loss 0.00143769, acc 1
2016-11-12T19:13:44.345851: step 6584, loss 0.0228747, acc 0.984375
2016-11-12T19:13:44.406246: step 6585, loss 0.000271454, acc 1
2016-11-12T19:13:44.463106: step 6586, loss 0.00622816, acc 1
2016-11-12T19:13:44.522200: step 6587, loss 0.0264585, acc 0.984375
2016-11-12T19:13:44.579362: step 6588, loss 0.0229918, acc 0.96875
2016-11-12T19:13:44.637164: step 6589, loss 0.000518165, acc 1
2016-11-12T19:13:44.695880: step 6590, loss 0.0249325, acc 0.984375
2016-11-12T19:13:44.753258: step 6591, loss 0.002172, acc 1
2016-11-12T19:13:44.809734: step 6592, loss 0.00429514, acc 1
2016-11-12T19:13:44.869312: step 6593, loss 0.0392095, acc 0.984375
2016-11-12T19:13:44.926940: step 6594, loss 0.00327736, acc 1
2016-11-12T19:13:44.985325: step 6595, loss 0.00168991, acc 1
2016-11-12T19:13:45.045280: step 6596, loss 0.0143669, acc 0.984375
2016-11-12T19:13:45.105189: step 6597, loss 0.00158796, acc 1
2016-11-12T19:13:45.164646: step 6598, loss 0.00553066, acc 1
2016-11-12T19:13:45.221612: step 6599, loss 0.00244288, acc 1
2016-11-12T19:13:45.282470: step 6600, loss 0.000385668, acc 1

Evaluation:
2016-11-12T19:13:45.352772: step 6600, loss 2.92986, acc 0.574

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-6600

2016-11-12T19:13:45.844908: step 6601, loss 0.00149517, acc 1
2016-11-12T19:13:45.901125: step 6602, loss 0.00088083, acc 1
2016-11-12T19:13:45.938846: step 6603, loss 2.72373e-05, acc 1
2016-11-12T19:13:45.996295: step 6604, loss 0.00178296, acc 1
2016-11-12T19:13:46.053154: step 6605, loss 0.00105344, acc 1
2016-11-12T19:13:46.110168: step 6606, loss 0.0136093, acc 0.984375
2016-11-12T19:13:46.166816: step 6607, loss 0.00151707, acc 1
2016-11-12T19:13:46.224447: step 6608, loss 0.010005, acc 1
2016-11-12T19:13:46.289133: step 6609, loss 0.000508298, acc 1
2016-11-12T19:13:46.347168: step 6610, loss 0.0183677, acc 0.984375
2016-11-12T19:13:46.404795: step 6611, loss 0.0221286, acc 0.984375
2016-11-12T19:13:46.464594: step 6612, loss 0.000382991, acc 1
2016-11-12T19:13:46.522958: step 6613, loss 0.000526661, acc 1
2016-11-12T19:13:46.580884: step 6614, loss 0.012338, acc 0.984375
2016-11-12T19:13:46.637695: step 6615, loss 0.00037467, acc 1
2016-11-12T19:13:46.694787: step 6616, loss 0.00586618, acc 1
2016-11-12T19:13:46.753277: step 6617, loss 0.000108263, acc 1
2016-11-12T19:13:46.808436: step 6618, loss 0.000847596, acc 1
2016-11-12T19:13:46.864531: step 6619, loss 0.00345882, acc 1
2016-11-12T19:13:46.921392: step 6620, loss 0.000605247, acc 1
2016-11-12T19:13:46.980024: step 6621, loss 0.000274915, acc 1
2016-11-12T19:13:47.037232: step 6622, loss 0.00121853, acc 1
2016-11-12T19:13:47.094965: step 6623, loss 0.00533396, acc 1
2016-11-12T19:13:47.152519: step 6624, loss 0.000847152, acc 1
2016-11-12T19:13:47.209335: step 6625, loss 0.000451588, acc 1
2016-11-12T19:13:47.266602: step 6626, loss 0.00257614, acc 1
2016-11-12T19:13:47.326492: step 6627, loss 0.000170283, acc 1
2016-11-12T19:13:47.385343: step 6628, loss 0.0316454, acc 0.984375
2016-11-12T19:13:47.445717: step 6629, loss 0.000445599, acc 1
2016-11-12T19:13:47.502248: step 6630, loss 0.000949929, acc 1
2016-11-12T19:13:47.558493: step 6631, loss 0.0004392, acc 1
2016-11-12T19:13:47.614357: step 6632, loss 0.00181197, acc 1
2016-11-12T19:13:47.672272: step 6633, loss 0.00149355, acc 1
2016-11-12T19:13:47.730880: step 6634, loss 0.000228288, acc 1
2016-11-12T19:13:47.787646: step 6635, loss 0.000407316, acc 1
2016-11-12T19:13:47.844977: step 6636, loss 0.00202861, acc 1
2016-11-12T19:13:47.904319: step 6637, loss 0.00225855, acc 1
2016-11-12T19:13:47.960752: step 6638, loss 0.00884376, acc 1
2016-11-12T19:13:48.019104: step 6639, loss 0.0153411, acc 0.984375
2016-11-12T19:13:48.076937: step 6640, loss 0.0265843, acc 0.984375
2016-11-12T19:13:48.135963: step 6641, loss 0.010805, acc 1
2016-11-12T19:13:48.192739: step 6642, loss 0.00194337, acc 1
2016-11-12T19:13:48.252086: step 6643, loss 0.100519, acc 0.984375
2016-11-12T19:13:48.311796: step 6644, loss 0.0242285, acc 1
2016-11-12T19:13:48.368676: step 6645, loss 0.0130077, acc 1
2016-11-12T19:13:48.425915: step 6646, loss 0.000704037, acc 1
2016-11-12T19:13:48.484410: step 6647, loss 0.067605, acc 0.984375
2016-11-12T19:13:48.542498: step 6648, loss 0.00433925, acc 1
2016-11-12T19:13:48.599466: step 6649, loss 0.00438668, acc 1
2016-11-12T19:13:48.657208: step 6650, loss 0.0100159, acc 1
2016-11-12T19:13:48.717232: step 6651, loss 0.000337604, acc 1
2016-11-12T19:13:48.774691: step 6652, loss 0.0184906, acc 0.984375
2016-11-12T19:13:48.830715: step 6653, loss 0.0153702, acc 0.984375
2016-11-12T19:13:48.889678: step 6654, loss 0.00112277, acc 1
2016-11-12T19:13:48.946026: step 6655, loss 0.00229972, acc 1
2016-11-12T19:13:49.002443: step 6656, loss 0.00652727, acc 1
2016-11-12T19:13:49.061022: step 6657, loss 0.00996228, acc 1
2016-11-12T19:13:49.118204: step 6658, loss 0.000107679, acc 1
2016-11-12T19:13:49.173456: step 6659, loss 0.00131655, acc 1
2016-11-12T19:13:49.229708: step 6660, loss 0.109259, acc 0.984375
2016-11-12T19:13:49.291367: step 6661, loss 0.189854, acc 0.984375
2016-11-12T19:13:49.349651: step 6662, loss 0.00100449, acc 1
2016-11-12T19:13:49.405889: step 6663, loss 0.00197402, acc 1
2016-11-12T19:13:49.464264: step 6664, loss 0.0119024, acc 1
2016-11-12T19:13:49.522344: step 6665, loss 0.0409497, acc 0.984375
2016-11-12T19:13:49.579710: step 6666, loss 0.016029, acc 0.984375
2016-11-12T19:13:49.635785: step 6667, loss 0.0255408, acc 0.984375
2016-11-12T19:13:49.693045: step 6668, loss 0.0153613, acc 1
2016-11-12T19:13:49.750537: step 6669, loss 0.000685272, acc 1
2016-11-12T19:13:49.806435: step 6670, loss 0.00227462, acc 1
2016-11-12T19:13:49.862600: step 6671, loss 0.00217889, acc 1
2016-11-12T19:13:49.919610: step 6672, loss 0.00811798, acc 1
2016-11-12T19:13:49.978599: step 6673, loss 0.00117997, acc 1
2016-11-12T19:13:50.020243: step 6674, loss 0.000459521, acc 1
2016-11-12T19:13:50.078954: step 6675, loss 0.00179552, acc 1
2016-11-12T19:13:50.136172: step 6676, loss 0.0258847, acc 0.984375
2016-11-12T19:13:50.193695: step 6677, loss 0.00112664, acc 1
2016-11-12T19:13:50.251248: step 6678, loss 0.00650152, acc 1
2016-11-12T19:13:50.309295: step 6679, loss 0.015591, acc 0.984375
2016-11-12T19:13:50.369675: step 6680, loss 0.00642251, acc 1
2016-11-12T19:13:50.428916: step 6681, loss 0.00879643, acc 1
2016-11-12T19:13:50.486484: step 6682, loss 0.00313081, acc 1
2016-11-12T19:13:50.542570: step 6683, loss 0.0116645, acc 1
2016-11-12T19:13:50.601207: step 6684, loss 0.00349757, acc 1
2016-11-12T19:13:50.659772: step 6685, loss 0.00092854, acc 1
2016-11-12T19:13:50.716939: step 6686, loss 0.00138335, acc 1
2016-11-12T19:13:50.773204: step 6687, loss 0.0443911, acc 0.96875
2016-11-12T19:13:50.832347: step 6688, loss 0.0110529, acc 1
2016-11-12T19:13:50.889581: step 6689, loss 0.00147186, acc 1
2016-11-12T19:13:50.946113: step 6690, loss 0.00213222, acc 1
2016-11-12T19:13:51.004050: step 6691, loss 0.00039659, acc 1
2016-11-12T19:13:51.061258: step 6692, loss 0.0013973, acc 1
2016-11-12T19:13:51.119440: step 6693, loss 0.000965531, acc 1
2016-11-12T19:13:51.176007: step 6694, loss 0.00103886, acc 1
2016-11-12T19:13:51.232707: step 6695, loss 0.108787, acc 0.984375
2016-11-12T19:13:51.290087: step 6696, loss 0.0315054, acc 0.96875
2016-11-12T19:13:51.346858: step 6697, loss 0.0177843, acc 0.984375
2016-11-12T19:13:51.405429: step 6698, loss 0.000155632, acc 1
2016-11-12T19:13:51.461342: step 6699, loss 0.000948503, acc 1
2016-11-12T19:13:51.517495: step 6700, loss 0.0202381, acc 0.984375

Evaluation:
2016-11-12T19:13:51.588524: step 6700, loss 2.98906, acc 0.574

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-6700

2016-11-12T19:13:52.081226: step 6701, loss 0.00192231, acc 1
2016-11-12T19:13:52.141004: step 6702, loss 0.000529788, acc 1
2016-11-12T19:13:52.196691: step 6703, loss 0.0447818, acc 0.984375
2016-11-12T19:13:52.255094: step 6704, loss 0.00266704, acc 1
2016-11-12T19:13:52.311661: step 6705, loss 0.00148431, acc 1
2016-11-12T19:13:52.369103: step 6706, loss 0.00195229, acc 1
2016-11-12T19:13:52.426319: step 6707, loss 0.0633632, acc 0.96875
2016-11-12T19:13:52.485431: step 6708, loss 0.000376107, acc 1
2016-11-12T19:13:52.541667: step 6709, loss 0.00160537, acc 1
2016-11-12T19:13:52.599018: step 6710, loss 0.0112127, acc 1
2016-11-12T19:13:52.657816: step 6711, loss 0.00501847, acc 1
2016-11-12T19:13:52.719393: step 6712, loss 0.00861477, acc 1
2016-11-12T19:13:52.777277: step 6713, loss 0.000656758, acc 1
2016-11-12T19:13:52.833900: step 6714, loss 0.00918973, acc 1
2016-11-12T19:13:52.893956: step 6715, loss 0.00322589, acc 1
2016-11-12T19:13:52.952776: step 6716, loss 0.0084578, acc 1
2016-11-12T19:13:53.010191: step 6717, loss 0.00103238, acc 1
2016-11-12T19:13:53.066580: step 6718, loss 0.000550234, acc 1
2016-11-12T19:13:53.125372: step 6719, loss 0.00150165, acc 1
2016-11-12T19:13:53.183327: step 6720, loss 0.0363624, acc 0.984375
2016-11-12T19:13:53.241290: step 6721, loss 0.00224786, acc 1
2016-11-12T19:13:53.299145: step 6722, loss 0.00241642, acc 1
2016-11-12T19:13:53.355197: step 6723, loss 0.00384126, acc 1
2016-11-12T19:13:53.414748: step 6724, loss 0.000490931, acc 1
2016-11-12T19:13:53.474786: step 6725, loss 0.000398152, acc 1
2016-11-12T19:13:53.537385: step 6726, loss 9.28777e-05, acc 1
2016-11-12T19:13:53.594793: step 6727, loss 0.00528965, acc 1
2016-11-12T19:13:53.651989: step 6728, loss 0.110124, acc 0.984375
2016-11-12T19:13:53.709354: step 6729, loss 0.00181125, acc 1
2016-11-12T19:13:53.766181: step 6730, loss 0.00196406, acc 1
2016-11-12T19:13:53.824856: step 6731, loss 0.0340191, acc 0.984375
2016-11-12T19:13:53.882123: step 6732, loss 0.0202531, acc 0.984375
2016-11-12T19:13:53.943119: step 6733, loss 0.0506543, acc 0.96875
2016-11-12T19:13:54.000520: step 6734, loss 0.00298966, acc 1
2016-11-12T19:13:54.058059: step 6735, loss 0.0147792, acc 1
2016-11-12T19:13:54.117644: step 6736, loss 0.00276306, acc 1
2016-11-12T19:13:54.175436: step 6737, loss 0.000392529, acc 1
2016-11-12T19:13:54.233078: step 6738, loss 0.0241957, acc 0.984375
2016-11-12T19:13:54.291692: step 6739, loss 0.0509266, acc 0.984375
2016-11-12T19:13:54.350874: step 6740, loss 0.033231, acc 0.96875
2016-11-12T19:13:54.408957: step 6741, loss 0.0103515, acc 1
2016-11-12T19:13:54.468688: step 6742, loss 0.000731005, acc 1
2016-11-12T19:13:54.527877: step 6743, loss 0.00137213, acc 1
2016-11-12T19:13:54.585920: step 6744, loss 0.00664742, acc 1
2016-11-12T19:13:54.624551: step 6745, loss 0.0206953, acc 1
2016-11-12T19:13:54.682209: step 6746, loss 0.0211802, acc 0.984375
2016-11-12T19:13:54.741666: step 6747, loss 0.0144074, acc 0.984375
2016-11-12T19:13:54.800549: step 6748, loss 0.00732422, acc 1
2016-11-12T19:13:54.859780: step 6749, loss 0.0112754, acc 1
2016-11-12T19:13:54.917197: step 6750, loss 0.00842579, acc 1
2016-11-12T19:13:54.974248: step 6751, loss 0.00238707, acc 1
2016-11-12T19:13:55.030782: step 6752, loss 0.00410703, acc 1
2016-11-12T19:13:55.087537: step 6753, loss 0.010307, acc 1
2016-11-12T19:13:55.144205: step 6754, loss 0.000168174, acc 1
2016-11-12T19:13:55.199576: step 6755, loss 0.0088215, acc 1
2016-11-12T19:13:55.256764: step 6756, loss 0.00920214, acc 1
2016-11-12T19:13:55.317176: step 6757, loss 0.018511, acc 0.984375
2016-11-12T19:13:55.378376: step 6758, loss 0.00235306, acc 1
2016-11-12T19:13:55.435286: step 6759, loss 0.00126516, acc 1
2016-11-12T19:13:55.492855: step 6760, loss 0.0195207, acc 0.984375
2016-11-12T19:13:55.551253: step 6761, loss 0.00630655, acc 1
2016-11-12T19:13:55.609724: step 6762, loss 0.000545085, acc 1
2016-11-12T19:13:55.667698: step 6763, loss 0.000930585, acc 1
2016-11-12T19:13:55.725590: step 6764, loss 0.000202379, acc 1
2016-11-12T19:13:55.781381: step 6765, loss 0.0018167, acc 1
2016-11-12T19:13:55.838062: step 6766, loss 8.13966e-05, acc 1
2016-11-12T19:13:55.896638: step 6767, loss 0.00704696, acc 1
2016-11-12T19:13:55.955619: step 6768, loss 0.00130679, acc 1
2016-11-12T19:13:56.013650: step 6769, loss 0.00139598, acc 1
2016-11-12T19:13:56.072886: step 6770, loss 0.00116486, acc 1
2016-11-12T19:13:56.135258: step 6771, loss 0.0217353, acc 0.984375
2016-11-12T19:13:56.194515: step 6772, loss 0.00521792, acc 1
2016-11-12T19:13:56.251048: step 6773, loss 0.00095827, acc 1
2016-11-12T19:13:56.310143: step 6774, loss 0.00386781, acc 1
2016-11-12T19:13:56.368557: step 6775, loss 0.0260212, acc 0.984375
2016-11-12T19:13:56.424885: step 6776, loss 0.000783945, acc 1
2016-11-12T19:13:56.479768: step 6777, loss 0.00151549, acc 1
2016-11-12T19:13:56.536826: step 6778, loss 0.00454012, acc 1
2016-11-12T19:13:56.594308: step 6779, loss 0.0016424, acc 1
2016-11-12T19:13:56.651687: step 6780, loss 0.0140562, acc 0.984375
2016-11-12T19:13:56.709549: step 6781, loss 0.000377191, acc 1
2016-11-12T19:13:56.765428: step 6782, loss 0.00201406, acc 1
2016-11-12T19:13:56.822072: step 6783, loss 0.010176, acc 1
2016-11-12T19:13:56.881893: step 6784, loss 0.0007177, acc 1
2016-11-12T19:13:56.939818: step 6785, loss 0.0388029, acc 0.984375
2016-11-12T19:13:56.997099: step 6786, loss 0.0171523, acc 0.984375
2016-11-12T19:13:57.054017: step 6787, loss 0.000881035, acc 1
2016-11-12T19:13:57.113242: step 6788, loss 0.000568569, acc 1
2016-11-12T19:13:57.172887: step 6789, loss 0.0125079, acc 0.984375
2016-11-12T19:13:57.233157: step 6790, loss 0.000487317, acc 1
2016-11-12T19:13:57.291560: step 6791, loss 0.00148805, acc 1
2016-11-12T19:13:57.349099: step 6792, loss 0.00123728, acc 1
2016-11-12T19:13:57.405131: step 6793, loss 0.0223741, acc 0.984375
2016-11-12T19:13:57.463190: step 6794, loss 0.0144467, acc 1
2016-11-12T19:13:57.521381: step 6795, loss 0.00331309, acc 1
2016-11-12T19:13:57.577571: step 6796, loss 0.00172361, acc 1
2016-11-12T19:13:57.635856: step 6797, loss 0.0121311, acc 0.984375
2016-11-12T19:13:57.695235: step 6798, loss 0.00927899, acc 1
2016-11-12T19:13:57.753969: step 6799, loss 0.000833092, acc 1
2016-11-12T19:13:57.812992: step 6800, loss 0.102196, acc 0.96875

Evaluation:
2016-11-12T19:13:57.884477: step 6800, loss 3.06099, acc 0.566

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-6800

2016-11-12T19:13:58.382625: step 6801, loss 0.0432052, acc 0.984375
2016-11-12T19:13:58.440041: step 6802, loss 0.00162456, acc 1
2016-11-12T19:13:58.498187: step 6803, loss 0.087801, acc 0.96875
2016-11-12T19:13:58.557452: step 6804, loss 0.0111245, acc 1
2016-11-12T19:13:58.615423: step 6805, loss 0.0160855, acc 0.984375
2016-11-12T19:13:58.674391: step 6806, loss 0.00176508, acc 1
2016-11-12T19:13:58.733061: step 6807, loss 0.000333073, acc 1
2016-11-12T19:13:58.789231: step 6808, loss 0.0065672, acc 1
2016-11-12T19:13:58.847042: step 6809, loss 0.0609164, acc 0.96875
2016-11-12T19:13:58.905935: step 6810, loss 0.0100808, acc 1
2016-11-12T19:13:58.964770: step 6811, loss 0.00267324, acc 1
2016-11-12T19:13:59.021224: step 6812, loss 0.000106792, acc 1
2016-11-12T19:13:59.079622: step 6813, loss 0.0233311, acc 0.984375
2016-11-12T19:13:59.140187: step 6814, loss 0.00195352, acc 1
2016-11-12T19:13:59.200375: step 6815, loss 0.000725143, acc 1
2016-11-12T19:13:59.240417: step 6816, loss 0.0033, acc 1
2016-11-12T19:13:59.300926: step 6817, loss 0.00104939, acc 1
2016-11-12T19:13:59.361759: step 6818, loss 0.000545909, acc 1
2016-11-12T19:13:59.420843: step 6819, loss 0.00203664, acc 1
2016-11-12T19:13:59.478781: step 6820, loss 0.0139359, acc 0.984375
2016-11-12T19:13:59.537241: step 6821, loss 0.0044326, acc 1
2016-11-12T19:13:59.597413: step 6822, loss 0.0107972, acc 1
2016-11-12T19:13:59.656300: step 6823, loss 0.00456066, acc 1
2016-11-12T19:13:59.713633: step 6824, loss 0.0079844, acc 1
2016-11-12T19:13:59.771090: step 6825, loss 0.0190235, acc 1
2016-11-12T19:13:59.828101: step 6826, loss 0.00308523, acc 1
2016-11-12T19:13:59.885054: step 6827, loss 0.030018, acc 0.96875
2016-11-12T19:13:59.942987: step 6828, loss 0.0016468, acc 1
2016-11-12T19:14:00.001831: step 6829, loss 0.00268486, acc 1
2016-11-12T19:14:00.058412: step 6830, loss 0.000101465, acc 1
2016-11-12T19:14:00.113828: step 6831, loss 0.0179205, acc 0.984375
2016-11-12T19:14:00.171836: step 6832, loss 0.00253226, acc 1
2016-11-12T19:14:00.231182: step 6833, loss 0.0160999, acc 0.984375
2016-11-12T19:14:00.289393: step 6834, loss 0.00241887, acc 1
2016-11-12T19:14:00.349290: step 6835, loss 0.0420975, acc 0.984375
2016-11-12T19:14:00.407949: step 6836, loss 0.000503858, acc 1
2016-11-12T19:14:00.464796: step 6837, loss 0.00616568, acc 1
2016-11-12T19:14:00.521113: step 6838, loss 0.000285415, acc 1
2016-11-12T19:14:00.577203: step 6839, loss 0.000367643, acc 1
2016-11-12T19:14:00.633432: step 6840, loss 0.0886269, acc 0.96875
2016-11-12T19:14:00.693441: step 6841, loss 0.00137087, acc 1
2016-11-12T19:14:00.750587: step 6842, loss 0.00163605, acc 1
2016-11-12T19:14:00.808920: step 6843, loss 0.00116452, acc 1
2016-11-12T19:14:00.864934: step 6844, loss 6.79355e-05, acc 1
2016-11-12T19:14:00.920593: step 6845, loss 0.00476585, acc 1
2016-11-12T19:14:00.977423: step 6846, loss 0.0124216, acc 0.984375
2016-11-12T19:14:01.037177: step 6847, loss 0.0318171, acc 0.96875
2016-11-12T19:14:01.097868: step 6848, loss 0.0125883, acc 0.984375
2016-11-12T19:14:01.156188: step 6849, loss 0.000672691, acc 1
2016-11-12T19:14:01.212714: step 6850, loss 0.00303766, acc 1
2016-11-12T19:14:01.270443: step 6851, loss 0.0157387, acc 0.984375
2016-11-12T19:14:01.329461: step 6852, loss 0.000650883, acc 1
2016-11-12T19:14:01.388462: step 6853, loss 0.0170146, acc 0.984375
2016-11-12T19:14:01.445900: step 6854, loss 0.20062, acc 0.984375
2016-11-12T19:14:01.504348: step 6855, loss 0.000512142, acc 1
2016-11-12T19:14:01.561889: step 6856, loss 0.00204527, acc 1
2016-11-12T19:14:01.620307: step 6857, loss 0.040057, acc 0.984375
2016-11-12T19:14:01.678159: step 6858, loss 0.0313183, acc 0.984375
2016-11-12T19:14:01.735783: step 6859, loss 0.00856768, acc 1
2016-11-12T19:14:01.793359: step 6860, loss 0.00101777, acc 1
2016-11-12T19:14:01.849420: step 6861, loss 0.031978, acc 0.96875
2016-11-12T19:14:01.909332: step 6862, loss 0.00186243, acc 1
2016-11-12T19:14:01.968673: step 6863, loss 0.00356739, acc 1
2016-11-12T19:14:02.025606: step 6864, loss 0.000800168, acc 1
2016-11-12T19:14:02.082668: step 6865, loss 0.0251741, acc 0.984375
2016-11-12T19:14:02.140340: step 6866, loss 0.00592043, acc 1
2016-11-12T19:14:02.200946: step 6867, loss 0.00114487, acc 1
2016-11-12T19:14:02.258360: step 6868, loss 0.00536621, acc 1
2016-11-12T19:14:02.317131: step 6869, loss 0.000683105, acc 1
2016-11-12T19:14:02.372679: step 6870, loss 0.00243072, acc 1
2016-11-12T19:14:02.431067: step 6871, loss 0.0038258, acc 1
2016-11-12T19:14:02.490848: step 6872, loss 0.000398449, acc 1
2016-11-12T19:14:02.548697: step 6873, loss 0.000944952, acc 1
2016-11-12T19:14:02.606542: step 6874, loss 0.00776063, acc 1
2016-11-12T19:14:02.663850: step 6875, loss 0.0157913, acc 0.984375
2016-11-12T19:14:02.720604: step 6876, loss 0.000669032, acc 1
2016-11-12T19:14:02.777261: step 6877, loss 0.00650037, acc 1
2016-11-12T19:14:02.835976: step 6878, loss 0.255881, acc 0.984375
2016-11-12T19:14:02.896643: step 6879, loss 0.000677222, acc 1
2016-11-12T19:14:02.953405: step 6880, loss 0.0015781, acc 1
2016-11-12T19:14:03.010341: step 6881, loss 0.00923024, acc 1
2016-11-12T19:14:03.069289: step 6882, loss 0.000397039, acc 1
2016-11-12T19:14:03.125337: step 6883, loss 0.00453059, acc 1
2016-11-12T19:14:03.184503: step 6884, loss 0.00630966, acc 1
2016-11-12T19:14:03.245675: step 6885, loss 0.0111741, acc 1
2016-11-12T19:14:03.303122: step 6886, loss 0.0325472, acc 0.984375
2016-11-12T19:14:03.342528: step 6887, loss 5.49317e-05, acc 1
2016-11-12T19:14:03.401736: step 6888, loss 0.000523723, acc 1
2016-11-12T19:14:03.457786: step 6889, loss 0.00106613, acc 1
2016-11-12T19:14:03.514525: step 6890, loss 0.000954699, acc 1
2016-11-12T19:14:03.573635: step 6891, loss 0.000250008, acc 1
2016-11-12T19:14:03.630679: step 6892, loss 0.000357366, acc 1
2016-11-12T19:14:03.690889: step 6893, loss 0.00185085, acc 1
2016-11-12T19:14:03.750141: step 6894, loss 0.00951049, acc 1
2016-11-12T19:14:03.807982: step 6895, loss 0.00941738, acc 1
2016-11-12T19:14:03.865696: step 6896, loss 0.00621175, acc 1
2016-11-12T19:14:03.922118: step 6897, loss 0.0949794, acc 0.984375
2016-11-12T19:14:03.980299: step 6898, loss 0.000188522, acc 1
2016-11-12T19:14:04.037181: step 6899, loss 0.000686451, acc 1
2016-11-12T19:14:04.093672: step 6900, loss 0.00736259, acc 1

Evaluation:
2016-11-12T19:14:04.165156: step 6900, loss 3.08784, acc 0.56

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-6900

2016-11-12T19:14:04.654622: step 6901, loss 0.0147687, acc 0.984375
2016-11-12T19:14:04.715098: step 6902, loss 0.00273656, acc 1
2016-11-12T19:14:04.774016: step 6903, loss 0.000343334, acc 1
2016-11-12T19:14:04.833950: step 6904, loss 0.000922939, acc 1
2016-11-12T19:14:04.892619: step 6905, loss 0.000266762, acc 1
2016-11-12T19:14:04.949116: step 6906, loss 0.00300649, acc 1
2016-11-12T19:14:05.008492: step 6907, loss 2.96246e-05, acc 1
2016-11-12T19:14:05.067824: step 6908, loss 0.000640331, acc 1
2016-11-12T19:14:05.125521: step 6909, loss 0.0223078, acc 0.984375
2016-11-12T19:14:05.184913: step 6910, loss 0.000759476, acc 1
2016-11-12T19:14:05.242597: step 6911, loss 0.00076845, acc 1
2016-11-12T19:14:05.298866: step 6912, loss 0.00207687, acc 1
2016-11-12T19:14:05.355865: step 6913, loss 0.000800531, acc 1
2016-11-12T19:14:05.411915: step 6914, loss 0.00116216, acc 1
2016-11-12T19:14:05.467898: step 6915, loss 0.0269469, acc 0.984375
2016-11-12T19:14:05.525092: step 6916, loss 0.000475069, acc 1
2016-11-12T19:14:05.580382: step 6917, loss 0.000337583, acc 1
2016-11-12T19:14:05.637470: step 6918, loss 0.000835161, acc 1
2016-11-12T19:14:05.694034: step 6919, loss 0.00185899, acc 1
2016-11-12T19:14:05.752511: step 6920, loss 0.0129437, acc 0.984375
2016-11-12T19:14:05.811405: step 6921, loss 0.000545913, acc 1
2016-11-12T19:14:05.866815: step 6922, loss 0.00155757, acc 1
2016-11-12T19:14:05.923221: step 6923, loss 0.00403215, acc 1
2016-11-12T19:14:05.981568: step 6924, loss 0.00223456, acc 1
2016-11-12T19:14:06.039389: step 6925, loss 0.00154823, acc 1
2016-11-12T19:14:06.097818: step 6926, loss 0.00376266, acc 1
2016-11-12T19:14:06.155684: step 6927, loss 0.000465672, acc 1
2016-11-12T19:14:06.212798: step 6928, loss 0.00220874, acc 1
2016-11-12T19:14:06.269104: step 6929, loss 0.00168346, acc 1
2016-11-12T19:14:06.325978: step 6930, loss 0.0113366, acc 1
2016-11-12T19:14:06.382973: step 6931, loss 0.0201592, acc 0.984375
2016-11-12T19:14:06.440151: step 6932, loss 0.112603, acc 0.96875
2016-11-12T19:14:06.500477: step 6933, loss 0.00452163, acc 1
2016-11-12T19:14:06.557318: step 6934, loss 0.0249644, acc 0.984375
2016-11-12T19:14:06.616263: step 6935, loss 0.00157325, acc 1
2016-11-12T19:14:06.673134: step 6936, loss 0.0201708, acc 0.984375
2016-11-12T19:14:06.732486: step 6937, loss 0.044066, acc 0.96875
2016-11-12T19:14:06.792129: step 6938, loss 0.00117926, acc 1
2016-11-12T19:14:06.848688: step 6939, loss 0.00141994, acc 1
2016-11-12T19:14:06.905867: step 6940, loss 0.00556029, acc 1
2016-11-12T19:14:06.963134: step 6941, loss 0.000166598, acc 1
2016-11-12T19:14:07.018419: step 6942, loss 0.000227272, acc 1
2016-11-12T19:14:07.076248: step 6943, loss 0.0111255, acc 0.984375
2016-11-12T19:14:07.134310: step 6944, loss 0.0011441, acc 1
2016-11-12T19:14:07.191684: step 6945, loss 0.0154616, acc 1
2016-11-12T19:14:07.248726: step 6946, loss 0.00136496, acc 1
2016-11-12T19:14:07.305674: step 6947, loss 0.00122081, acc 1
2016-11-12T19:14:07.368330: step 6948, loss 0.00200212, acc 1
2016-11-12T19:14:07.428013: step 6949, loss 0.0309285, acc 0.984375
2016-11-12T19:14:07.486573: step 6950, loss 0.00199708, acc 1
2016-11-12T19:14:07.544356: step 6951, loss 0.00177344, acc 1
2016-11-12T19:14:07.600101: step 6952, loss 0.00407636, acc 1
2016-11-12T19:14:07.657133: step 6953, loss 0.010244, acc 1
2016-11-12T19:14:07.714750: step 6954, loss 0.000623787, acc 1
2016-11-12T19:14:07.772939: step 6955, loss 0.0176538, acc 0.984375
2016-11-12T19:14:07.829544: step 6956, loss 0.000601893, acc 1
2016-11-12T19:14:07.886010: step 6957, loss 0.0168973, acc 0.984375
2016-11-12T19:14:07.925072: step 6958, loss 0.000297351, acc 1
2016-11-12T19:14:07.983601: step 6959, loss 0.000874346, acc 1
2016-11-12T19:14:08.039984: step 6960, loss 0.0013896, acc 1
2016-11-12T19:14:08.096818: step 6961, loss 0.00766227, acc 1
2016-11-12T19:14:08.153629: step 6962, loss 0.00194188, acc 1
2016-11-12T19:14:08.213290: step 6963, loss 0.00100842, acc 1
2016-11-12T19:14:08.271913: step 6964, loss 0.00623754, acc 1
2016-11-12T19:14:08.330928: step 6965, loss 0.000117688, acc 1
2016-11-12T19:14:08.389297: step 6966, loss 0.000433751, acc 1
2016-11-12T19:14:08.447981: step 6967, loss 0.000225759, acc 1
2016-11-12T19:14:08.505527: step 6968, loss 0.0096237, acc 1
2016-11-12T19:14:08.563061: step 6969, loss 0.014373, acc 1
2016-11-12T19:14:08.619577: step 6970, loss 0.000129039, acc 1
2016-11-12T19:14:08.676549: step 6971, loss 0.00552369, acc 1
2016-11-12T19:14:08.734528: step 6972, loss 0.000117946, acc 1
2016-11-12T19:14:08.791960: step 6973, loss 0.0127518, acc 1
2016-11-12T19:14:08.850223: step 6974, loss 0.000476893, acc 1
2016-11-12T19:14:08.909328: step 6975, loss 0.000235187, acc 1
2016-11-12T19:14:08.966667: step 6976, loss 7.86015e-05, acc 1
2016-11-12T19:14:09.024903: step 6977, loss 0.00149805, acc 1
2016-11-12T19:14:09.082487: step 6978, loss 0.000301661, acc 1
2016-11-12T19:14:09.139145: step 6979, loss 0.00713152, acc 1
2016-11-12T19:14:09.196940: step 6980, loss 0.0148001, acc 0.984375
2016-11-12T19:14:09.256494: step 6981, loss 0.0205328, acc 0.984375
2016-11-12T19:14:09.315329: step 6982, loss 0.000931893, acc 1
2016-11-12T19:14:09.372995: step 6983, loss 0.00116291, acc 1
2016-11-12T19:14:09.432329: step 6984, loss 0.0384239, acc 0.96875
2016-11-12T19:14:09.490045: step 6985, loss 0.0222406, acc 0.984375
2016-11-12T19:14:09.549094: step 6986, loss 0.00667844, acc 1
2016-11-12T19:14:09.605963: step 6987, loss 0.0038971, acc 1
2016-11-12T19:14:09.664985: step 6988, loss 0.000256982, acc 1
2016-11-12T19:14:09.722687: step 6989, loss 0.000172589, acc 1
2016-11-12T19:14:09.779927: step 6990, loss 0.103414, acc 0.96875
2016-11-12T19:14:09.839333: step 6991, loss 0.00520899, acc 1
2016-11-12T19:14:09.896677: step 6992, loss 0.000153963, acc 1
2016-11-12T19:14:09.952163: step 6993, loss 0.0133405, acc 0.984375
2016-11-12T19:14:10.013318: step 6994, loss 0.00276365, acc 1
2016-11-12T19:14:10.073391: step 6995, loss 0.06628, acc 0.953125
2016-11-12T19:14:10.133230: step 6996, loss 0.000655739, acc 1
2016-11-12T19:14:10.191085: step 6997, loss 0.00310422, acc 1
2016-11-12T19:14:10.249309: step 6998, loss 0.00106377, acc 1
2016-11-12T19:14:10.306042: step 6999, loss 0.00269594, acc 1
2016-11-12T19:14:10.363846: step 7000, loss 0.00242784, acc 1

Evaluation:
2016-11-12T19:14:10.434795: step 7000, loss 3.1148, acc 0.562

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-7000

2016-11-12T19:14:10.928803: step 7001, loss 0.000721294, acc 1
2016-11-12T19:14:10.984979: step 7002, loss 0.000673612, acc 1
2016-11-12T19:14:11.041355: step 7003, loss 0.0224507, acc 0.984375
2016-11-12T19:14:11.101307: step 7004, loss 0.011038, acc 1
2016-11-12T19:14:11.158148: step 7005, loss 0.00433717, acc 1
2016-11-12T19:14:11.214641: step 7006, loss 0.000287359, acc 1
2016-11-12T19:14:11.272307: step 7007, loss 0.00126715, acc 1
2016-11-12T19:14:11.331075: step 7008, loss 0.0317332, acc 0.984375
2016-11-12T19:14:11.389613: step 7009, loss 0.0111495, acc 1
2016-11-12T19:14:11.446353: step 7010, loss 0.0111417, acc 1
2016-11-12T19:14:11.505340: step 7011, loss 0.00313947, acc 1
2016-11-12T19:14:11.561522: step 7012, loss 0.000731759, acc 1
2016-11-12T19:14:11.617470: step 7013, loss 0.0164463, acc 0.984375
2016-11-12T19:14:11.675258: step 7014, loss 0.00629584, acc 1
2016-11-12T19:14:11.733695: step 7015, loss 0.00240357, acc 1
2016-11-12T19:14:11.791127: step 7016, loss 0.000670212, acc 1
2016-11-12T19:14:11.846782: step 7017, loss 0.00261069, acc 1
2016-11-12T19:14:11.905673: step 7018, loss 0.00111837, acc 1
2016-11-12T19:14:11.965635: step 7019, loss 0.0537613, acc 0.984375
2016-11-12T19:14:12.023257: step 7020, loss 0.0600899, acc 0.984375
2016-11-12T19:14:12.082677: step 7021, loss 0.000639949, acc 1
2016-11-12T19:14:12.140980: step 7022, loss 0.00759114, acc 1
2016-11-12T19:14:12.198410: step 7023, loss 8.72483e-05, acc 1
2016-11-12T19:14:12.255149: step 7024, loss 0.000343376, acc 1
2016-11-12T19:14:12.313000: step 7025, loss 0.000859846, acc 1
2016-11-12T19:14:12.372921: step 7026, loss 0.0171161, acc 1
2016-11-12T19:14:12.431010: step 7027, loss 0.000612715, acc 1
2016-11-12T19:14:12.489386: step 7028, loss 0.0261931, acc 0.96875
2016-11-12T19:14:12.527957: step 7029, loss 0.0951297, acc 0.95
2016-11-12T19:14:12.589541: step 7030, loss 0.000244758, acc 1
2016-11-12T19:14:12.647924: step 7031, loss 0.00252363, acc 1
2016-11-12T19:14:12.706917: step 7032, loss 0.000425574, acc 1
2016-11-12T19:14:12.765044: step 7033, loss 0.00104317, acc 1
2016-11-12T19:14:12.822757: step 7034, loss 0.033794, acc 0.96875
2016-11-12T19:14:12.880369: step 7035, loss 0.000563386, acc 1
2016-11-12T19:14:12.936482: step 7036, loss 0.000803613, acc 1
2016-11-12T19:14:12.993380: step 7037, loss 0.00294395, acc 1
2016-11-12T19:14:13.049090: step 7038, loss 0.00840848, acc 1
2016-11-12T19:14:13.105935: step 7039, loss 0.00117916, acc 1
2016-11-12T19:14:13.165132: step 7040, loss 0.0138512, acc 1
2016-11-12T19:14:13.225390: step 7041, loss 0.000441339, acc 1
2016-11-12T19:14:13.281264: step 7042, loss 0.000818816, acc 1
2016-11-12T19:14:13.337081: step 7043, loss 0.00244175, acc 1
2016-11-12T19:14:13.394762: step 7044, loss 0.00393057, acc 1
2016-11-12T19:14:13.452272: step 7045, loss 0.0230217, acc 0.984375
2016-11-12T19:14:13.510936: step 7046, loss 0.00109325, acc 1
2016-11-12T19:14:13.569258: step 7047, loss 0.00103461, acc 1
2016-11-12T19:14:13.625241: step 7048, loss 0.00158311, acc 1
2016-11-12T19:14:13.681492: step 7049, loss 0.00223015, acc 1
2016-11-12T19:14:13.739485: step 7050, loss 0.00538896, acc 1
2016-11-12T19:14:13.797342: step 7051, loss 0.0214285, acc 0.984375
2016-11-12T19:14:13.857236: step 7052, loss 0.00125762, acc 1
2016-11-12T19:14:13.921319: step 7053, loss 0.00292433, acc 1
2016-11-12T19:14:13.980953: step 7054, loss 0.123254, acc 0.984375
2016-11-12T19:14:14.040308: step 7055, loss 0.0162095, acc 0.984375
2016-11-12T19:14:14.099997: step 7056, loss 0.00156797, acc 1
2016-11-12T19:14:14.157004: step 7057, loss 0.000450257, acc 1
2016-11-12T19:14:14.213885: step 7058, loss 0.00741575, acc 1
2016-11-12T19:14:14.272342: step 7059, loss 0.00784234, acc 1
2016-11-12T19:14:14.328928: step 7060, loss 0.0177425, acc 0.984375
2016-11-12T19:14:14.386534: step 7061, loss 0.0215498, acc 0.984375
2016-11-12T19:14:14.448209: step 7062, loss 0.0483278, acc 0.96875
2016-11-12T19:14:14.505646: step 7063, loss 0.000439679, acc 1
2016-11-12T19:14:14.562571: step 7064, loss 0.0104419, acc 1
2016-11-12T19:14:14.619960: step 7065, loss 0.000391757, acc 1
2016-11-12T19:14:14.677405: step 7066, loss 0.00200057, acc 1
2016-11-12T19:14:14.736514: step 7067, loss 0.00128181, acc 1
2016-11-12T19:14:14.792887: step 7068, loss 0.00320745, acc 1
2016-11-12T19:14:14.849264: step 7069, loss 0.0249655, acc 0.984375
2016-11-12T19:14:14.910572: step 7070, loss 0.000644379, acc 1
2016-11-12T19:14:14.966493: step 7071, loss 0.00429344, acc 1
2016-11-12T19:14:15.022551: step 7072, loss 0.0179509, acc 0.984375
2016-11-12T19:14:15.081068: step 7073, loss 0.00144215, acc 1
2016-11-12T19:14:15.137091: step 7074, loss 0.000685605, acc 1
2016-11-12T19:14:15.196629: step 7075, loss 0.00125587, acc 1
2016-11-12T19:14:15.253582: step 7076, loss 0.00428209, acc 1
2016-11-12T19:14:15.311026: step 7077, loss 0.00379303, acc 1
2016-11-12T19:14:15.370325: step 7078, loss 0.000739444, acc 1
2016-11-12T19:14:15.429499: step 7079, loss 0.000898491, acc 1
2016-11-12T19:14:15.485936: step 7080, loss 0.0166183, acc 0.984375
2016-11-12T19:14:15.544314: step 7081, loss 0.00105854, acc 1
2016-11-12T19:14:15.603086: step 7082, loss 0.00275742, acc 1
2016-11-12T19:14:15.661560: step 7083, loss 0.000926405, acc 1
2016-11-12T19:14:15.720963: step 7084, loss 0.0141753, acc 0.984375
2016-11-12T19:14:15.778755: step 7085, loss 0.000337732, acc 1
2016-11-12T19:14:15.836029: step 7086, loss 0.0151557, acc 1
2016-11-12T19:14:15.893146: step 7087, loss 0.0135887, acc 0.984375
2016-11-12T19:14:15.953732: step 7088, loss 0.000994118, acc 1
2016-11-12T19:14:16.010360: step 7089, loss 0.00188331, acc 1
2016-11-12T19:14:16.066749: step 7090, loss 0.00158222, acc 1
2016-11-12T19:14:16.123250: step 7091, loss 0.0705617, acc 0.984375
2016-11-12T19:14:16.181391: step 7092, loss 0.00953755, acc 1
2016-11-12T19:14:16.237660: step 7093, loss 0.0436977, acc 0.984375
2016-11-12T19:14:16.295522: step 7094, loss 0.0287615, acc 0.984375
2016-11-12T19:14:16.356982: step 7095, loss 0.00222221, acc 1
2016-11-12T19:14:16.416904: step 7096, loss 0.00312288, acc 1
2016-11-12T19:14:16.473958: step 7097, loss 0.00766298, acc 1
2016-11-12T19:14:16.530836: step 7098, loss 0.000272733, acc 1
2016-11-12T19:14:16.586709: step 7099, loss 0.0300805, acc 0.984375
2016-11-12T19:14:16.626767: step 7100, loss 0.000804536, acc 1

Evaluation:
2016-11-12T19:14:16.698112: step 7100, loss 3.34268, acc 0.554

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-7100

2016-11-12T19:14:17.187761: step 7101, loss 0.00429968, acc 1
2016-11-12T19:14:17.245136: step 7102, loss 0.0828259, acc 0.984375
2016-11-12T19:14:17.302635: step 7103, loss 0.00109393, acc 1
2016-11-12T19:14:17.361726: step 7104, loss 0.01096, acc 1
2016-11-12T19:14:17.421084: step 7105, loss 0.00666289, acc 1
2016-11-12T19:14:17.477567: step 7106, loss 0.00587294, acc 1
2016-11-12T19:14:17.534766: step 7107, loss 0.0426829, acc 0.96875
2016-11-12T19:14:17.594699: step 7108, loss 0.0205178, acc 0.984375
2016-11-12T19:14:17.651796: step 7109, loss 0.316069, acc 0.953125
2016-11-12T19:14:17.712741: step 7110, loss 0.00134925, acc 1
2016-11-12T19:14:17.769926: step 7111, loss 0.000964188, acc 1
2016-11-12T19:14:17.828912: step 7112, loss 0.0331119, acc 0.96875
2016-11-12T19:14:17.886944: step 7113, loss 0.0226992, acc 0.984375
2016-11-12T19:14:17.944365: step 7114, loss 0.0056588, acc 1
2016-11-12T19:14:18.004518: step 7115, loss 0.0102676, acc 1
2016-11-12T19:14:18.063334: step 7116, loss 0.122111, acc 0.984375
2016-11-12T19:14:18.122036: step 7117, loss 0.00256495, acc 1
2016-11-12T19:14:18.179225: step 7118, loss 0.0225849, acc 0.984375
2016-11-12T19:14:18.236713: step 7119, loss 0.0035704, acc 1
2016-11-12T19:14:18.297093: step 7120, loss 0.002262, acc 1
2016-11-12T19:14:18.356399: step 7121, loss 0.0140197, acc 0.984375
2016-11-12T19:14:18.414063: step 7122, loss 0.00697849, acc 1
2016-11-12T19:14:18.473764: step 7123, loss 0.00179784, acc 1
2016-11-12T19:14:18.530385: step 7124, loss 0.0222388, acc 0.984375
2016-11-12T19:14:18.587900: step 7125, loss 0.00610364, acc 1
2016-11-12T19:14:18.645473: step 7126, loss 0.0184853, acc 0.984375
2016-11-12T19:14:18.703992: step 7127, loss 0.00713633, acc 1
2016-11-12T19:14:18.762021: step 7128, loss 0.00101481, acc 1
2016-11-12T19:14:18.820997: step 7129, loss 0.00195194, acc 1
2016-11-12T19:14:18.878429: step 7130, loss 0.00346758, acc 1
2016-11-12T19:14:18.935614: step 7131, loss 0.0556389, acc 0.96875
2016-11-12T19:14:18.993689: step 7132, loss 0.00197325, acc 1
2016-11-12T19:14:19.052110: step 7133, loss 0.0133957, acc 0.984375
2016-11-12T19:14:19.108984: step 7134, loss 0.0248046, acc 0.984375
2016-11-12T19:14:19.168501: step 7135, loss 0.0277159, acc 0.984375
2016-11-12T19:14:19.229832: step 7136, loss 0.00914419, acc 1
2016-11-12T19:14:19.287503: step 7137, loss 0.00144584, acc 1
2016-11-12T19:14:19.344167: step 7138, loss 0.000161011, acc 1
2016-11-12T19:14:19.399966: step 7139, loss 0.000454255, acc 1
2016-11-12T19:14:19.457425: step 7140, loss 0.0014779, acc 1
2016-11-12T19:14:19.517309: step 7141, loss 0.00777929, acc 1
2016-11-12T19:14:19.577225: step 7142, loss 0.0159849, acc 1
2016-11-12T19:14:19.636054: step 7143, loss 0.0310463, acc 0.984375
2016-11-12T19:14:19.697052: step 7144, loss 0.0398155, acc 0.984375
2016-11-12T19:14:19.754771: step 7145, loss 0.0150024, acc 1
2016-11-12T19:14:19.812968: step 7146, loss 0.000977818, acc 1
2016-11-12T19:14:19.874482: step 7147, loss 0.0122242, acc 0.984375
2016-11-12T19:14:19.932879: step 7148, loss 0.00439838, acc 1
2016-11-12T19:14:19.990593: step 7149, loss 0.0389556, acc 0.984375
2016-11-12T19:14:20.049276: step 7150, loss 0.014869, acc 1
2016-11-12T19:14:20.106417: step 7151, loss 0.0212606, acc 0.984375
2016-11-12T19:14:20.164070: step 7152, loss 0.000611789, acc 1
2016-11-12T19:14:20.220510: step 7153, loss 0.000718938, acc 1
2016-11-12T19:14:20.277050: step 7154, loss 0.000706387, acc 1
2016-11-12T19:14:20.335705: step 7155, loss 0.000760842, acc 1
2016-11-12T19:14:20.393592: step 7156, loss 0.00836511, acc 1
2016-11-12T19:14:20.450866: step 7157, loss 0.00461125, acc 1
2016-11-12T19:14:20.509123: step 7158, loss 0.000310663, acc 1
2016-11-12T19:14:20.565148: step 7159, loss 0.00139164, acc 1
2016-11-12T19:14:20.622047: step 7160, loss 0.0151208, acc 0.984375
2016-11-12T19:14:20.679260: step 7161, loss 0.000420499, acc 1
2016-11-12T19:14:20.736524: step 7162, loss 0.0166053, acc 0.984375
2016-11-12T19:14:20.795367: step 7163, loss 0.00905293, acc 1
2016-11-12T19:14:20.852038: step 7164, loss 0.00107851, acc 1
2016-11-12T19:14:20.910166: step 7165, loss 0.00995898, acc 1
2016-11-12T19:14:20.969141: step 7166, loss 0.0170142, acc 0.984375
2016-11-12T19:14:21.028290: step 7167, loss 0.0037557, acc 1
2016-11-12T19:14:21.084659: step 7168, loss 0.00079774, acc 1
2016-11-12T19:14:21.141978: step 7169, loss 0.00753591, acc 1
2016-11-12T19:14:21.199325: step 7170, loss 0.000448755, acc 1
2016-11-12T19:14:21.237177: step 7171, loss 1.38397e-05, acc 1
2016-11-12T19:14:21.295092: step 7172, loss 0.00152667, acc 1
2016-11-12T19:14:21.352341: step 7173, loss 0.00229226, acc 1
2016-11-12T19:14:21.409195: step 7174, loss 0.00421144, acc 1
2016-11-12T19:14:21.467666: step 7175, loss 0.000219967, acc 1
2016-11-12T19:14:21.523935: step 7176, loss 0.00113318, acc 1
2016-11-12T19:14:21.581096: step 7177, loss 0.00114158, acc 1
2016-11-12T19:14:21.644469: step 7178, loss 0.000134127, acc 1
2016-11-12T19:14:21.699362: step 7179, loss 0.00121709, acc 1
2016-11-12T19:14:21.756882: step 7180, loss 0.00234803, acc 1
2016-11-12T19:14:21.814452: step 7181, loss 0.00150558, acc 1
2016-11-12T19:14:21.872804: step 7182, loss 0.000607624, acc 1
2016-11-12T19:14:21.929875: step 7183, loss 0.0016692, acc 1
2016-11-12T19:14:21.988101: step 7184, loss 0.0121298, acc 0.984375
2016-11-12T19:14:22.045686: step 7185, loss 0.000657134, acc 1
2016-11-12T19:14:22.102603: step 7186, loss 0.0102053, acc 1
2016-11-12T19:14:22.159830: step 7187, loss 0.128373, acc 0.96875
2016-11-12T19:14:22.221942: step 7188, loss 0.0100087, acc 1
2016-11-12T19:14:22.280677: step 7189, loss 0.000610634, acc 1
2016-11-12T19:14:22.336286: step 7190, loss 0.00636795, acc 1
2016-11-12T19:14:22.393707: step 7191, loss 0.0188891, acc 0.984375
2016-11-12T19:14:22.453632: step 7192, loss 0.00640892, acc 1
2016-11-12T19:14:22.511734: step 7193, loss 0.0206639, acc 0.984375
2016-11-12T19:14:22.569939: step 7194, loss 0.00178426, acc 1
2016-11-12T19:14:22.626282: step 7195, loss 0.00455185, acc 1
2016-11-12T19:14:22.685019: step 7196, loss 0.000312613, acc 1
2016-11-12T19:14:22.740876: step 7197, loss 0.00242406, acc 1
2016-11-12T19:14:22.796747: step 7198, loss 0.0170148, acc 0.984375
2016-11-12T19:14:22.856045: step 7199, loss 4.73723e-05, acc 1
2016-11-12T19:14:22.913193: step 7200, loss 0.00587302, acc 1

Evaluation:
2016-11-12T19:14:22.985411: step 7200, loss 3.16537, acc 0.572

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-7200

2016-11-12T19:14:23.477188: step 7201, loss 0.00126546, acc 1
2016-11-12T19:14:23.536700: step 7202, loss 0.0130521, acc 0.984375
2016-11-12T19:14:23.594045: step 7203, loss 0.00209201, acc 1
2016-11-12T19:14:23.650966: step 7204, loss 0.00851705, acc 1
2016-11-12T19:14:23.709373: step 7205, loss 0.0135386, acc 0.984375
2016-11-12T19:14:23.769051: step 7206, loss 0.0112016, acc 1
2016-11-12T19:14:23.825423: step 7207, loss 0.00186029, acc 1
2016-11-12T19:14:23.882957: step 7208, loss 0.00248273, acc 1
2016-11-12T19:14:23.941070: step 7209, loss 0.0100889, acc 1
2016-11-12T19:14:24.000772: step 7210, loss 0.00149638, acc 1
2016-11-12T19:14:24.059719: step 7211, loss 0.00139815, acc 1
2016-11-12T19:14:24.116658: step 7212, loss 0.00109477, acc 1
2016-11-12T19:14:24.176502: step 7213, loss 0.0223187, acc 0.984375
2016-11-12T19:14:24.236668: step 7214, loss 0.000328035, acc 1
2016-11-12T19:14:24.293694: step 7215, loss 0.000854229, acc 1
2016-11-12T19:14:24.349042: step 7216, loss 0.0138248, acc 0.984375
2016-11-12T19:14:24.405476: step 7217, loss 0.00221662, acc 1
2016-11-12T19:14:24.463172: step 7218, loss 0.00226243, acc 1
2016-11-12T19:14:24.520435: step 7219, loss 0.0131855, acc 0.984375
2016-11-12T19:14:24.581119: step 7220, loss 0.00143045, acc 1
2016-11-12T19:14:24.638770: step 7221, loss 0.000808766, acc 1
2016-11-12T19:14:24.696516: step 7222, loss 0.026815, acc 0.96875
2016-11-12T19:14:24.753609: step 7223, loss 0.00339644, acc 1
2016-11-12T19:14:24.812556: step 7224, loss 0.00203495, acc 1
2016-11-12T19:14:24.869205: step 7225, loss 0.000582416, acc 1
2016-11-12T19:14:24.928387: step 7226, loss 0.000325886, acc 1
2016-11-12T19:14:24.984361: step 7227, loss 0.0017007, acc 1
2016-11-12T19:14:25.041706: step 7228, loss 0.00172955, acc 1
2016-11-12T19:14:25.099421: step 7229, loss 0.000485172, acc 1
2016-11-12T19:14:25.157124: step 7230, loss 0.00076833, acc 1
2016-11-12T19:14:25.214145: step 7231, loss 0.0742653, acc 0.96875
2016-11-12T19:14:25.273443: step 7232, loss 0.00056446, acc 1
2016-11-12T19:14:25.331386: step 7233, loss 0.000310472, acc 1
2016-11-12T19:14:25.387616: step 7234, loss 0.00947372, acc 1
2016-11-12T19:14:25.445290: step 7235, loss 0.000216948, acc 1
2016-11-12T19:14:25.503977: step 7236, loss 0.00487041, acc 1
2016-11-12T19:14:25.560978: step 7237, loss 0.0159244, acc 1
2016-11-12T19:14:25.621850: step 7238, loss 0.0462297, acc 0.96875
2016-11-12T19:14:25.680056: step 7239, loss 0.00372531, acc 1
2016-11-12T19:14:25.736517: step 7240, loss 0.00116859, acc 1
2016-11-12T19:14:25.794300: step 7241, loss 0.0422592, acc 0.984375
2016-11-12T19:14:25.835985: step 7242, loss 1.9133e-06, acc 1
2016-11-12T19:14:25.897025: step 7243, loss 0.00135098, acc 1
2016-11-12T19:14:25.954029: step 7244, loss 0.000564429, acc 1
2016-11-12T19:14:26.012765: step 7245, loss 0.0182768, acc 0.984375
2016-11-12T19:14:26.070817: step 7246, loss 0.000345792, acc 1
2016-11-12T19:14:26.129139: step 7247, loss 7.38494e-05, acc 1
2016-11-12T19:14:26.185411: step 7248, loss 0.000333427, acc 1
2016-11-12T19:14:26.242638: step 7249, loss 0.0107491, acc 1
2016-11-12T19:14:26.301028: step 7250, loss 0.00137484, acc 1
2016-11-12T19:14:26.360476: step 7251, loss 0.00066816, acc 1
2016-11-12T19:14:26.417529: step 7252, loss 0.00144105, acc 1
2016-11-12T19:14:26.477183: step 7253, loss 0.00641906, acc 1
2016-11-12T19:14:26.537519: step 7254, loss 0.0161233, acc 0.984375
2016-11-12T19:14:26.595485: step 7255, loss 0.000742095, acc 1
2016-11-12T19:14:26.653166: step 7256, loss 0.00425932, acc 1
2016-11-12T19:14:26.710269: step 7257, loss 0.00177718, acc 1
2016-11-12T19:14:26.769143: step 7258, loss 0.0047016, acc 1
2016-11-12T19:14:26.828196: step 7259, loss 0.000291867, acc 1
2016-11-12T19:14:26.884528: step 7260, loss 0.00024549, acc 1
2016-11-12T19:14:26.941213: step 7261, loss 0.0123522, acc 0.984375
2016-11-12T19:14:26.998469: step 7262, loss 0.0050745, acc 1
2016-11-12T19:14:27.056447: step 7263, loss 0.000335309, acc 1
2016-11-12T19:14:27.113540: step 7264, loss 0.00821348, acc 1
2016-11-12T19:14:27.172013: step 7265, loss 0.0215146, acc 0.984375
2016-11-12T19:14:27.228934: step 7266, loss 0.00415629, acc 1
2016-11-12T19:14:27.286097: step 7267, loss 0.0966955, acc 0.984375
2016-11-12T19:14:27.345711: step 7268, loss 0.000950243, acc 1
2016-11-12T19:14:27.404325: step 7269, loss 0.0253288, acc 0.984375
2016-11-12T19:14:27.460681: step 7270, loss 0.00270611, acc 1
2016-11-12T19:14:27.516836: step 7271, loss 0.00338448, acc 1
2016-11-12T19:14:27.573992: step 7272, loss 0.00799627, acc 1
2016-11-12T19:14:27.631670: step 7273, loss 0.00220759, acc 1
2016-11-12T19:14:27.690383: step 7274, loss 0.00181616, acc 1
2016-11-12T19:14:27.747922: step 7275, loss 0.000329747, acc 1
2016-11-12T19:14:27.803937: step 7276, loss 0.040969, acc 0.96875
2016-11-12T19:14:27.861038: step 7277, loss 0.000113046, acc 1
2016-11-12T19:14:27.918505: step 7278, loss 0.00124005, acc 1
2016-11-12T19:14:27.976214: step 7279, loss 0.0058251, acc 1
2016-11-12T19:14:28.032903: step 7280, loss 0.000196904, acc 1
2016-11-12T19:14:28.089158: step 7281, loss 0.015844, acc 1
2016-11-12T19:14:28.148630: step 7282, loss 0.0119046, acc 0.984375
2016-11-12T19:14:28.208687: step 7283, loss 0.00623278, acc 1
2016-11-12T19:14:28.269148: step 7284, loss 0.00121726, acc 1
2016-11-12T19:14:28.326789: step 7285, loss 0.0395583, acc 0.984375
2016-11-12T19:14:28.385353: step 7286, loss 0.00195835, acc 1
2016-11-12T19:14:28.443285: step 7287, loss 0.0327418, acc 0.984375
2016-11-12T19:14:28.501496: step 7288, loss 0.0198584, acc 0.984375
2016-11-12T19:14:28.561363: step 7289, loss 0.0157952, acc 0.984375
2016-11-12T19:14:28.619097: step 7290, loss 0.000628811, acc 1
2016-11-12T19:14:28.677638: step 7291, loss 0.00586074, acc 1
2016-11-12T19:14:28.735865: step 7292, loss 0.00213427, acc 1
2016-11-12T19:14:28.795274: step 7293, loss 0.00823072, acc 1
2016-11-12T19:14:28.853056: step 7294, loss 0.00898944, acc 1
2016-11-12T19:14:28.910728: step 7295, loss 0.02659, acc 0.96875
2016-11-12T19:14:28.969387: step 7296, loss 0.00110355, acc 1
2016-11-12T19:14:29.028986: step 7297, loss 0.00626722, acc 1
2016-11-12T19:14:29.089308: step 7298, loss 0.00183694, acc 1
2016-11-12T19:14:29.145829: step 7299, loss 0.000201139, acc 1
2016-11-12T19:14:29.205186: step 7300, loss 0.00103685, acc 1

Evaluation:
2016-11-12T19:14:29.276181: step 7300, loss 3.25283, acc 0.562

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-7300

2016-11-12T19:14:29.765636: step 7301, loss 0.000301764, acc 1
2016-11-12T19:14:29.824855: step 7302, loss 0.00519816, acc 1
2016-11-12T19:14:29.882231: step 7303, loss 0.0621338, acc 0.984375
2016-11-12T19:14:29.940784: step 7304, loss 0.00109027, acc 1
2016-11-12T19:14:29.998476: step 7305, loss 8.603e-05, acc 1
2016-11-12T19:14:30.054829: step 7306, loss 0.031627, acc 0.984375
2016-11-12T19:14:30.113050: step 7307, loss 0.00138377, acc 1
2016-11-12T19:14:30.170565: step 7308, loss 0.0003828, acc 1
2016-11-12T19:14:30.225834: step 7309, loss 0.00219155, acc 1
2016-11-12T19:14:30.284781: step 7310, loss 0.0154201, acc 0.984375
2016-11-12T19:14:30.346129: step 7311, loss 0.0107526, acc 1
2016-11-12T19:14:30.405640: step 7312, loss 0.00948125, acc 1
2016-11-12T19:14:30.446205: step 7313, loss 0.00153774, acc 1
2016-11-12T19:14:30.505013: step 7314, loss 0.00112709, acc 1
2016-11-12T19:14:30.561805: step 7315, loss 0.0022024, acc 1
2016-11-12T19:14:30.619574: step 7316, loss 0.0299917, acc 0.984375
2016-11-12T19:14:30.681079: step 7317, loss 0.00116569, acc 1
2016-11-12T19:14:30.737064: step 7318, loss 0.000131604, acc 1
2016-11-12T19:14:30.796045: step 7319, loss 0.0215681, acc 0.984375
2016-11-12T19:14:30.857380: step 7320, loss 7.31595e-05, acc 1
2016-11-12T19:14:30.916374: step 7321, loss 0.0270043, acc 0.984375
2016-11-12T19:14:30.975390: step 7322, loss 0.000362476, acc 1
2016-11-12T19:14:31.032619: step 7323, loss 0.000327695, acc 1
2016-11-12T19:14:31.090807: step 7324, loss 0.0337491, acc 0.984375
2016-11-12T19:14:31.150469: step 7325, loss 0.00216035, acc 1
2016-11-12T19:14:31.209779: step 7326, loss 0.0140784, acc 0.984375
2016-11-12T19:14:31.268633: step 7327, loss 0.0132581, acc 1
2016-11-12T19:14:31.326417: step 7328, loss 0.00104936, acc 1
2016-11-12T19:14:31.383831: step 7329, loss 0.000231469, acc 1
2016-11-12T19:14:31.441295: step 7330, loss 0.00100997, acc 1
2016-11-12T19:14:31.498856: step 7331, loss 0.00106089, acc 1
2016-11-12T19:14:31.556347: step 7332, loss 0.00112565, acc 1
2016-11-12T19:14:31.613650: step 7333, loss 0.000226792, acc 1
2016-11-12T19:14:31.673125: step 7334, loss 0.000531331, acc 1
2016-11-12T19:14:31.729882: step 7335, loss 0.0065795, acc 1
2016-11-12T19:14:31.789660: step 7336, loss 0.0177075, acc 0.984375
2016-11-12T19:14:31.847523: step 7337, loss 0.0161129, acc 1
2016-11-12T19:14:31.904538: step 7338, loss 0.0299087, acc 0.984375
2016-11-12T19:14:31.963691: step 7339, loss 0.00388272, acc 1
2016-11-12T19:14:32.021087: step 7340, loss 0.0224865, acc 0.984375
2016-11-12T19:14:32.079122: step 7341, loss 0.000502911, acc 1
2016-11-12T19:14:32.135062: step 7342, loss 0.0648398, acc 0.96875
2016-11-12T19:14:32.193216: step 7343, loss 0.00523149, acc 1
2016-11-12T19:14:32.250470: step 7344, loss 0.000509845, acc 1
2016-11-12T19:14:32.306912: step 7345, loss 8.7063e-05, acc 1
2016-11-12T19:14:32.363615: step 7346, loss 0.000161369, acc 1
2016-11-12T19:14:32.419115: step 7347, loss 0.00623112, acc 1
2016-11-12T19:14:32.478871: step 7348, loss 0.000162081, acc 1
2016-11-12T19:14:32.536429: step 7349, loss 0.00112037, acc 1
2016-11-12T19:14:32.596072: step 7350, loss 0.0123499, acc 1
2016-11-12T19:14:32.653405: step 7351, loss 0.0190069, acc 0.984375
2016-11-12T19:14:32.710356: step 7352, loss 0.000953166, acc 1
2016-11-12T19:14:32.768870: step 7353, loss 0.0129165, acc 0.984375
2016-11-12T19:14:32.828228: step 7354, loss 0.0175428, acc 0.984375
2016-11-12T19:14:32.886049: step 7355, loss 0.00141896, acc 1
2016-11-12T19:14:32.943521: step 7356, loss 0.000268693, acc 1
2016-11-12T19:14:33.001195: step 7357, loss 0.0548181, acc 0.984375
2016-11-12T19:14:33.060479: step 7358, loss 0.0388804, acc 0.984375
2016-11-12T19:14:33.119965: step 7359, loss 0.147166, acc 0.984375
2016-11-12T19:14:33.178829: step 7360, loss 0.00244465, acc 1
2016-11-12T19:14:33.238129: step 7361, loss 0.0311017, acc 0.984375
2016-11-12T19:14:33.296710: step 7362, loss 0.00175713, acc 1
2016-11-12T19:14:33.355030: step 7363, loss 0.000578352, acc 1
2016-11-12T19:14:33.413252: step 7364, loss 0.0201771, acc 0.984375
2016-11-12T19:14:33.470523: step 7365, loss 0.015321, acc 1
2016-11-12T19:14:33.528996: step 7366, loss 0.0292631, acc 0.984375
2016-11-12T19:14:33.585924: step 7367, loss 0.000423358, acc 1
2016-11-12T19:14:33.643282: step 7368, loss 0.00164588, acc 1
2016-11-12T19:14:33.701097: step 7369, loss 0.00267162, acc 1
2016-11-12T19:14:33.758464: step 7370, loss 0.00636866, acc 1
2016-11-12T19:14:33.815839: step 7371, loss 0.000251764, acc 1
2016-11-12T19:14:33.871612: step 7372, loss 0.0121311, acc 0.984375
2016-11-12T19:14:33.927611: step 7373, loss 0.00411078, acc 1
2016-11-12T19:14:33.988721: step 7374, loss 0.0169414, acc 0.984375
2016-11-12T19:14:34.049189: step 7375, loss 0.000613222, acc 1
2016-11-12T19:14:34.105278: step 7376, loss 0.00114033, acc 1
2016-11-12T19:14:34.163611: step 7377, loss 0.00177244, acc 1
2016-11-12T19:14:34.221471: step 7378, loss 0.0107169, acc 1
2016-11-12T19:14:34.281473: step 7379, loss 0.00221592, acc 1
2016-11-12T19:14:34.339413: step 7380, loss 0.0130662, acc 0.984375
2016-11-12T19:14:34.396802: step 7381, loss 0.00110546, acc 1
2016-11-12T19:14:34.453867: step 7382, loss 0.017093, acc 0.984375
2016-11-12T19:14:34.512301: step 7383, loss 0.00113706, acc 1
2016-11-12T19:14:34.552174: step 7384, loss 0.00305817, acc 1
2016-11-12T19:14:34.611441: step 7385, loss 0.000305525, acc 1
2016-11-12T19:14:34.668746: step 7386, loss 0.00183159, acc 1
2016-11-12T19:14:34.729138: step 7387, loss 0.000423081, acc 1
2016-11-12T19:14:34.786991: step 7388, loss 0.00429998, acc 1
2016-11-12T19:14:34.845207: step 7389, loss 0.001691, acc 1
2016-11-12T19:14:34.903681: step 7390, loss 0.000967112, acc 1
2016-11-12T19:14:34.959703: step 7391, loss 0.0058277, acc 1
2016-11-12T19:14:35.017104: step 7392, loss 0.000926996, acc 1
2016-11-12T19:14:35.073220: step 7393, loss 0.00497004, acc 1
2016-11-12T19:14:35.131211: step 7394, loss 0.000138631, acc 1
2016-11-12T19:14:35.189291: step 7395, loss 0.00055112, acc 1
2016-11-12T19:14:35.246842: step 7396, loss 0.0176411, acc 0.984375
2016-11-12T19:14:35.305260: step 7397, loss 0.000929396, acc 1
2016-11-12T19:14:35.364263: step 7398, loss 0.000515819, acc 1
2016-11-12T19:14:35.421172: step 7399, loss 0.00317841, acc 1
2016-11-12T19:14:35.477899: step 7400, loss 0.00533246, acc 1

Evaluation:
2016-11-12T19:14:35.549368: step 7400, loss 3.26234, acc 0.58

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-7400

2016-11-12T19:14:36.038797: step 7401, loss 0.0326133, acc 0.984375
2016-11-12T19:14:36.096907: step 7402, loss 0.0105801, acc 1
2016-11-12T19:14:36.157139: step 7403, loss 0.000319266, acc 1
2016-11-12T19:14:36.214483: step 7404, loss 0.000213424, acc 1
2016-11-12T19:14:36.273478: step 7405, loss 0.00150793, acc 1
2016-11-12T19:14:36.332657: step 7406, loss 0.025893, acc 0.984375
2016-11-12T19:14:36.390136: step 7407, loss 0.0449894, acc 0.984375
2016-11-12T19:14:36.449303: step 7408, loss 0.00126134, acc 1
2016-11-12T19:14:36.509092: step 7409, loss 0.000458165, acc 1
2016-11-12T19:14:36.567632: step 7410, loss 0.000249687, acc 1
2016-11-12T19:14:36.624854: step 7411, loss 0.0179479, acc 1
2016-11-12T19:14:36.682463: step 7412, loss 0.000501371, acc 1
2016-11-12T19:14:36.744089: step 7413, loss 0.0172677, acc 0.984375
2016-11-12T19:14:36.801377: step 7414, loss 0.0110447, acc 1
2016-11-12T19:14:36.860875: step 7415, loss 0.00800306, acc 1
2016-11-12T19:14:36.917426: step 7416, loss 0.00868064, acc 1
2016-11-12T19:14:36.976197: step 7417, loss 0.00236468, acc 1
2016-11-12T19:14:37.032947: step 7418, loss 0.0168509, acc 0.984375
2016-11-12T19:14:37.091661: step 7419, loss 0.0236902, acc 0.984375
2016-11-12T19:14:37.150810: step 7420, loss 0.0027163, acc 1
2016-11-12T19:14:37.209047: step 7421, loss 0.000592746, acc 1
2016-11-12T19:14:37.265869: step 7422, loss 0.000432268, acc 1
2016-11-12T19:14:37.322920: step 7423, loss 0.000743701, acc 1
2016-11-12T19:14:37.380799: step 7424, loss 0.000651965, acc 1
2016-11-12T19:14:37.438797: step 7425, loss 0.00284017, acc 1
2016-11-12T19:14:37.497133: step 7426, loss 0.000335742, acc 1
2016-11-12T19:14:37.553879: step 7427, loss 0.000290768, acc 1
2016-11-12T19:14:37.609185: step 7428, loss 0.000157122, acc 1
2016-11-12T19:14:37.665176: step 7429, loss 0.00178623, acc 1
2016-11-12T19:14:37.721727: step 7430, loss 0.00195309, acc 1
2016-11-12T19:14:37.781023: step 7431, loss 0.00265352, acc 1
2016-11-12T19:14:37.837070: step 7432, loss 0.0329697, acc 0.984375
2016-11-12T19:14:37.895874: step 7433, loss 0.0111818, acc 1
2016-11-12T19:14:37.953004: step 7434, loss 0.0325065, acc 0.984375
2016-11-12T19:14:38.010700: step 7435, loss 0.000449081, acc 1
2016-11-12T19:14:38.068965: step 7436, loss 0.0240326, acc 0.984375
2016-11-12T19:14:38.129158: step 7437, loss 0.00836882, acc 1
2016-11-12T19:14:38.189705: step 7438, loss 0.00199471, acc 1
2016-11-12T19:14:38.247061: step 7439, loss 0.00130089, acc 1
2016-11-12T19:14:38.303538: step 7440, loss 0.0016432, acc 1
2016-11-12T19:14:38.363751: step 7441, loss 0.000126761, acc 1
2016-11-12T19:14:38.420892: step 7442, loss 0.00538803, acc 1
2016-11-12T19:14:38.479265: step 7443, loss 0.00164791, acc 1
2016-11-12T19:14:38.538354: step 7444, loss 0.0288151, acc 0.984375
2016-11-12T19:14:38.597207: step 7445, loss 0.00478595, acc 1
2016-11-12T19:14:38.655744: step 7446, loss 0.000853844, acc 1
2016-11-12T19:14:38.713913: step 7447, loss 0.0537198, acc 0.984375
2016-11-12T19:14:38.772926: step 7448, loss 0.0328517, acc 0.984375
2016-11-12T19:14:38.832357: step 7449, loss 0.0290101, acc 0.984375
2016-11-12T19:14:38.893201: step 7450, loss 0.00166198, acc 1
2016-11-12T19:14:38.951937: step 7451, loss 0.00576285, acc 1
2016-11-12T19:14:39.009020: step 7452, loss 0.000152497, acc 1
2016-11-12T19:14:39.066398: step 7453, loss 0.279205, acc 0.984375
2016-11-12T19:14:39.126392: step 7454, loss 0.000825248, acc 1
2016-11-12T19:14:39.167230: step 7455, loss 0.000589244, acc 1
2016-11-12T19:14:39.227215: step 7456, loss 0.000447759, acc 1
2016-11-12T19:14:39.283614: step 7457, loss 0.000578747, acc 1
2016-11-12T19:14:39.341861: step 7458, loss 0.00107956, acc 1
2016-11-12T19:14:39.398908: step 7459, loss 0.00203413, acc 1
2016-11-12T19:14:39.457667: step 7460, loss 0.0002603, acc 1
2016-11-12T19:14:39.513789: step 7461, loss 0.000513139, acc 1
2016-11-12T19:14:39.572515: step 7462, loss 0.000386591, acc 1
2016-11-12T19:14:39.632631: step 7463, loss 0.0301408, acc 0.984375
2016-11-12T19:14:39.691281: step 7464, loss 0.00263626, acc 1
2016-11-12T19:14:39.747568: step 7465, loss 0.000210165, acc 1
2016-11-12T19:14:39.805353: step 7466, loss 0.00322887, acc 1
2016-11-12T19:14:39.866117: step 7467, loss 0.0372068, acc 0.984375
2016-11-12T19:14:39.923451: step 7468, loss 0.0147372, acc 0.984375
2016-11-12T19:14:39.983404: step 7469, loss 0.0016601, acc 1
2016-11-12T19:14:40.040352: step 7470, loss 0.211722, acc 0.984375
2016-11-12T19:14:40.099464: step 7471, loss 0.00269531, acc 1
2016-11-12T19:14:40.155852: step 7472, loss 0.00194007, acc 1
2016-11-12T19:14:40.215989: step 7473, loss 0.00442601, acc 1
2016-11-12T19:14:40.273841: step 7474, loss 0.00110811, acc 1
2016-11-12T19:14:40.333670: step 7475, loss 0.0010247, acc 1
2016-11-12T19:14:40.393154: step 7476, loss 0.00795678, acc 1
2016-11-12T19:14:40.452029: step 7477, loss 0.00478732, acc 1
2016-11-12T19:14:40.509107: step 7478, loss 0.00602094, acc 1
2016-11-12T19:14:40.569176: step 7479, loss 0.00243938, acc 1
2016-11-12T19:14:40.627132: step 7480, loss 0.000205407, acc 1
2016-11-12T19:14:40.684278: step 7481, loss 0.00787788, acc 1
2016-11-12T19:14:40.742077: step 7482, loss 0.000411702, acc 1
2016-11-12T19:14:40.801000: step 7483, loss 0.0244668, acc 0.984375
2016-11-12T19:14:40.860647: step 7484, loss 0.0183563, acc 0.984375
2016-11-12T19:14:40.920988: step 7485, loss 0.00117326, acc 1
2016-11-12T19:14:40.976935: step 7486, loss 0.0105998, acc 1
2016-11-12T19:14:41.037066: step 7487, loss 0.00489816, acc 1
2016-11-12T19:14:41.094642: step 7488, loss 0.0399876, acc 0.984375
2016-11-12T19:14:41.154270: step 7489, loss 0.000406561, acc 1
2016-11-12T19:14:41.212229: step 7490, loss 0.00415856, acc 1
2016-11-12T19:14:41.268685: step 7491, loss 0.00107367, acc 1
2016-11-12T19:14:41.325658: step 7492, loss 0.00463127, acc 1
2016-11-12T19:14:41.384184: step 7493, loss 0.013139, acc 0.984375
2016-11-12T19:14:41.441673: step 7494, loss 0.000719951, acc 1
2016-11-12T19:14:41.498159: step 7495, loss 0.00517539, acc 1
2016-11-12T19:14:41.554609: step 7496, loss 0.00029159, acc 1
2016-11-12T19:14:41.612590: step 7497, loss 5.54173e-05, acc 1
2016-11-12T19:14:41.668852: step 7498, loss 0.000616833, acc 1
2016-11-12T19:14:41.727173: step 7499, loss 0.000866324, acc 1
2016-11-12T19:14:41.784876: step 7500, loss 0.0148574, acc 0.984375

Evaluation:
2016-11-12T19:14:41.855941: step 7500, loss 3.28636, acc 0.572

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-7500

2016-11-12T19:14:42.349285: step 7501, loss 0.0165126, acc 0.984375
2016-11-12T19:14:42.408720: step 7502, loss 0.0228034, acc 0.984375
2016-11-12T19:14:42.467588: step 7503, loss 0.000665886, acc 1
2016-11-12T19:14:42.524428: step 7504, loss 0.019248, acc 0.984375
2016-11-12T19:14:42.584625: step 7505, loss 0.00238373, acc 1
2016-11-12T19:14:42.642389: step 7506, loss 0.0175107, acc 0.984375
2016-11-12T19:14:42.701668: step 7507, loss 0.000331892, acc 1
2016-11-12T19:14:42.758523: step 7508, loss 0.0145637, acc 0.984375
2016-11-12T19:14:42.815197: step 7509, loss 0.00106243, acc 1
2016-11-12T19:14:42.876973: step 7510, loss 0.00188015, acc 1
2016-11-12T19:14:42.933627: step 7511, loss 0.00810103, acc 1
2016-11-12T19:14:42.992942: step 7512, loss 0.0165162, acc 0.984375
2016-11-12T19:14:43.048965: step 7513, loss 1.4576e-05, acc 1
2016-11-12T19:14:43.104267: step 7514, loss 0.00694175, acc 1
2016-11-12T19:14:43.163046: step 7515, loss 0.000984072, acc 1
2016-11-12T19:14:43.224635: step 7516, loss 0.27053, acc 0.96875
2016-11-12T19:14:43.282981: step 7517, loss 0.0488799, acc 0.984375
2016-11-12T19:14:43.340979: step 7518, loss 0.0093625, acc 1
2016-11-12T19:14:43.398159: step 7519, loss 4.41359e-05, acc 1
2016-11-12T19:14:43.453704: step 7520, loss 0.000217897, acc 1
2016-11-12T19:14:43.509750: step 7521, loss 0.00186447, acc 1
2016-11-12T19:14:43.565873: step 7522, loss 0.0124871, acc 0.984375
2016-11-12T19:14:43.624363: step 7523, loss 0.00384002, acc 1
2016-11-12T19:14:43.685269: step 7524, loss 0.00460379, acc 1
2016-11-12T19:14:43.744780: step 7525, loss 0.00019749, acc 1
2016-11-12T19:14:43.784264: step 7526, loss 0.0017712, acc 1
2016-11-12T19:14:43.842584: step 7527, loss 0.00339239, acc 1
2016-11-12T19:14:43.900669: step 7528, loss 0.0005149, acc 1
2016-11-12T19:14:43.960972: step 7529, loss 0.00120698, acc 1
2016-11-12T19:14:44.019054: step 7530, loss 0.0106419, acc 1
2016-11-12T19:14:44.076921: step 7531, loss 0.00209317, acc 1
2016-11-12T19:14:44.134352: step 7532, loss 0.000668905, acc 1
2016-11-12T19:14:44.192595: step 7533, loss 0.0181842, acc 0.984375
2016-11-12T19:14:44.250082: step 7534, loss 0.0133684, acc 0.984375
2016-11-12T19:14:44.306343: step 7535, loss 0.000467143, acc 1
2016-11-12T19:14:44.367544: step 7536, loss 0.00771441, acc 1
2016-11-12T19:14:44.425152: step 7537, loss 0.00288385, acc 1
2016-11-12T19:14:44.482474: step 7538, loss 0.00774293, acc 1
2016-11-12T19:14:44.541011: step 7539, loss 0.00143893, acc 1
2016-11-12T19:14:44.597675: step 7540, loss 0.000219138, acc 1
2016-11-12T19:14:44.657107: step 7541, loss 0.000682082, acc 1
2016-11-12T19:14:44.715679: step 7542, loss 8.48845e-05, acc 1
2016-11-12T19:14:44.772848: step 7543, loss 0.00208786, acc 1
2016-11-12T19:14:44.829526: step 7544, loss 0.0062144, acc 1
2016-11-12T19:14:44.889433: step 7545, loss 0.0066348, acc 1
2016-11-12T19:14:44.948664: step 7546, loss 0.0291512, acc 0.984375
2016-11-12T19:14:45.010631: step 7547, loss 0.0013185, acc 1
2016-11-12T19:14:45.067554: step 7548, loss 0.00101635, acc 1
2016-11-12T19:14:45.125845: step 7549, loss 0.000139198, acc 1
2016-11-12T19:14:45.185172: step 7550, loss 0.0211229, acc 0.984375
2016-11-12T19:14:45.246559: step 7551, loss 0.00116472, acc 1
2016-11-12T19:14:45.305086: step 7552, loss 0.00149902, acc 1
2016-11-12T19:14:45.362521: step 7553, loss 0.000395629, acc 1
2016-11-12T19:14:45.419028: step 7554, loss 0.0331217, acc 0.96875
2016-11-12T19:14:45.476186: step 7555, loss 0.00388946, acc 1
2016-11-12T19:14:45.535149: step 7556, loss 0.000176836, acc 1
2016-11-12T19:14:45.591860: step 7557, loss 0.0127049, acc 1
2016-11-12T19:14:45.649042: step 7558, loss 0.00267367, acc 1
2016-11-12T19:14:45.708058: step 7559, loss 0.0420967, acc 0.984375
2016-11-12T19:14:45.766443: step 7560, loss 0.000939163, acc 1
2016-11-12T19:14:45.822091: step 7561, loss 0.00288198, acc 1
2016-11-12T19:14:45.880777: step 7562, loss 0.0145374, acc 0.984375
2016-11-12T19:14:45.938076: step 7563, loss 0.000236838, acc 1
2016-11-12T19:14:45.995045: step 7564, loss 0.00021161, acc 1
2016-11-12T19:14:46.051224: step 7565, loss 6.32518e-05, acc 1
2016-11-12T19:14:46.108823: step 7566, loss 0.000324246, acc 1
2016-11-12T19:14:46.165346: step 7567, loss 0.000153247, acc 1
2016-11-12T19:14:46.221618: step 7568, loss 0.000853678, acc 1
2016-11-12T19:14:46.280413: step 7569, loss 0.0369885, acc 0.984375
2016-11-12T19:14:46.341155: step 7570, loss 0.000398786, acc 1
2016-11-12T19:14:46.397264: step 7571, loss 0.0492823, acc 0.984375
2016-11-12T19:14:46.455256: step 7572, loss 0.000183604, acc 1
2016-11-12T19:14:46.512544: step 7573, loss 0.00652121, acc 1
2016-11-12T19:14:46.569563: step 7574, loss 0.00157007, acc 1
2016-11-12T19:14:46.627416: step 7575, loss 0.0313487, acc 0.984375
2016-11-12T19:14:46.687824: step 7576, loss 0.0162278, acc 0.984375
2016-11-12T19:14:46.744996: step 7577, loss 0.00466793, acc 1
2016-11-12T19:14:46.804803: step 7578, loss 0.0158544, acc 0.984375
2016-11-12T19:14:46.866523: step 7579, loss 0.00066854, acc 1
2016-11-12T19:14:46.922277: step 7580, loss 0.00192801, acc 1
2016-11-12T19:14:46.981707: step 7581, loss 0.000106737, acc 1
2016-11-12T19:14:47.040273: step 7582, loss 0.000218179, acc 1
2016-11-12T19:14:47.097051: step 7583, loss 0.00383756, acc 1
2016-11-12T19:14:47.155129: step 7584, loss 0.00214307, acc 1
2016-11-12T19:14:47.211308: step 7585, loss 0.00025016, acc 1
2016-11-12T19:14:47.267370: step 7586, loss 0.0221638, acc 0.984375
2016-11-12T19:14:47.325573: step 7587, loss 5.02358e-05, acc 1
2016-11-12T19:14:47.384827: step 7588, loss 0.0384947, acc 0.96875
2016-11-12T19:14:47.444790: step 7589, loss 0.20347, acc 0.984375
2016-11-12T19:14:47.502629: step 7590, loss 0.0108448, acc 1
2016-11-12T19:14:47.561129: step 7591, loss 0.0150846, acc 0.984375
2016-11-12T19:14:47.620734: step 7592, loss 0.001748, acc 1
2016-11-12T19:14:47.678638: step 7593, loss 0.000123186, acc 1
2016-11-12T19:14:47.734764: step 7594, loss 0.0141321, acc 0.984375
2016-11-12T19:14:47.791930: step 7595, loss 0.00423976, acc 1
2016-11-12T19:14:47.851914: step 7596, loss 0.00446264, acc 1
2016-11-12T19:14:47.891235: step 7597, loss 0.000266999, acc 1
2016-11-12T19:14:47.953014: step 7598, loss 0.0151064, acc 1
2016-11-12T19:14:48.011010: step 7599, loss 0.0174486, acc 0.984375
2016-11-12T19:14:48.074712: step 7600, loss 0.0222386, acc 0.984375

Evaluation:
2016-11-12T19:14:48.146628: step 7600, loss 3.32464, acc 0.574

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-7600

2016-11-12T19:14:48.638286: step 7601, loss 0.000176778, acc 1
2016-11-12T19:14:48.695591: step 7602, loss 0.000820684, acc 1
2016-11-12T19:14:48.753157: step 7603, loss 0.0290705, acc 0.984375
2016-11-12T19:14:48.810828: step 7604, loss 0.00213899, acc 1
2016-11-12T19:14:48.868591: step 7605, loss 0.0231733, acc 0.984375
2016-11-12T19:14:48.927819: step 7606, loss 0.000381194, acc 1
2016-11-12T19:14:48.986113: step 7607, loss 0.00028586, acc 1
2016-11-12T19:14:49.043565: step 7608, loss 0.00257467, acc 1
2016-11-12T19:14:49.102707: step 7609, loss 0.0208673, acc 0.984375
2016-11-12T19:14:49.161411: step 7610, loss 0.000994666, acc 1
2016-11-12T19:14:49.220852: step 7611, loss 0.000764008, acc 1
2016-11-12T19:14:49.280433: step 7612, loss 0.000927607, acc 1
2016-11-12T19:14:49.338941: step 7613, loss 0.000884159, acc 1
2016-11-12T19:14:49.397535: step 7614, loss 0.013275, acc 0.984375
2016-11-12T19:14:49.455012: step 7615, loss 0.000424835, acc 1
2016-11-12T19:14:49.512178: step 7616, loss 0.036946, acc 0.984375
2016-11-12T19:14:49.572622: step 7617, loss 0.000216892, acc 1
2016-11-12T19:14:49.631116: step 7618, loss 0.0242561, acc 0.984375
2016-11-12T19:14:49.688555: step 7619, loss 0.00305742, acc 1
2016-11-12T19:14:49.744622: step 7620, loss 7.26004e-05, acc 1
2016-11-12T19:14:49.800637: step 7621, loss 0.0050131, acc 1
2016-11-12T19:14:49.860258: step 7622, loss 0.00441744, acc 1
2016-11-12T19:14:49.921445: step 7623, loss 0.069214, acc 0.96875
2016-11-12T19:14:49.981482: step 7624, loss 0.0101768, acc 1
2016-11-12T19:14:50.039591: step 7625, loss 0.0049773, acc 1
2016-11-12T19:14:50.097662: step 7626, loss 0.00018124, acc 1
2016-11-12T19:14:50.154529: step 7627, loss 0.0145829, acc 0.984375
2016-11-12T19:14:50.212047: step 7628, loss 0.015442, acc 0.984375
2016-11-12T19:14:50.270735: step 7629, loss 0.00193339, acc 1
2016-11-12T19:14:50.327637: step 7630, loss 0.000318345, acc 1
2016-11-12T19:14:50.385095: step 7631, loss 0.052017, acc 0.984375
2016-11-12T19:14:50.444175: step 7632, loss 0.00907199, acc 1
2016-11-12T19:14:50.501290: step 7633, loss 0.00340876, acc 1
2016-11-12T19:14:50.558506: step 7634, loss 0.0712118, acc 0.984375
2016-11-12T19:14:50.616084: step 7635, loss 0.00834703, acc 1
2016-11-12T19:14:50.674172: step 7636, loss 0.00251852, acc 1
2016-11-12T19:14:50.730834: step 7637, loss 0.00145715, acc 1
2016-11-12T19:14:50.787887: step 7638, loss 0.018515, acc 0.984375
2016-11-12T19:14:50.845960: step 7639, loss 0.0513314, acc 0.984375
2016-11-12T19:14:50.903699: step 7640, loss 0.0076079, acc 1
2016-11-12T19:14:50.960786: step 7641, loss 0.000254879, acc 1
2016-11-12T19:14:51.016382: step 7642, loss 0.00761275, acc 1
2016-11-12T19:14:51.076353: step 7643, loss 0.0111258, acc 1
2016-11-12T19:14:51.133661: step 7644, loss 0.000338914, acc 1
2016-11-12T19:14:51.189853: step 7645, loss 0.00925384, acc 1
2016-11-12T19:14:51.247298: step 7646, loss 0.00202668, acc 1
2016-11-12T19:14:51.308087: step 7647, loss 0.000239027, acc 1
2016-11-12T19:14:51.368415: step 7648, loss 0.0110258, acc 1
2016-11-12T19:14:51.429076: step 7649, loss 0.0190113, acc 1
2016-11-12T19:14:51.489517: step 7650, loss 0.0136094, acc 1
2016-11-12T19:14:51.550036: step 7651, loss 0.00253827, acc 1
2016-11-12T19:14:51.606938: step 7652, loss 0.000260677, acc 1
2016-11-12T19:14:51.665012: step 7653, loss 0.00091191, acc 1
2016-11-12T19:14:51.721342: step 7654, loss 0.00596982, acc 1
2016-11-12T19:14:51.778701: step 7655, loss 0.00216083, acc 1
2016-11-12T19:14:51.836876: step 7656, loss 8.01675e-05, acc 1
2016-11-12T19:14:51.892457: step 7657, loss 0.0263819, acc 0.96875
2016-11-12T19:14:51.949648: step 7658, loss 0.00363903, acc 1
2016-11-12T19:14:52.005745: step 7659, loss 0.0013361, acc 1
2016-11-12T19:14:52.067994: step 7660, loss 0.0159723, acc 0.984375
2016-11-12T19:14:52.126957: step 7661, loss 0.0016918, acc 1
2016-11-12T19:14:52.185961: step 7662, loss 0.0163086, acc 0.984375
2016-11-12T19:14:52.245055: step 7663, loss 0.000283848, acc 1
2016-11-12T19:14:52.300519: step 7664, loss 0.0241397, acc 0.984375
2016-11-12T19:14:52.357887: step 7665, loss 0.00025278, acc 1
2016-11-12T19:14:52.416822: step 7666, loss 0.0224896, acc 0.984375
2016-11-12T19:14:52.475169: step 7667, loss 0.0349432, acc 0.984375
2016-11-12T19:14:52.513091: step 7668, loss 3.95164e-06, acc 1
2016-11-12T19:14:52.573424: step 7669, loss 0.0149962, acc 1
2016-11-12T19:14:52.634778: step 7670, loss 0.000235333, acc 1
2016-11-12T19:14:52.691747: step 7671, loss 0.000519308, acc 1
2016-11-12T19:14:52.749694: step 7672, loss 0.000527237, acc 1
2016-11-12T19:14:52.808075: step 7673, loss 0.000300812, acc 1
2016-11-12T19:14:52.864652: step 7674, loss 0.00254909, acc 1
2016-11-12T19:14:52.922720: step 7675, loss 0.000481655, acc 1
2016-11-12T19:14:52.979224: step 7676, loss 0.00472826, acc 1
2016-11-12T19:14:53.037270: step 7677, loss 0.00117664, acc 1
2016-11-12T19:14:53.097754: step 7678, loss 0.000858606, acc 1
2016-11-12T19:14:53.156098: step 7679, loss 0.0154551, acc 0.984375
2016-11-12T19:14:53.214923: step 7680, loss 0.000955926, acc 1
2016-11-12T19:14:53.273243: step 7681, loss 0.0385653, acc 0.984375
2016-11-12T19:14:53.332862: step 7682, loss 0.000421743, acc 1
2016-11-12T19:14:53.389408: step 7683, loss 3.9973e-05, acc 1
2016-11-12T19:14:53.445676: step 7684, loss 2.00508e-05, acc 1
2016-11-12T19:14:53.501776: step 7685, loss 0.00118277, acc 1
2016-11-12T19:14:53.561691: step 7686, loss 0.00020229, acc 1
2016-11-12T19:14:53.618653: step 7687, loss 0.0165049, acc 0.984375
2016-11-12T19:14:53.677267: step 7688, loss 0.00308911, acc 1
2016-11-12T19:14:53.734755: step 7689, loss 0.0130149, acc 1
2016-11-12T19:14:53.792954: step 7690, loss 0.00793955, acc 1
2016-11-12T19:14:53.852773: step 7691, loss 0.00355937, acc 1
2016-11-12T19:14:53.911260: step 7692, loss 0.0357023, acc 0.984375
2016-11-12T19:14:53.967943: step 7693, loss 0.00065007, acc 1
2016-11-12T19:14:54.024434: step 7694, loss 8.217e-05, acc 1
2016-11-12T19:14:54.080908: step 7695, loss 0.000246341, acc 1
2016-11-12T19:14:54.139862: step 7696, loss 0.000814787, acc 1
2016-11-12T19:14:54.197307: step 7697, loss 0.00158973, acc 1
2016-11-12T19:14:54.254887: step 7698, loss 0.00524994, acc 1
2016-11-12T19:14:54.312998: step 7699, loss 0.000997326, acc 1
2016-11-12T19:14:54.370690: step 7700, loss 0.00516674, acc 1

Evaluation:
2016-11-12T19:14:54.442358: step 7700, loss 3.33881, acc 0.566

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-7700

2016-11-12T19:14:54.931747: step 7701, loss 0.00165938, acc 1
2016-11-12T19:14:54.991499: step 7702, loss 0.0314364, acc 0.984375
2016-11-12T19:14:55.049250: step 7703, loss 0.00503076, acc 1
2016-11-12T19:14:55.107972: step 7704, loss 0.00159963, acc 1
2016-11-12T19:14:55.164419: step 7705, loss 0.0128111, acc 0.984375
2016-11-12T19:14:55.222568: step 7706, loss 0.00666006, acc 1
2016-11-12T19:14:55.280453: step 7707, loss 0.00392815, acc 1
2016-11-12T19:14:55.341119: step 7708, loss 0.015484, acc 0.984375
2016-11-12T19:14:55.401054: step 7709, loss 0.263875, acc 0.984375
2016-11-12T19:14:55.462324: step 7710, loss 0.0450414, acc 0.96875
2016-11-12T19:14:55.521565: step 7711, loss 0.0500556, acc 0.984375
2016-11-12T19:14:55.579524: step 7712, loss 0.000368709, acc 1
2016-11-12T19:14:55.637762: step 7713, loss 0.000154799, acc 1
2016-11-12T19:14:55.696537: step 7714, loss 0.000953083, acc 1
2016-11-12T19:14:55.753381: step 7715, loss 0.00119129, acc 1
2016-11-12T19:14:55.811002: step 7716, loss 0.00459619, acc 1
2016-11-12T19:14:55.870033: step 7717, loss 0.0105133, acc 1
2016-11-12T19:14:55.929318: step 7718, loss 0.000324629, acc 1
2016-11-12T19:14:55.985099: step 7719, loss 0.0151952, acc 0.984375
2016-11-12T19:14:56.044929: step 7720, loss 0.0354136, acc 0.984375
2016-11-12T19:14:56.105877: step 7721, loss 0.000494423, acc 1
2016-11-12T19:14:56.163614: step 7722, loss 0.00171945, acc 1
2016-11-12T19:14:56.219717: step 7723, loss 0.0123695, acc 0.984375
2016-11-12T19:14:56.279132: step 7724, loss 0.00112565, acc 1
2016-11-12T19:14:56.335455: step 7725, loss 0.000666376, acc 1
2016-11-12T19:14:56.393010: step 7726, loss 0.00281727, acc 1
2016-11-12T19:14:56.450583: step 7727, loss 0.0114135, acc 0.984375
2016-11-12T19:14:56.507904: step 7728, loss 0.00197446, acc 1
2016-11-12T19:14:56.564772: step 7729, loss 0.00204867, acc 1
2016-11-12T19:14:56.625120: step 7730, loss 0.0187075, acc 0.984375
2016-11-12T19:14:56.682352: step 7731, loss 0.000352033, acc 1
2016-11-12T19:14:56.739358: step 7732, loss 0.0182991, acc 0.984375
2016-11-12T19:14:56.801164: step 7733, loss 0.0137153, acc 0.984375
2016-11-12T19:14:56.859259: step 7734, loss 0.0296273, acc 0.984375
2016-11-12T19:14:56.917818: step 7735, loss 0.000309785, acc 1
2016-11-12T19:14:56.973190: step 7736, loss 0.000339483, acc 1
2016-11-12T19:14:57.029950: step 7737, loss 0.00248808, acc 1
2016-11-12T19:14:57.087279: step 7738, loss 0.00428916, acc 1
2016-11-12T19:14:57.125882: step 7739, loss 0.000695369, acc 1
2016-11-12T19:14:57.186405: step 7740, loss 0.000564702, acc 1
2016-11-12T19:14:57.242779: step 7741, loss 0.000309277, acc 1
2016-11-12T19:14:57.299553: step 7742, loss 0.00109422, acc 1
2016-11-12T19:14:57.356826: step 7743, loss 0.0124296, acc 0.984375
2016-11-12T19:14:57.414823: step 7744, loss 0.000183344, acc 1
2016-11-12T19:14:57.471597: step 7745, loss 0.00717222, acc 1
2016-11-12T19:14:57.528732: step 7746, loss 0.00587741, acc 1
2016-11-12T19:14:57.588411: step 7747, loss 0.00113329, acc 1
2016-11-12T19:14:57.648656: step 7748, loss 0.00012825, acc 1
2016-11-12T19:14:57.705351: step 7749, loss 0.00904582, acc 1
2016-11-12T19:14:57.765740: step 7750, loss 7.68724e-05, acc 1
2016-11-12T19:14:57.822883: step 7751, loss 6.5526e-05, acc 1
2016-11-12T19:14:57.879086: step 7752, loss 0.00605517, acc 1
2016-11-12T19:14:57.936227: step 7753, loss 0.000690026, acc 1
2016-11-12T19:14:57.993418: step 7754, loss 0.00265627, acc 1
2016-11-12T19:14:58.056424: step 7755, loss 0.17008, acc 0.984375
2016-11-12T19:14:58.118977: step 7756, loss 0.0253659, acc 0.96875
2016-11-12T19:14:58.176766: step 7757, loss 0.000107463, acc 1
2016-11-12T19:14:58.232135: step 7758, loss 0.00544144, acc 1
2016-11-12T19:14:58.292071: step 7759, loss 0.013749, acc 0.984375
2016-11-12T19:14:58.349007: step 7760, loss 0.00592151, acc 1
2016-11-12T19:14:58.408660: step 7761, loss 0.000131632, acc 1
2016-11-12T19:14:58.469072: step 7762, loss 0.00294497, acc 1
2016-11-12T19:14:58.528846: step 7763, loss 0.0022103, acc 1
2016-11-12T19:14:58.586372: step 7764, loss 0.00253209, acc 1
2016-11-12T19:14:58.644289: step 7765, loss 0.00298467, acc 1
2016-11-12T19:14:58.701768: step 7766, loss 0.000175736, acc 1
2016-11-12T19:14:58.758175: step 7767, loss 0.0247938, acc 0.984375
2016-11-12T19:14:58.816711: step 7768, loss 0.0436604, acc 0.984375
2016-11-12T19:14:58.874769: step 7769, loss 0.000594805, acc 1
2016-11-12T19:14:58.933065: step 7770, loss 0.000650897, acc 1
2016-11-12T19:14:58.992584: step 7771, loss 0.0306473, acc 0.984375
2016-11-12T19:14:59.050505: step 7772, loss 0.0147741, acc 0.984375
2016-11-12T19:14:59.108796: step 7773, loss 0.00853399, acc 1
2016-11-12T19:14:59.168658: step 7774, loss 0.00510777, acc 1
2016-11-12T19:14:59.228570: step 7775, loss 0.00123245, acc 1
2016-11-12T19:14:59.285470: step 7776, loss 0.00832308, acc 1
2016-11-12T19:14:59.344194: step 7777, loss 0.0317037, acc 0.984375
2016-11-12T19:14:59.402624: step 7778, loss 0.000309873, acc 1
2016-11-12T19:14:59.460572: step 7779, loss 0.000900977, acc 1
2016-11-12T19:14:59.518945: step 7780, loss 0.000785947, acc 1
2016-11-12T19:14:59.575870: step 7781, loss 0.000831946, acc 1
2016-11-12T19:14:59.633611: step 7782, loss 0.0111936, acc 1
2016-11-12T19:14:59.692004: step 7783, loss 0.00121792, acc 1
2016-11-12T19:14:59.748175: step 7784, loss 0.0040186, acc 1
2016-11-12T19:14:59.807619: step 7785, loss 0.0196302, acc 1
2016-11-12T19:14:59.865366: step 7786, loss 0.015248, acc 1
2016-11-12T19:14:59.925046: step 7787, loss 0.0151086, acc 0.984375
2016-11-12T19:14:59.983335: step 7788, loss 0.0208444, acc 0.984375
2016-11-12T19:15:00.040908: step 7789, loss 0.00161215, acc 1
2016-11-12T19:15:00.101737: step 7790, loss 0.00791868, acc 1
2016-11-12T19:15:00.159155: step 7791, loss 0.00198596, acc 1
2016-11-12T19:15:00.215401: step 7792, loss 0.000416312, acc 1
2016-11-12T19:15:00.273733: step 7793, loss 0.00067432, acc 1
2016-11-12T19:15:00.329646: step 7794, loss 0.000750039, acc 1
2016-11-12T19:15:00.386142: step 7795, loss 0.00511966, acc 1
2016-11-12T19:15:00.443604: step 7796, loss 0.00141669, acc 1
2016-11-12T19:15:00.500076: step 7797, loss 0.00293134, acc 1
2016-11-12T19:15:00.557059: step 7798, loss 0.00106099, acc 1
2016-11-12T19:15:00.613138: step 7799, loss 0.000167068, acc 1
2016-11-12T19:15:00.669207: step 7800, loss 0.0201422, acc 0.984375

Evaluation:
2016-11-12T19:15:00.743053: step 7800, loss 3.26307, acc 0.566

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-7800

2016-11-12T19:15:01.234964: step 7801, loss 0.00940688, acc 1
2016-11-12T19:15:01.291842: step 7802, loss 0.0151679, acc 0.984375
2016-11-12T19:15:01.350039: step 7803, loss 0.000315125, acc 1
2016-11-12T19:15:01.408323: step 7804, loss 0.000253607, acc 1
2016-11-12T19:15:01.463865: step 7805, loss 0.00763921, acc 1
2016-11-12T19:15:01.525054: step 7806, loss 0.001028, acc 1
2016-11-12T19:15:01.582454: step 7807, loss 0.00985573, acc 1
2016-11-12T19:15:01.639099: step 7808, loss 0.0015836, acc 1
2016-11-12T19:15:01.696689: step 7809, loss 0.00144205, acc 1
2016-11-12T19:15:01.735467: step 7810, loss 0.0327274, acc 1
2016-11-12T19:15:01.794403: step 7811, loss 0.000989491, acc 1
2016-11-12T19:15:01.853016: step 7812, loss 0.00215363, acc 1
2016-11-12T19:15:01.910791: step 7813, loss 0.0013325, acc 1
2016-11-12T19:15:01.967145: step 7814, loss 0.00366782, acc 1
2016-11-12T19:15:02.024159: step 7815, loss 0.000206134, acc 1
2016-11-12T19:15:02.083558: step 7816, loss 0.000499085, acc 1
2016-11-12T19:15:02.140950: step 7817, loss 0.000559018, acc 1
2016-11-12T19:15:02.197058: step 7818, loss 0.00147614, acc 1
2016-11-12T19:15:02.254190: step 7819, loss 0.00183666, acc 1
2016-11-12T19:15:02.313100: step 7820, loss 0.0556657, acc 0.96875
2016-11-12T19:15:02.373135: step 7821, loss 0.00190619, acc 1
2016-11-12T19:15:02.430337: step 7822, loss 3.84331e-05, acc 1
2016-11-12T19:15:02.488641: step 7823, loss 0.00325434, acc 1
2016-11-12T19:15:02.549675: step 7824, loss 0.00751516, acc 1
2016-11-12T19:15:02.607551: step 7825, loss 0.0110909, acc 1
2016-11-12T19:15:02.665171: step 7826, loss 0.0270698, acc 0.984375
2016-11-12T19:15:02.725196: step 7827, loss 0.00814538, acc 1
2016-11-12T19:15:02.784962: step 7828, loss 0.0170669, acc 0.984375
2016-11-12T19:15:02.842102: step 7829, loss 0.000834105, acc 1
2016-11-12T19:15:02.901174: step 7830, loss 0.000218085, acc 1
2016-11-12T19:15:02.958344: step 7831, loss 0.000709551, acc 1
2016-11-12T19:15:03.013728: step 7832, loss 0.028592, acc 0.984375
2016-11-12T19:15:03.071471: step 7833, loss 0.0122978, acc 0.984375
2016-11-12T19:15:03.129298: step 7834, loss 0.000109183, acc 1
2016-11-12T19:15:03.187977: step 7835, loss 0.000450495, acc 1
2016-11-12T19:15:03.248779: step 7836, loss 0.0281088, acc 0.984375
2016-11-12T19:15:03.307260: step 7837, loss 0.000737534, acc 1
2016-11-12T19:15:03.365194: step 7838, loss 0.000434785, acc 1
2016-11-12T19:15:03.424315: step 7839, loss 0.000429251, acc 1
2016-11-12T19:15:03.481665: step 7840, loss 0.000238946, acc 1
2016-11-12T19:15:03.541861: step 7841, loss 0.00290101, acc 1
2016-11-12T19:15:03.601607: step 7842, loss 0.000844989, acc 1
2016-11-12T19:15:03.658694: step 7843, loss 0.0072624, acc 1
2016-11-12T19:15:03.720186: step 7844, loss 0.00704526, acc 1
2016-11-12T19:15:03.780392: step 7845, loss 0.021348, acc 0.984375
2016-11-12T19:15:03.841328: step 7846, loss 0.00105969, acc 1
2016-11-12T19:15:03.898551: step 7847, loss 0.000215662, acc 1
2016-11-12T19:15:03.957125: step 7848, loss 0.000136261, acc 1
2016-11-12T19:15:04.014436: step 7849, loss 0.0144876, acc 1
2016-11-12T19:15:04.072987: step 7850, loss 0.00656927, acc 1
2016-11-12T19:15:04.130902: step 7851, loss 0.0109377, acc 1
2016-11-12T19:15:04.190341: step 7852, loss 0.0137951, acc 0.984375
2016-11-12T19:15:04.249317: step 7853, loss 0.00140884, acc 1
2016-11-12T19:15:04.307652: step 7854, loss 0.0384344, acc 0.984375
2016-11-12T19:15:04.365905: step 7855, loss 0.00935816, acc 1
2016-11-12T19:15:04.424149: step 7856, loss 0.0714486, acc 0.96875
2016-11-12T19:15:04.482124: step 7857, loss 0.000262725, acc 1
2016-11-12T19:15:04.541188: step 7858, loss 0.00181604, acc 1
2016-11-12T19:15:04.599761: step 7859, loss 0.140518, acc 0.984375
2016-11-12T19:15:04.660884: step 7860, loss 0.000264094, acc 1
2016-11-12T19:15:04.718715: step 7861, loss 0.00294662, acc 1
2016-11-12T19:15:04.777455: step 7862, loss 0.000914432, acc 1
2016-11-12T19:15:04.836182: step 7863, loss 0.000776036, acc 1
2016-11-12T19:15:04.895422: step 7864, loss 0.0259842, acc 0.984375
2016-11-12T19:15:04.953887: step 7865, loss 0.000124118, acc 1
2016-11-12T19:15:05.010651: step 7866, loss 0.000374605, acc 1
2016-11-12T19:15:05.067672: step 7867, loss 0.0527829, acc 0.984375
2016-11-12T19:15:05.125967: step 7868, loss 0.00046092, acc 1
2016-11-12T19:15:05.183573: step 7869, loss 0.0348204, acc 0.984375
2016-11-12T19:15:05.241628: step 7870, loss 0.00383608, acc 1
2016-11-12T19:15:05.300752: step 7871, loss 0.00801616, acc 1
2016-11-12T19:15:05.360084: step 7872, loss 0.0150682, acc 0.984375
2016-11-12T19:15:05.417489: step 7873, loss 0.0095976, acc 1
2016-11-12T19:15:05.475542: step 7874, loss 0.00180122, acc 1
2016-11-12T19:15:05.532523: step 7875, loss 0.000314431, acc 1
2016-11-12T19:15:05.589037: step 7876, loss 0.000536382, acc 1
2016-11-12T19:15:05.645831: step 7877, loss 8.67671e-05, acc 1
2016-11-12T19:15:05.702611: step 7878, loss 0.000317359, acc 1
2016-11-12T19:15:05.761803: step 7879, loss 0.00549102, acc 1
2016-11-12T19:15:05.820764: step 7880, loss 0.00907961, acc 1
2016-11-12T19:15:05.860548: step 7881, loss 0.0051973, acc 1
2016-11-12T19:15:05.922621: step 7882, loss 0.021497, acc 0.984375
2016-11-12T19:15:05.979791: step 7883, loss 0.000195497, acc 1
2016-11-12T19:15:06.039212: step 7884, loss 0.000602292, acc 1
2016-11-12T19:15:06.100458: step 7885, loss 0.00924721, acc 1
2016-11-12T19:15:06.160322: step 7886, loss 0.0129841, acc 0.984375
2016-11-12T19:15:06.217905: step 7887, loss 0.00540779, acc 1
2016-11-12T19:15:06.275588: step 7888, loss 0.00146539, acc 1
2016-11-12T19:15:06.333329: step 7889, loss 0.0169502, acc 0.984375
2016-11-12T19:15:06.392751: step 7890, loss 2.2437e-05, acc 1
2016-11-12T19:15:06.449374: step 7891, loss 0.0149579, acc 0.984375
2016-11-12T19:15:06.508528: step 7892, loss 0.0118939, acc 0.984375
2016-11-12T19:15:06.567838: step 7893, loss 0.000268954, acc 1
2016-11-12T19:15:06.625296: step 7894, loss 0.020178, acc 0.984375
2016-11-12T19:15:06.689162: step 7895, loss 0.000198528, acc 1
2016-11-12T19:15:06.745663: step 7896, loss 0.0101567, acc 1
2016-11-12T19:15:06.804161: step 7897, loss 0.00138196, acc 1
2016-11-12T19:15:06.861230: step 7898, loss 0.119083, acc 0.96875
2016-11-12T19:15:06.919129: step 7899, loss 0.000968933, acc 1
2016-11-12T19:15:06.976443: step 7900, loss 0.00393436, acc 1

Evaluation:
2016-11-12T19:15:07.047640: step 7900, loss 3.33966, acc 0.584

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-7900

2016-11-12T19:15:07.537331: step 7901, loss 0.00716953, acc 1
2016-11-12T19:15:07.594402: step 7902, loss 0.00859309, acc 1
2016-11-12T19:15:07.653047: step 7903, loss 0.00261197, acc 1
2016-11-12T19:15:07.710662: step 7904, loss 0.0113072, acc 1
2016-11-12T19:15:07.768641: step 7905, loss 0.00118112, acc 1
2016-11-12T19:15:07.826892: step 7906, loss 0.000859857, acc 1
2016-11-12T19:15:07.886227: step 7907, loss 0.0268994, acc 0.984375
2016-11-12T19:15:07.945473: step 7908, loss 0.000453241, acc 1
2016-11-12T19:15:08.001527: step 7909, loss 0.00353681, acc 1
2016-11-12T19:15:08.058823: step 7910, loss 0.0233102, acc 0.984375
2016-11-12T19:15:08.117653: step 7911, loss 0.00115625, acc 1
2016-11-12T19:15:08.174174: step 7912, loss 0.000896441, acc 1
2016-11-12T19:15:08.231641: step 7913, loss 0.000557597, acc 1
2016-11-12T19:15:08.289866: step 7914, loss 0.000341831, acc 1
2016-11-12T19:15:08.349695: step 7915, loss 0.000152561, acc 1
2016-11-12T19:15:08.406907: step 7916, loss 0.00116472, acc 1
2016-11-12T19:15:08.465102: step 7917, loss 0.00217166, acc 1
2016-11-12T19:15:08.522512: step 7918, loss 0.0119071, acc 1
2016-11-12T19:15:08.579468: step 7919, loss 0.000499766, acc 1
2016-11-12T19:15:08.635961: step 7920, loss 0.0414726, acc 0.984375
2016-11-12T19:15:08.692465: step 7921, loss 0.00158194, acc 1
2016-11-12T19:15:08.750822: step 7922, loss 0.00332991, acc 1
2016-11-12T19:15:08.809629: step 7923, loss 0.000869451, acc 1
2016-11-12T19:15:08.865847: step 7924, loss 0.022245, acc 0.984375
2016-11-12T19:15:08.925905: step 7925, loss 0.000577373, acc 1
2016-11-12T19:15:08.983577: step 7926, loss 0.0126242, acc 0.984375
2016-11-12T19:15:09.044212: step 7927, loss 0.000190863, acc 1
2016-11-12T19:15:09.102674: step 7928, loss 0.0154034, acc 0.984375
2016-11-12T19:15:09.160065: step 7929, loss 0.00129825, acc 1
2016-11-12T19:15:09.221221: step 7930, loss 0.00484139, acc 1
2016-11-12T19:15:09.279474: step 7931, loss 0.000313628, acc 1
2016-11-12T19:15:09.340279: step 7932, loss 0.00325156, acc 1
2016-11-12T19:15:09.396975: step 7933, loss 0.00228362, acc 1
2016-11-12T19:15:09.454523: step 7934, loss 0.0131043, acc 0.984375
2016-11-12T19:15:09.512819: step 7935, loss 0.000658214, acc 1
2016-11-12T19:15:09.571578: step 7936, loss 0.00636272, acc 1
2016-11-12T19:15:09.632834: step 7937, loss 0.00027796, acc 1
2016-11-12T19:15:09.691456: step 7938, loss 0.0124549, acc 0.984375
2016-11-12T19:15:09.752433: step 7939, loss 0.00789332, acc 1
2016-11-12T19:15:09.811889: step 7940, loss 0.00504274, acc 1
2016-11-12T19:15:09.869385: step 7941, loss 0.0372501, acc 0.984375
2016-11-12T19:15:09.927235: step 7942, loss 0.000729342, acc 1
2016-11-12T19:15:09.982941: step 7943, loss 0.00030894, acc 1
2016-11-12T19:15:10.040377: step 7944, loss 0.00380064, acc 1
2016-11-12T19:15:10.097973: step 7945, loss 0.0149365, acc 0.984375
2016-11-12T19:15:10.155749: step 7946, loss 0.00015033, acc 1
2016-11-12T19:15:10.212857: step 7947, loss 0.00275477, acc 1
2016-11-12T19:15:10.271386: step 7948, loss 0.00253604, acc 1
2016-11-12T19:15:10.328872: step 7949, loss 0.000341334, acc 1
2016-11-12T19:15:10.388659: step 7950, loss 0.00375983, acc 1
2016-11-12T19:15:10.446479: step 7951, loss 0.000309812, acc 1
2016-11-12T19:15:10.484198: step 7952, loss 0.00102866, acc 1
2016-11-12T19:15:10.544087: step 7953, loss 0.00507262, acc 1
2016-11-12T19:15:10.602202: step 7954, loss 0.00158157, acc 1
2016-11-12T19:15:10.661662: step 7955, loss 0.0154531, acc 0.984375
2016-11-12T19:15:10.718649: step 7956, loss 0.000342943, acc 1
2016-11-12T19:15:10.777018: step 7957, loss 0.00413834, acc 1
2016-11-12T19:15:10.835494: step 7958, loss 0.00146563, acc 1
2016-11-12T19:15:10.893849: step 7959, loss 0.000112674, acc 1
2016-11-12T19:15:10.949129: step 7960, loss 0.00438396, acc 1
2016-11-12T19:15:11.009071: step 7961, loss 0.00196286, acc 1
2016-11-12T19:15:11.068075: step 7962, loss 0.000840588, acc 1
2016-11-12T19:15:11.125685: step 7963, loss 0.0130161, acc 1
2016-11-12T19:15:11.183089: step 7964, loss 0.0150206, acc 0.984375
2016-11-12T19:15:11.241815: step 7965, loss 0.000810784, acc 1
2016-11-12T19:15:11.301121: step 7966, loss 0.000418836, acc 1
2016-11-12T19:15:11.357057: step 7967, loss 0.000274731, acc 1
2016-11-12T19:15:11.416947: step 7968, loss 0.00350563, acc 1
2016-11-12T19:15:11.476747: step 7969, loss 5.76545e-05, acc 1
2016-11-12T19:15:11.533415: step 7970, loss 0.00901716, acc 1
2016-11-12T19:15:11.591428: step 7971, loss 0.00431677, acc 1
2016-11-12T19:15:11.652136: step 7972, loss 0.0349529, acc 0.984375
2016-11-12T19:15:11.710611: step 7973, loss 0.00653496, acc 1
2016-11-12T19:15:11.769057: step 7974, loss 0.00424643, acc 1
2016-11-12T19:15:11.827889: step 7975, loss 0.00203043, acc 1
2016-11-12T19:15:11.888243: step 7976, loss 0.0055203, acc 1
2016-11-12T19:15:11.946718: step 7977, loss 0.00180242, acc 1
2016-11-12T19:15:12.005896: step 7978, loss 0.00989739, acc 1
2016-11-12T19:15:12.063130: step 7979, loss 0.0162474, acc 0.984375
2016-11-12T19:15:12.121142: step 7980, loss 0.00287695, acc 1
2016-11-12T19:15:12.179542: step 7981, loss 0.00460012, acc 1
2016-11-12T19:15:12.236522: step 7982, loss 0.00191007, acc 1
2016-11-12T19:15:12.294161: step 7983, loss 0.195034, acc 0.984375
2016-11-12T19:15:12.352889: step 7984, loss 0.000529799, acc 1
2016-11-12T19:15:12.409473: step 7985, loss 0.00101807, acc 1
2016-11-12T19:15:12.466452: step 7986, loss 0.044371, acc 0.984375
2016-11-12T19:15:12.523519: step 7987, loss 0.00104141, acc 1
2016-11-12T19:15:12.581372: step 7988, loss 0.000208667, acc 1
2016-11-12T19:15:12.641015: step 7989, loss 7.44914e-05, acc 1
2016-11-12T19:15:12.701813: step 7990, loss 0.00108471, acc 1
2016-11-12T19:15:12.760974: step 7991, loss 0.000438102, acc 1
2016-11-12T19:15:12.817523: step 7992, loss 0.000364758, acc 1
2016-11-12T19:15:12.873677: step 7993, loss 0.00221929, acc 1
2016-11-12T19:15:12.932899: step 7994, loss 0.00064884, acc 1
2016-11-12T19:15:12.990386: step 7995, loss 0.0219402, acc 0.984375
2016-11-12T19:15:13.049518: step 7996, loss 0.000291671, acc 1
2016-11-12T19:15:13.109335: step 7997, loss 0.0061056, acc 1
2016-11-12T19:15:13.169139: step 7998, loss 0.00305288, acc 1
2016-11-12T19:15:13.226011: step 7999, loss 0.000326379, acc 1
2016-11-12T19:15:13.285666: step 8000, loss 0.000784616, acc 1

Evaluation:
2016-11-12T19:15:13.356997: step 8000, loss 3.37265, acc 0.576

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-8000

2016-11-12T19:15:13.873456: step 8001, loss 0.00350086, acc 1
2016-11-12T19:15:13.932278: step 8002, loss 0.000614986, acc 1
2016-11-12T19:15:13.989244: step 8003, loss 0.000181392, acc 1
2016-11-12T19:15:14.047010: step 8004, loss 0.000376119, acc 1
2016-11-12T19:15:14.105258: step 8005, loss 0.000578708, acc 1
2016-11-12T19:15:14.163451: step 8006, loss 0.0127929, acc 0.984375
2016-11-12T19:15:14.219718: step 8007, loss 0.0707823, acc 0.96875
2016-11-12T19:15:14.280228: step 8008, loss 0.00121029, acc 1
2016-11-12T19:15:14.339732: step 8009, loss 0.000860203, acc 1
2016-11-12T19:15:14.396454: step 8010, loss 0.00202921, acc 1
2016-11-12T19:15:14.456848: step 8011, loss 0.0148441, acc 0.984375
2016-11-12T19:15:14.516193: step 8012, loss 0.00209804, acc 1
2016-11-12T19:15:14.573119: step 8013, loss 0.0171108, acc 0.984375
2016-11-12T19:15:14.632760: step 8014, loss 0.000227052, acc 1
2016-11-12T19:15:14.689576: step 8015, loss 0.000362405, acc 1
2016-11-12T19:15:14.747066: step 8016, loss 0.000870345, acc 1
2016-11-12T19:15:14.803117: step 8017, loss 0.0009037, acc 1
2016-11-12T19:15:14.860895: step 8018, loss 0.0149733, acc 0.984375
2016-11-12T19:15:14.920036: step 8019, loss 0.000425863, acc 1
2016-11-12T19:15:14.976398: step 8020, loss 0.0391914, acc 0.984375
2016-11-12T19:15:15.036969: step 8021, loss 0.0175154, acc 0.984375
2016-11-12T19:15:15.094450: step 8022, loss 0.00876741, acc 1
2016-11-12T19:15:15.133627: step 8023, loss 1.59015e-05, acc 1
2016-11-12T19:15:15.196416: step 8024, loss 0.000281838, acc 1
2016-11-12T19:15:15.252070: step 8025, loss 0.0170969, acc 0.984375
2016-11-12T19:15:15.310051: step 8026, loss 0.000667523, acc 1
2016-11-12T19:15:15.367348: step 8027, loss 0.00217013, acc 1
2016-11-12T19:15:15.424167: step 8028, loss 0.000866531, acc 1
2016-11-12T19:15:15.482905: step 8029, loss 0.0777335, acc 0.96875
2016-11-12T19:15:15.541369: step 8030, loss 0.000878339, acc 1
2016-11-12T19:15:15.598536: step 8031, loss 0.00015377, acc 1
2016-11-12T19:15:15.654521: step 8032, loss 0.00113973, acc 1
2016-11-12T19:15:15.713564: step 8033, loss 0.00884534, acc 1
2016-11-12T19:15:15.771870: step 8034, loss 0.00665091, acc 1
2016-11-12T19:15:15.830525: step 8035, loss 0.0027459, acc 1
2016-11-12T19:15:15.892408: step 8036, loss 0.000130186, acc 1
2016-11-12T19:15:15.948922: step 8037, loss 0.000719819, acc 1
2016-11-12T19:15:16.005209: step 8038, loss 0.00123607, acc 1
2016-11-12T19:15:16.062445: step 8039, loss 7.44998e-05, acc 1
2016-11-12T19:15:16.118776: step 8040, loss 0.000412404, acc 1
2016-11-12T19:15:16.179971: step 8041, loss 0.00465276, acc 1
2016-11-12T19:15:16.244840: step 8042, loss 0.0073976, acc 1
2016-11-12T19:15:16.303170: step 8043, loss 0.00995408, acc 1
2016-11-12T19:15:16.360710: step 8044, loss 0.000981502, acc 1
2016-11-12T19:15:16.420866: step 8045, loss 0.00112519, acc 1
2016-11-12T19:15:16.476873: step 8046, loss 0.00822258, acc 1
2016-11-12T19:15:16.534961: step 8047, loss 0.000634557, acc 1
2016-11-12T19:15:16.592924: step 8048, loss 0.0137118, acc 0.984375
2016-11-12T19:15:16.650266: step 8049, loss 0.0054615, acc 1
2016-11-12T19:15:16.707869: step 8050, loss 0.000922567, acc 1
2016-11-12T19:15:16.764362: step 8051, loss 0.0102281, acc 1
2016-11-12T19:15:16.821101: step 8052, loss 0.00206753, acc 1
2016-11-12T19:15:16.880568: step 8053, loss 0.000882289, acc 1
2016-11-12T19:15:16.936809: step 8054, loss 0.000478467, acc 1
2016-11-12T19:15:16.992888: step 8055, loss 0.00745374, acc 1
2016-11-12T19:15:17.049965: step 8056, loss 0.0386675, acc 0.984375
2016-11-12T19:15:17.107878: step 8057, loss 0.00101688, acc 1
2016-11-12T19:15:17.165002: step 8058, loss 0.00059812, acc 1
2016-11-12T19:15:17.223219: step 8059, loss 0.0382996, acc 0.984375
2016-11-12T19:15:17.284867: step 8060, loss 0.0047159, acc 1
2016-11-12T19:15:17.342550: step 8061, loss 0.0121737, acc 1
2016-11-12T19:15:17.401387: step 8062, loss 0.000390741, acc 1
2016-11-12T19:15:17.460861: step 8063, loss 0.000263881, acc 1
2016-11-12T19:15:17.517676: step 8064, loss 0.00771532, acc 1
2016-11-12T19:15:17.575584: step 8065, loss 0.00550258, acc 1
2016-11-12T19:15:17.633285: step 8066, loss 0.00313565, acc 1
2016-11-12T19:15:17.691828: step 8067, loss 0.000439199, acc 1
2016-11-12T19:15:17.748816: step 8068, loss 0.00282098, acc 1
2016-11-12T19:15:17.808713: step 8069, loss 0.0168599, acc 0.984375
2016-11-12T19:15:17.867498: step 8070, loss 0.0402352, acc 0.96875
2016-11-12T19:15:17.924856: step 8071, loss 0.00207365, acc 1
2016-11-12T19:15:17.984401: step 8072, loss 0.0178791, acc 0.984375
2016-11-12T19:15:18.042802: step 8073, loss 0.0116421, acc 0.984375
2016-11-12T19:15:18.100411: step 8074, loss 0.0106709, acc 1
2016-11-12T19:15:18.157979: step 8075, loss 0.000311641, acc 1
2016-11-12T19:15:18.216913: step 8076, loss 0.0159338, acc 0.984375
2016-11-12T19:15:18.275730: step 8077, loss 0.00819332, acc 1
2016-11-12T19:15:18.332628: step 8078, loss 0.0598065, acc 0.984375
2016-11-12T19:15:18.391712: step 8079, loss 0.00367605, acc 1
2016-11-12T19:15:18.449324: step 8080, loss 0.000653404, acc 1
2016-11-12T19:15:18.506518: step 8081, loss 0.0346834, acc 0.984375
2016-11-12T19:15:18.565194: step 8082, loss 0.00992597, acc 1
2016-11-12T19:15:18.627994: step 8083, loss 0.0392133, acc 0.984375
2016-11-12T19:15:18.685921: step 8084, loss 0.250493, acc 0.984375
2016-11-12T19:15:18.744484: step 8085, loss 0.00429497, acc 1
2016-11-12T19:15:18.801923: step 8086, loss 0.000744179, acc 1
2016-11-12T19:15:18.861340: step 8087, loss 0.00193068, acc 1
2016-11-12T19:15:18.921384: step 8088, loss 0.00391643, acc 1
2016-11-12T19:15:18.979418: step 8089, loss 0.000358203, acc 1
2016-11-12T19:15:19.035805: step 8090, loss 0.000370397, acc 1
2016-11-12T19:15:19.092142: step 8091, loss 0.000715577, acc 1
2016-11-12T19:15:19.148651: step 8092, loss 0.0341874, acc 0.984375
2016-11-12T19:15:19.209247: step 8093, loss 0.00727777, acc 1
2016-11-12T19:15:19.250521: step 8094, loss 0.000742222, acc 1
2016-11-12T19:15:19.308646: step 8095, loss 0.0123548, acc 0.984375
2016-11-12T19:15:19.365460: step 8096, loss 0.0189513, acc 1
2016-11-12T19:15:19.422083: step 8097, loss 0.00068155, acc 1
2016-11-12T19:15:19.480294: step 8098, loss 0.00195206, acc 1
2016-11-12T19:15:19.537835: step 8099, loss 0.000756324, acc 1
2016-11-12T19:15:19.594789: step 8100, loss 0.0565674, acc 0.984375

Evaluation:
2016-11-12T19:15:19.666819: step 8100, loss 3.48963, acc 0.59

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-8100

2016-11-12T19:15:20.154847: step 8101, loss 0.000470099, acc 1
2016-11-12T19:15:20.212505: step 8102, loss 0.00342722, acc 1
2016-11-12T19:15:20.269275: step 8103, loss 0.0319047, acc 0.984375
2016-11-12T19:15:20.326912: step 8104, loss 0.0247607, acc 0.984375
2016-11-12T19:15:20.385710: step 8105, loss 0.0103721, acc 1
2016-11-12T19:15:20.444139: step 8106, loss 0.0160033, acc 0.984375
2016-11-12T19:15:20.503847: step 8107, loss 0.0290669, acc 0.984375
2016-11-12T19:15:20.561864: step 8108, loss 0.00667886, acc 1
2016-11-12T19:15:20.618999: step 8109, loss 0.00650356, acc 1
2016-11-12T19:15:20.678682: step 8110, loss 8.07252e-05, acc 1
2016-11-12T19:15:20.737651: step 8111, loss 0.00623057, acc 1
2016-11-12T19:15:20.795181: step 8112, loss 0.0140037, acc 0.984375
2016-11-12T19:15:20.853206: step 8113, loss 0.000571586, acc 1
2016-11-12T19:15:20.909386: step 8114, loss 0.000403019, acc 1
2016-11-12T19:15:20.966360: step 8115, loss 0.00101706, acc 1
2016-11-12T19:15:21.023828: step 8116, loss 0.000223418, acc 1
2016-11-12T19:15:21.080395: step 8117, loss 0.00106591, acc 1
2016-11-12T19:15:21.137186: step 8118, loss 0.003588, acc 1
2016-11-12T19:15:21.194834: step 8119, loss 0.0111116, acc 0.984375
2016-11-12T19:15:21.252961: step 8120, loss 0.00175242, acc 1
2016-11-12T19:15:21.312919: step 8121, loss 0.00436312, acc 1
2016-11-12T19:15:21.370078: step 8122, loss 0.0004138, acc 1
2016-11-12T19:15:21.429488: step 8123, loss 0.0973944, acc 0.984375
2016-11-12T19:15:21.491470: step 8124, loss 0.00308852, acc 1
2016-11-12T19:15:21.549066: step 8125, loss 0.000646221, acc 1
2016-11-12T19:15:21.607568: step 8126, loss 0.0004335, acc 1
2016-11-12T19:15:21.664876: step 8127, loss 0.00109235, acc 1
2016-11-12T19:15:21.724687: step 8128, loss 0.0124025, acc 1
2016-11-12T19:15:21.784849: step 8129, loss 0.0129972, acc 0.984375
2016-11-12T19:15:21.844112: step 8130, loss 0.00990713, acc 1
2016-11-12T19:15:21.902211: step 8131, loss 0.0221256, acc 0.984375
2016-11-12T19:15:21.961341: step 8132, loss 0.00109007, acc 1
2016-11-12T19:15:22.018752: step 8133, loss 0.000626067, acc 1
2016-11-12T19:15:22.077135: step 8134, loss 0.0091405, acc 1
2016-11-12T19:15:22.136478: step 8135, loss 0.000750989, acc 1
2016-11-12T19:15:22.194105: step 8136, loss 0.00146763, acc 1
2016-11-12T19:15:22.251172: step 8137, loss 0.000155633, acc 1
2016-11-12T19:15:22.307571: step 8138, loss 0.029537, acc 0.984375
2016-11-12T19:15:22.364902: step 8139, loss 0.0155503, acc 0.984375
2016-11-12T19:15:22.423813: step 8140, loss 0.0371629, acc 0.984375
2016-11-12T19:15:22.482292: step 8141, loss 0.00901533, acc 1
2016-11-12T19:15:22.541832: step 8142, loss 0.00134058, acc 1
2016-11-12T19:15:22.599895: step 8143, loss 0.000319196, acc 1
2016-11-12T19:15:22.656902: step 8144, loss 0.0301959, acc 0.96875
2016-11-12T19:15:22.716953: step 8145, loss 0.000327425, acc 1
2016-11-12T19:15:22.772575: step 8146, loss 0.0022828, acc 1
2016-11-12T19:15:22.832927: step 8147, loss 0.00295524, acc 1
2016-11-12T19:15:22.891048: step 8148, loss 0.00790244, acc 1
2016-11-12T19:15:22.950709: step 8149, loss 0.0153799, acc 0.984375
2016-11-12T19:15:23.009245: step 8150, loss 0.0280322, acc 0.96875
2016-11-12T19:15:23.068175: step 8151, loss 0.00212176, acc 1
2016-11-12T19:15:23.124936: step 8152, loss 0.000103162, acc 1
2016-11-12T19:15:23.181745: step 8153, loss 0.000245437, acc 1
2016-11-12T19:15:23.241008: step 8154, loss 0.00254324, acc 1
2016-11-12T19:15:23.300066: step 8155, loss 0.000229316, acc 1
2016-11-12T19:15:23.357128: step 8156, loss 0.00250285, acc 1
2016-11-12T19:15:23.417495: step 8157, loss 0.00713311, acc 1
2016-11-12T19:15:23.477094: step 8158, loss 0.00272831, acc 1
2016-11-12T19:15:23.535324: step 8159, loss 0.0099082, acc 1
2016-11-12T19:15:23.593100: step 8160, loss 4.44438e-05, acc 1
2016-11-12T19:15:23.650145: step 8161, loss 0.0041362, acc 1
2016-11-12T19:15:23.710473: step 8162, loss 0.0363386, acc 0.984375
2016-11-12T19:15:23.768374: step 8163, loss 0.000909517, acc 1
2016-11-12T19:15:23.825314: step 8164, loss 0.000131379, acc 1
2016-11-12T19:15:23.863905: step 8165, loss 0.000689946, acc 1
2016-11-12T19:15:23.924295: step 8166, loss 6.91678e-05, acc 1
2016-11-12T19:15:23.980518: step 8167, loss 0.000237789, acc 1
2016-11-12T19:15:24.040610: step 8168, loss 0.000231156, acc 1
2016-11-12T19:15:24.096357: step 8169, loss 0.0118131, acc 0.984375
2016-11-12T19:15:24.157627: step 8170, loss 0.00237386, acc 1
2016-11-12T19:15:24.217099: step 8171, loss 0.0111936, acc 1
2016-11-12T19:15:24.274706: step 8172, loss 0.000414082, acc 1
2016-11-12T19:15:24.331037: step 8173, loss 0.00142994, acc 1
2016-11-12T19:15:24.392379: step 8174, loss 0.00361795, acc 1
2016-11-12T19:15:24.450005: step 8175, loss 0.000349133, acc 1
2016-11-12T19:15:24.507006: step 8176, loss 0.0152575, acc 0.984375
2016-11-12T19:15:24.564802: step 8177, loss 0.000974356, acc 1
2016-11-12T19:15:24.621143: step 8178, loss 0.000176041, acc 1
2016-11-12T19:15:24.681083: step 8179, loss 0.0102984, acc 1
2016-11-12T19:15:24.740845: step 8180, loss 0.00230333, acc 1
2016-11-12T19:15:24.797450: step 8181, loss 0.00612069, acc 1
2016-11-12T19:15:24.854732: step 8182, loss 0.000540666, acc 1
2016-11-12T19:15:24.913065: step 8183, loss 0.0147786, acc 0.984375
2016-11-12T19:15:24.973250: step 8184, loss 0.000756227, acc 1
2016-11-12T19:15:25.029996: step 8185, loss 0.0131562, acc 0.984375
2016-11-12T19:15:25.088051: step 8186, loss 0.0431135, acc 0.984375
2016-11-12T19:15:25.148687: step 8187, loss 0.0301158, acc 0.984375
2016-11-12T19:15:25.208850: step 8188, loss 0.0452566, acc 0.984375
2016-11-12T19:15:25.268885: step 8189, loss 8.80968e-05, acc 1
2016-11-12T19:15:25.325724: step 8190, loss 0.00133029, acc 1
2016-11-12T19:15:25.382286: step 8191, loss 0.0165594, acc 1
2016-11-12T19:15:25.441785: step 8192, loss 0.00154274, acc 1
2016-11-12T19:15:25.500949: step 8193, loss 0.00291551, acc 1
2016-11-12T19:15:25.558368: step 8194, loss 0.0118964, acc 0.984375
2016-11-12T19:15:25.617232: step 8195, loss 0.00105665, acc 1
2016-11-12T19:15:25.677600: step 8196, loss 0.00402095, acc 1
2016-11-12T19:15:25.736243: step 8197, loss 0.000231256, acc 1
2016-11-12T19:15:25.793209: step 8198, loss 0.00193965, acc 1
2016-11-12T19:15:25.852762: step 8199, loss 0.00365251, acc 1
2016-11-12T19:15:25.909869: step 8200, loss 0.00442811, acc 1

Evaluation:
2016-11-12T19:15:25.981563: step 8200, loss 3.44426, acc 0.574

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-8200

2016-11-12T19:15:26.470356: step 8201, loss 0.00974915, acc 1
2016-11-12T19:15:26.529367: step 8202, loss 0.000419895, acc 1
2016-11-12T19:15:26.588462: step 8203, loss 0.0070663, acc 1
2016-11-12T19:15:26.644908: step 8204, loss 0.00427856, acc 1
2016-11-12T19:15:26.702504: step 8205, loss 0.102378, acc 0.984375
2016-11-12T19:15:26.760374: step 8206, loss 0.00019956, acc 1
2016-11-12T19:15:26.817708: step 8207, loss 0.00474308, acc 1
2016-11-12T19:15:26.874718: step 8208, loss 0.000572527, acc 1
2016-11-12T19:15:26.931294: step 8209, loss 2.78404e-05, acc 1
2016-11-12T19:15:26.988924: step 8210, loss 0.0282735, acc 0.984375
2016-11-12T19:15:27.048402: step 8211, loss 3.17552e-05, acc 1
2016-11-12T19:15:27.104272: step 8212, loss 0.000307051, acc 1
2016-11-12T19:15:27.160553: step 8213, loss 0.00209555, acc 1
2016-11-12T19:15:27.216890: step 8214, loss 6.14377e-05, acc 1
2016-11-12T19:15:27.276172: step 8215, loss 0.000484414, acc 1
2016-11-12T19:15:27.337386: step 8216, loss 0.0104102, acc 1
2016-11-12T19:15:27.396906: step 8217, loss 0.00090442, acc 1
2016-11-12T19:15:27.456339: step 8218, loss 0.000640428, acc 1
2016-11-12T19:15:27.513232: step 8219, loss 0.000105636, acc 1
2016-11-12T19:15:27.571899: step 8220, loss 0.00407466, acc 1
2016-11-12T19:15:27.629224: step 8221, loss 0.00215085, acc 1
2016-11-12T19:15:27.688397: step 8222, loss 0.000351719, acc 1
2016-11-12T19:15:27.744586: step 8223, loss 0.0439341, acc 0.96875
2016-11-12T19:15:27.804052: step 8224, loss 0.00174131, acc 1
2016-11-12T19:15:27.861080: step 8225, loss 0.00098491, acc 1
2016-11-12T19:15:27.917800: step 8226, loss 4.53825e-05, acc 1
2016-11-12T19:15:27.974833: step 8227, loss 0.000427918, acc 1
2016-11-12T19:15:28.033620: step 8228, loss 0.0148491, acc 0.984375
2016-11-12T19:15:28.092223: step 8229, loss 0.00135935, acc 1
2016-11-12T19:15:28.149249: step 8230, loss 0.0146099, acc 0.984375
2016-11-12T19:15:28.207312: step 8231, loss 0.000518199, acc 1
2016-11-12T19:15:28.264943: step 8232, loss 7.99622e-05, acc 1
2016-11-12T19:15:28.321084: step 8233, loss 0.0258285, acc 0.984375
2016-11-12T19:15:28.378480: step 8234, loss 0.00189791, acc 1
2016-11-12T19:15:28.434529: step 8235, loss 0.0484219, acc 0.96875
2016-11-12T19:15:28.473012: step 8236, loss 0.0613114, acc 0.95
2016-11-12T19:15:28.532494: step 8237, loss 0.00258545, acc 1
2016-11-12T19:15:28.591105: step 8238, loss 0.00248776, acc 1
2016-11-12T19:15:28.649222: step 8239, loss 4.35094e-05, acc 1
2016-11-12T19:15:28.707594: step 8240, loss 0.0252454, acc 0.984375
2016-11-12T19:15:28.769412: step 8241, loss 0.013068, acc 1
2016-11-12T19:15:28.829070: step 8242, loss 0.0152949, acc 1
2016-11-12T19:15:28.888675: step 8243, loss 0.0090744, acc 1
2016-11-12T19:15:28.946938: step 8244, loss 0.000594132, acc 1
2016-11-12T19:15:29.005475: step 8245, loss 0.00155007, acc 1
2016-11-12T19:15:29.062184: step 8246, loss 0.00877298, acc 1
2016-11-12T19:15:29.119673: step 8247, loss 0.0343604, acc 0.984375
2016-11-12T19:15:29.177295: step 8248, loss 0.00199052, acc 1
2016-11-12T19:15:29.237850: step 8249, loss 0.0079232, acc 1
2016-11-12T19:15:29.295537: step 8250, loss 0.0495172, acc 0.96875
2016-11-12T19:15:29.354430: step 8251, loss 0.0280903, acc 0.96875
2016-11-12T19:15:29.412147: step 8252, loss 0.000499149, acc 1
2016-11-12T19:15:29.471253: step 8253, loss 0.0021447, acc 1
2016-11-12T19:15:29.528752: step 8254, loss 0.000837419, acc 1
2016-11-12T19:15:29.587113: step 8255, loss 0.0108824, acc 1
2016-11-12T19:15:29.644796: step 8256, loss 0.0101281, acc 1
2016-11-12T19:15:29.704553: step 8257, loss 0.012114, acc 0.984375
2016-11-12T19:15:29.763396: step 8258, loss 0.000127508, acc 1
2016-11-12T19:15:29.819777: step 8259, loss 0.000720757, acc 1
2016-11-12T19:15:29.876109: step 8260, loss 0.000289266, acc 1
2016-11-12T19:15:29.935055: step 8261, loss 0.000452865, acc 1
2016-11-12T19:15:29.993359: step 8262, loss 0.00363478, acc 1
2016-11-12T19:15:30.051866: step 8263, loss 0.0282053, acc 0.96875
2016-11-12T19:15:30.109355: step 8264, loss 0.000442395, acc 1
2016-11-12T19:15:30.166313: step 8265, loss 0.0252316, acc 0.984375
2016-11-12T19:15:30.224368: step 8266, loss 0.000553244, acc 1
2016-11-12T19:15:30.281058: step 8267, loss 0.00168272, acc 1
2016-11-12T19:15:30.340661: step 8268, loss 0.0474199, acc 0.984375
2016-11-12T19:15:30.399384: step 8269, loss 0.000431448, acc 1
2016-11-12T19:15:30.457124: step 8270, loss 0.000465705, acc 1
2016-11-12T19:15:30.516075: step 8271, loss 0.00463403, acc 1
2016-11-12T19:15:30.573080: step 8272, loss 0.00233558, acc 1
2016-11-12T19:15:30.632184: step 8273, loss 0.00441998, acc 1
2016-11-12T19:15:30.690382: step 8274, loss 0.1291, acc 0.984375
2016-11-12T19:15:30.748841: step 8275, loss 0.0105573, acc 1
2016-11-12T19:15:30.808451: step 8276, loss 0.00553132, acc 1
2016-11-12T19:15:30.865882: step 8277, loss 0.0239903, acc 0.984375
2016-11-12T19:15:30.925754: step 8278, loss 0.00211342, acc 1
2016-11-12T19:15:30.984743: step 8279, loss 0.000284751, acc 1
2016-11-12T19:15:31.040613: step 8280, loss 0.010087, acc 1
2016-11-12T19:15:31.097779: step 8281, loss 0.0345269, acc 0.984375
2016-11-12T19:15:31.156617: step 8282, loss 0.000606128, acc 1
2016-11-12T19:15:31.212910: step 8283, loss 0.000742208, acc 1
2016-11-12T19:15:31.269209: step 8284, loss 0.0125507, acc 0.984375
2016-11-12T19:15:31.326209: step 8285, loss 0.00695271, acc 1
2016-11-12T19:15:31.384788: step 8286, loss 0.00074222, acc 1
2016-11-12T19:15:31.442239: step 8287, loss 0.000190906, acc 1
2016-11-12T19:15:31.498684: step 8288, loss 0.000345961, acc 1
2016-11-12T19:15:31.556206: step 8289, loss 0.000973189, acc 1
2016-11-12T19:15:31.612216: step 8290, loss 0.0054554, acc 1
2016-11-12T19:15:31.671401: step 8291, loss 0.00593669, acc 1
2016-11-12T19:15:31.728715: step 8292, loss 0.049431, acc 0.984375
2016-11-12T19:15:31.787196: step 8293, loss 0.00223308, acc 1
2016-11-12T19:15:31.844301: step 8294, loss 0.0211964, acc 0.984375
2016-11-12T19:15:31.905164: step 8295, loss 0.000866879, acc 1
2016-11-12T19:15:31.961355: step 8296, loss 0.0136491, acc 1
2016-11-12T19:15:32.019486: step 8297, loss 0.0254595, acc 0.984375
2016-11-12T19:15:32.079341: step 8298, loss 0.00420073, acc 1
2016-11-12T19:15:32.140500: step 8299, loss 1.27341e-05, acc 1
2016-11-12T19:15:32.197356: step 8300, loss 0.0196518, acc 0.984375

Evaluation:
2016-11-12T19:15:32.269490: step 8300, loss 3.56146, acc 0.566

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-8300

2016-11-12T19:15:32.757124: step 8301, loss 0.00254585, acc 1
2016-11-12T19:15:32.816935: step 8302, loss 0.0272898, acc 0.984375
2016-11-12T19:15:32.875128: step 8303, loss 0.0214817, acc 0.984375
2016-11-12T19:15:32.933490: step 8304, loss 0.000579049, acc 1
2016-11-12T19:15:32.994061: step 8305, loss 0.00149104, acc 1
2016-11-12T19:15:33.049933: step 8306, loss 0.000244257, acc 1
2016-11-12T19:15:33.088038: step 8307, loss 0.00376254, acc 1
2016-11-12T19:15:33.148740: step 8308, loss 0.00338614, acc 1
2016-11-12T19:15:33.205799: step 8309, loss 0.0193469, acc 0.984375
2016-11-12T19:15:33.264152: step 8310, loss 0.00587702, acc 1
2016-11-12T19:15:33.323636: step 8311, loss 0.000375971, acc 1
2016-11-12T19:15:33.380961: step 8312, loss 0.0173045, acc 0.984375
2016-11-12T19:15:33.439320: step 8313, loss 0.00115482, acc 1
2016-11-12T19:15:33.497361: step 8314, loss 0.00164699, acc 1
2016-11-12T19:15:33.552828: step 8315, loss 0.00408712, acc 1
2016-11-12T19:15:33.609057: step 8316, loss 0.0334492, acc 0.96875
2016-11-12T19:15:33.665798: step 8317, loss 0.000613817, acc 1
2016-11-12T19:15:33.725161: step 8318, loss 0.000727502, acc 1
2016-11-12T19:15:33.787405: step 8319, loss 0.0269403, acc 0.984375
2016-11-12T19:15:33.845170: step 8320, loss 0.00574972, acc 1
2016-11-12T19:15:33.901475: step 8321, loss 0.000218039, acc 1
2016-11-12T19:15:33.958540: step 8322, loss 0.00589816, acc 1
2016-11-12T19:15:34.017399: step 8323, loss 0.000413996, acc 1
2016-11-12T19:15:34.075505: step 8324, loss 0.000427739, acc 1
2016-11-12T19:15:34.133217: step 8325, loss 0.000869511, acc 1
2016-11-12T19:15:34.193036: step 8326, loss 0.000917309, acc 1
2016-11-12T19:15:34.253602: step 8327, loss 0.00125466, acc 1
2016-11-12T19:15:34.313171: step 8328, loss 6.69633e-05, acc 1
2016-11-12T19:15:34.369543: step 8329, loss 0.000200874, acc 1
2016-11-12T19:15:34.426255: step 8330, loss 0.0052827, acc 1
2016-11-12T19:15:34.485641: step 8331, loss 0.00325668, acc 1
2016-11-12T19:15:34.545103: step 8332, loss 0.00611663, acc 1
2016-11-12T19:15:34.604142: step 8333, loss 0.0507274, acc 0.953125
2016-11-12T19:15:34.662488: step 8334, loss 0.0255811, acc 0.984375
2016-11-12T19:15:34.719629: step 8335, loss 0.0271051, acc 1
2016-11-12T19:15:34.781087: step 8336, loss 0.0180021, acc 0.984375
2016-11-12T19:15:34.838702: step 8337, loss 0.0213745, acc 0.984375
2016-11-12T19:15:34.896230: step 8338, loss 6.65297e-05, acc 1
2016-11-12T19:15:34.954746: step 8339, loss 0.0019345, acc 1
2016-11-12T19:15:35.013385: step 8340, loss 0.000663431, acc 1
2016-11-12T19:15:35.073020: step 8341, loss 0.00902935, acc 1
2016-11-12T19:15:35.130527: step 8342, loss 0.00236535, acc 1
2016-11-12T19:15:35.187905: step 8343, loss 0.00830287, acc 1
2016-11-12T19:15:35.246056: step 8344, loss 0.00159193, acc 1
2016-11-12T19:15:35.304632: step 8345, loss 0.0458482, acc 0.984375
2016-11-12T19:15:35.362090: step 8346, loss 0.000953583, acc 1
2016-11-12T19:15:35.420905: step 8347, loss 0.000980428, acc 1
2016-11-12T19:15:35.479359: step 8348, loss 0.000227778, acc 1
2016-11-12T19:15:35.537162: step 8349, loss 0.000431135, acc 1
2016-11-12T19:15:35.593331: step 8350, loss 0.0143542, acc 1
2016-11-12T19:15:35.651779: step 8351, loss 0.00100451, acc 1
2016-11-12T19:15:35.710621: step 8352, loss 0.00226556, acc 1
2016-11-12T19:15:35.767896: step 8353, loss 0.0512526, acc 0.96875
2016-11-12T19:15:35.825215: step 8354, loss 0.00496392, acc 1
2016-11-12T19:15:35.883893: step 8355, loss 0.00456353, acc 1
2016-11-12T19:15:35.944948: step 8356, loss 0.328785, acc 0.96875
2016-11-12T19:15:36.004624: step 8357, loss 0.0281292, acc 0.96875
2016-11-12T19:15:36.062994: step 8358, loss 0.0207121, acc 0.984375
2016-11-12T19:15:36.121157: step 8359, loss 0.00323217, acc 1
2016-11-12T19:15:36.181071: step 8360, loss 0.000390478, acc 1
2016-11-12T19:15:36.240889: step 8361, loss 0.0372438, acc 0.984375
2016-11-12T19:15:36.299399: step 8362, loss 7.60375e-05, acc 1
2016-11-12T19:15:36.354501: step 8363, loss 0.00692622, acc 1
2016-11-12T19:15:36.412825: step 8364, loss 0.00691489, acc 1
2016-11-12T19:15:36.470015: step 8365, loss 0.000330047, acc 1
2016-11-12T19:15:36.526928: step 8366, loss 0.000496497, acc 1
2016-11-12T19:15:36.588236: step 8367, loss 0.000159941, acc 1
2016-11-12T19:15:36.647888: step 8368, loss 0.000355809, acc 1
2016-11-12T19:15:36.707477: step 8369, loss 0.0184682, acc 0.984375
2016-11-12T19:15:36.765201: step 8370, loss 0.00343654, acc 1
2016-11-12T19:15:36.822312: step 8371, loss 0.00109461, acc 1
2016-11-12T19:15:36.881884: step 8372, loss 0.00220721, acc 1
2016-11-12T19:15:36.940190: step 8373, loss 0.00151676, acc 1
2016-11-12T19:15:36.999030: step 8374, loss 0.000180284, acc 1
2016-11-12T19:15:37.055034: step 8375, loss 0.00384241, acc 1
2016-11-12T19:15:37.114748: step 8376, loss 4.84719e-05, acc 1
2016-11-12T19:15:37.170951: step 8377, loss 0.0107303, acc 1
2016-11-12T19:15:37.211141: step 8378, loss 0.000457654, acc 1
2016-11-12T19:15:37.269245: step 8379, loss 0.0412892, acc 0.96875
2016-11-12T19:15:37.330787: step 8380, loss 0.000373682, acc 1
2016-11-12T19:15:37.388375: step 8381, loss 0.0054414, acc 1
2016-11-12T19:15:37.447809: step 8382, loss 0.000148151, acc 1
2016-11-12T19:15:37.505058: step 8383, loss 0.0178069, acc 1
2016-11-12T19:15:37.563532: step 8384, loss 0.000769178, acc 1
2016-11-12T19:15:37.622931: step 8385, loss 0.000268707, acc 1
2016-11-12T19:15:37.680934: step 8386, loss 0.0104607, acc 1
2016-11-12T19:15:37.740876: step 8387, loss 0.00038163, acc 1
2016-11-12T19:15:37.797029: step 8388, loss 0.0201488, acc 0.984375
2016-11-12T19:15:37.859169: step 8389, loss 1.9676e-05, acc 1
2016-11-12T19:15:37.918332: step 8390, loss 0.0195179, acc 0.984375
2016-11-12T19:15:37.976500: step 8391, loss 0.00203316, acc 1
2016-11-12T19:15:38.033783: step 8392, loss 0.00519626, acc 1
2016-11-12T19:15:38.089878: step 8393, loss 0.000417279, acc 1
2016-11-12T19:15:38.147356: step 8394, loss 0.0276158, acc 0.984375
2016-11-12T19:15:38.205214: step 8395, loss 0.0379168, acc 0.96875
2016-11-12T19:15:38.264166: step 8396, loss 0.000856811, acc 1
2016-11-12T19:15:38.321790: step 8397, loss 0.0197845, acc 1
2016-11-12T19:15:38.380767: step 8398, loss 0.0129183, acc 0.984375
2016-11-12T19:15:38.440493: step 8399, loss 0.00204572, acc 1
2016-11-12T19:15:38.500480: step 8400, loss 8.85117e-05, acc 1

Evaluation:
2016-11-12T19:15:38.571732: step 8400, loss 3.61424, acc 0.564

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-8400

2016-11-12T19:15:39.060743: step 8401, loss 0.0506695, acc 0.96875
2016-11-12T19:15:39.120413: step 8402, loss 0.00125119, acc 1
2016-11-12T19:15:39.177084: step 8403, loss 3.77854e-05, acc 1
2016-11-12T19:15:39.233275: step 8404, loss 0.00361114, acc 1
2016-11-12T19:15:39.290415: step 8405, loss 2.31961e-05, acc 1
2016-11-12T19:15:39.348985: step 8406, loss 4.88411e-05, acc 1
2016-11-12T19:15:39.405389: step 8407, loss 0.0323351, acc 0.984375
2016-11-12T19:15:39.464590: step 8408, loss 0.00414055, acc 1
2016-11-12T19:15:39.522717: step 8409, loss 0.000245112, acc 1
2016-11-12T19:15:39.581520: step 8410, loss 0.00438819, acc 1
2016-11-12T19:15:39.641166: step 8411, loss 0.0905203, acc 0.984375
2016-11-12T19:15:39.702235: step 8412, loss 0.000461829, acc 1
2016-11-12T19:15:39.761971: step 8413, loss 0.0146806, acc 0.984375
2016-11-12T19:15:39.820516: step 8414, loss 0.00679335, acc 1
2016-11-12T19:15:39.879884: step 8415, loss 0.001573, acc 1
2016-11-12T19:15:39.939186: step 8416, loss 0.000601664, acc 1
2016-11-12T19:15:39.996052: step 8417, loss 0.000610237, acc 1
2016-11-12T19:15:40.053211: step 8418, loss 0.0467802, acc 0.984375
2016-11-12T19:15:40.112323: step 8419, loss 0.00103313, acc 1
2016-11-12T19:15:40.168908: step 8420, loss 0.000529845, acc 1
2016-11-12T19:15:40.229634: step 8421, loss 0.00171092, acc 1
2016-11-12T19:15:40.288475: step 8422, loss 0.0244275, acc 0.984375
2016-11-12T19:15:40.348797: step 8423, loss 0.000426107, acc 1
2016-11-12T19:15:40.409284: step 8424, loss 0.00100088, acc 1
2016-11-12T19:15:40.467879: step 8425, loss 0.000622824, acc 1
2016-11-12T19:15:40.525487: step 8426, loss 0.0432595, acc 0.984375
2016-11-12T19:15:40.585583: step 8427, loss 0.000381758, acc 1
2016-11-12T19:15:40.645319: step 8428, loss 0.000130041, acc 1
2016-11-12T19:15:40.703794: step 8429, loss 0.0133998, acc 0.984375
2016-11-12T19:15:40.764839: step 8430, loss 0.00251442, acc 1
2016-11-12T19:15:40.828871: step 8431, loss 0.013859, acc 0.984375
2016-11-12T19:15:40.887891: step 8432, loss 0.00388225, acc 1
2016-11-12T19:15:40.946213: step 8433, loss 0.0213747, acc 0.984375
2016-11-12T19:15:41.004921: step 8434, loss 0.00632134, acc 1
2016-11-12T19:15:41.064740: step 8435, loss 0.000935691, acc 1
2016-11-12T19:15:41.121071: step 8436, loss 0.000949186, acc 1
2016-11-12T19:15:41.180474: step 8437, loss 0.0203674, acc 0.984375
2016-11-12T19:15:41.238670: step 8438, loss 0.00257819, acc 1
2016-11-12T19:15:41.298003: step 8439, loss 0.00300432, acc 1
2016-11-12T19:15:41.356830: step 8440, loss 0.000299044, acc 1
2016-11-12T19:15:41.413499: step 8441, loss 0.000638673, acc 1
2016-11-12T19:15:41.472956: step 8442, loss 0.000309481, acc 1
2016-11-12T19:15:41.530513: step 8443, loss 0.000833107, acc 1
2016-11-12T19:15:41.587545: step 8444, loss 0.00220877, acc 1
2016-11-12T19:15:41.644452: step 8445, loss 0.00076507, acc 1
2016-11-12T19:15:41.700418: step 8446, loss 0.0117384, acc 1
2016-11-12T19:15:41.761022: step 8447, loss 0.000761208, acc 1
2016-11-12T19:15:41.818068: step 8448, loss 0.164801, acc 0.96875
2016-11-12T19:15:41.859741: step 8449, loss 0.000867364, acc 1
2016-11-12T19:15:41.919041: step 8450, loss 0.000880966, acc 1
2016-11-12T19:15:41.976511: step 8451, loss 0.000545099, acc 1
2016-11-12T19:15:42.033192: step 8452, loss 0.00910703, acc 1
2016-11-12T19:15:42.093158: step 8453, loss 0.000532845, acc 1
2016-11-12T19:15:42.149760: step 8454, loss 0.0087554, acc 1
2016-11-12T19:15:42.209951: step 8455, loss 0.00581192, acc 1
2016-11-12T19:15:42.269253: step 8456, loss 0.0145993, acc 1
2016-11-12T19:15:42.327921: step 8457, loss 0.000394485, acc 1
2016-11-12T19:15:42.385075: step 8458, loss 0.00265478, acc 1
2016-11-12T19:15:42.443381: step 8459, loss 0.00032539, acc 1
2016-11-12T19:15:42.499488: step 8460, loss 0.000137033, acc 1
2016-11-12T19:15:42.557906: step 8461, loss 0.000174477, acc 1
2016-11-12T19:15:42.617342: step 8462, loss 0.00158384, acc 1
2016-11-12T19:15:42.677345: step 8463, loss 0.0174366, acc 0.984375
2016-11-12T19:15:42.736472: step 8464, loss 0.0512707, acc 0.984375
2016-11-12T19:15:42.794512: step 8465, loss 0.000473152, acc 1
2016-11-12T19:15:42.852389: step 8466, loss 0.0286606, acc 0.984375
2016-11-12T19:15:42.911002: step 8467, loss 0.000362141, acc 1
2016-11-12T19:15:42.968814: step 8468, loss 0.0107462, acc 1
2016-11-12T19:15:43.026051: step 8469, loss 0.000278712, acc 1
2016-11-12T19:15:43.084401: step 8470, loss 0.00685607, acc 1
2016-11-12T19:15:43.142786: step 8471, loss 0.01075, acc 1
2016-11-12T19:15:43.200385: step 8472, loss 0.0302077, acc 0.984375
2016-11-12T19:15:43.261244: step 8473, loss 0.00472554, acc 1
2016-11-12T19:15:43.316925: step 8474, loss 0.00089688, acc 1
2016-11-12T19:15:43.373690: step 8475, loss 0.000860939, acc 1
2016-11-12T19:15:43.432800: step 8476, loss 0.252273, acc 0.984375
2016-11-12T19:15:43.491319: step 8477, loss 0.000324768, acc 1
2016-11-12T19:15:43.548539: step 8478, loss 0.000621305, acc 1
2016-11-12T19:15:43.607052: step 8479, loss 0.00287839, acc 1
2016-11-12T19:15:43.666679: step 8480, loss 0.00251486, acc 1
2016-11-12T19:15:43.727073: step 8481, loss 0.00199997, acc 1
2016-11-12T19:15:43.784821: step 8482, loss 0.000651998, acc 1
2016-11-12T19:15:43.846065: step 8483, loss 0.0291062, acc 0.984375
2016-11-12T19:15:43.903093: step 8484, loss 0.0537822, acc 0.984375
2016-11-12T19:15:43.961031: step 8485, loss 0.000160523, acc 1
2016-11-12T19:15:44.017373: step 8486, loss 0.014055, acc 0.984375
2016-11-12T19:15:44.075666: step 8487, loss 0.00103662, acc 1
2016-11-12T19:15:44.132704: step 8488, loss 8.40478e-05, acc 1
2016-11-12T19:15:44.188950: step 8489, loss 0.00672956, acc 1
2016-11-12T19:15:44.246235: step 8490, loss 0.00128419, acc 1
2016-11-12T19:15:44.303361: step 8491, loss 0.000334641, acc 1
2016-11-12T19:15:44.360591: step 8492, loss 0.010495, acc 1
2016-11-12T19:15:44.421114: step 8493, loss 0.0289447, acc 0.984375
2016-11-12T19:15:44.480920: step 8494, loss 0.0110424, acc 1
2016-11-12T19:15:44.540692: step 8495, loss 0.000278474, acc 1
2016-11-12T19:15:44.598591: step 8496, loss 0.00116362, acc 1
2016-11-12T19:15:44.655935: step 8497, loss 0.000764236, acc 1
2016-11-12T19:15:44.712961: step 8498, loss 0.0134963, acc 0.984375
2016-11-12T19:15:44.771819: step 8499, loss 0.00718313, acc 1
2016-11-12T19:15:44.830783: step 8500, loss 0.01, acc 1

Evaluation:
2016-11-12T19:15:44.902168: step 8500, loss 3.60528, acc 0.57

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-8500

2016-11-12T19:15:45.389244: step 8501, loss 0.00142598, acc 1
2016-11-12T19:15:45.446399: step 8502, loss 0.00379551, acc 1
2016-11-12T19:15:45.505474: step 8503, loss 0.00474224, acc 1
2016-11-12T19:15:45.562582: step 8504, loss 0.000590762, acc 1
2016-11-12T19:15:45.619206: step 8505, loss 0.00296125, acc 1
2016-11-12T19:15:45.677755: step 8506, loss 0.000498431, acc 1
2016-11-12T19:15:45.737304: step 8507, loss 0.000294669, acc 1
2016-11-12T19:15:45.797285: step 8508, loss 0.000558679, acc 1
2016-11-12T19:15:45.856991: step 8509, loss 0.0184977, acc 0.984375
2016-11-12T19:15:45.916219: step 8510, loss 0.000463483, acc 1
2016-11-12T19:15:45.974501: step 8511, loss 0.000489567, acc 1
2016-11-12T19:15:46.032063: step 8512, loss 0.000503883, acc 1
2016-11-12T19:15:46.091085: step 8513, loss 0.00207327, acc 1
2016-11-12T19:15:46.152382: step 8514, loss 0.000554005, acc 1
2016-11-12T19:15:46.209099: step 8515, loss 0.0618839, acc 0.984375
2016-11-12T19:15:46.269234: step 8516, loss 0.00172598, acc 1
2016-11-12T19:15:46.328929: step 8517, loss 0.0154198, acc 0.984375
2016-11-12T19:15:46.388108: step 8518, loss 0.02042, acc 0.984375
2016-11-12T19:15:46.445618: step 8519, loss 0.000251183, acc 1
2016-11-12T19:15:46.482684: step 8520, loss 0.0153491, acc 1
2016-11-12T19:15:46.544796: step 8521, loss 0.00968004, acc 1
2016-11-12T19:15:46.605039: step 8522, loss 0.0124008, acc 0.984375
2016-11-12T19:15:46.664104: step 8523, loss 0.0127488, acc 1
2016-11-12T19:15:46.724265: step 8524, loss 0.00087115, acc 1
2016-11-12T19:15:46.781415: step 8525, loss 0.00774271, acc 1
2016-11-12T19:15:46.839747: step 8526, loss 0.0139299, acc 0.984375
2016-11-12T19:15:46.898083: step 8527, loss 0.00274717, acc 1
2016-11-12T19:15:46.957276: step 8528, loss 0.00318304, acc 1
2016-11-12T19:15:47.017777: step 8529, loss 0.0872302, acc 0.96875
2016-11-12T19:15:47.077978: step 8530, loss 0.0128693, acc 1
2016-11-12T19:15:47.138076: step 8531, loss 0.00188626, acc 1
2016-11-12T19:15:47.197668: step 8532, loss 0.0227096, acc 0.984375
2016-11-12T19:15:47.257328: step 8533, loss 0.000587231, acc 1
2016-11-12T19:15:47.315022: step 8534, loss 0.0106624, acc 1
2016-11-12T19:15:47.372303: step 8535, loss 0.0169975, acc 0.984375
2016-11-12T19:15:47.429629: step 8536, loss 0.0302559, acc 0.984375
2016-11-12T19:15:47.489568: step 8537, loss 0.0248513, acc 0.984375
2016-11-12T19:15:47.548253: step 8538, loss 0.042558, acc 0.984375
2016-11-12T19:15:47.606697: step 8539, loss 0.0395511, acc 0.984375
2016-11-12T19:15:47.664473: step 8540, loss 0.00028606, acc 1
2016-11-12T19:15:47.721760: step 8541, loss 0.00822363, acc 1
2016-11-12T19:15:47.779600: step 8542, loss 0.000304936, acc 1
2016-11-12T19:15:47.835528: step 8543, loss 0.000145387, acc 1
2016-11-12T19:15:47.892779: step 8544, loss 0.00723324, acc 1
2016-11-12T19:15:47.950564: step 8545, loss 0.0183377, acc 1
2016-11-12T19:15:48.009864: step 8546, loss 0.012483, acc 1
2016-11-12T19:15:48.069074: step 8547, loss 0.0829507, acc 0.984375
2016-11-12T19:15:48.130139: step 8548, loss 0.00994853, acc 1
2016-11-12T19:15:48.188653: step 8549, loss 0.000807128, acc 1
2016-11-12T19:15:48.245089: step 8550, loss 0.0548083, acc 0.96875
2016-11-12T19:15:48.304041: step 8551, loss 0.0268133, acc 0.984375
2016-11-12T19:15:48.361768: step 8552, loss 0.000994774, acc 1
2016-11-12T19:15:48.418467: step 8553, loss 0.00302023, acc 1
2016-11-12T19:15:48.478586: step 8554, loss 0.00132452, acc 1
2016-11-12T19:15:48.536122: step 8555, loss 0.0492154, acc 0.953125
2016-11-12T19:15:48.593858: step 8556, loss 0.00204146, acc 1
2016-11-12T19:15:48.651488: step 8557, loss 0.0425286, acc 0.96875
2016-11-12T19:15:48.709491: step 8558, loss 0.00433067, acc 1
2016-11-12T19:15:48.765821: step 8559, loss 0.000167682, acc 1
2016-11-12T19:15:48.822388: step 8560, loss 0.00133753, acc 1
2016-11-12T19:15:48.879029: step 8561, loss 0.0020163, acc 1
2016-11-12T19:15:48.936188: step 8562, loss 0.00268077, acc 1
2016-11-12T19:15:48.993067: step 8563, loss 0.000476866, acc 1
2016-11-12T19:15:49.049199: step 8564, loss 0.000700311, acc 1
2016-11-12T19:15:49.105792: step 8565, loss 0.00497383, acc 1
2016-11-12T19:15:49.163790: step 8566, loss 0.000630811, acc 1
2016-11-12T19:15:49.220561: step 8567, loss 0.0297041, acc 0.96875
2016-11-12T19:15:49.281007: step 8568, loss 0.00713439, acc 1
2016-11-12T19:15:49.336953: step 8569, loss 0.0178326, acc 0.984375
2016-11-12T19:15:49.396398: step 8570, loss 0.000869229, acc 1
2016-11-12T19:15:49.453778: step 8571, loss 0.0197368, acc 0.984375
2016-11-12T19:15:49.512116: step 8572, loss 0.0226249, acc 0.984375
2016-11-12T19:15:49.570287: step 8573, loss 0.0150591, acc 0.984375
2016-11-12T19:15:49.629043: step 8574, loss 0.000879427, acc 1
2016-11-12T19:15:49.689005: step 8575, loss 0.00168788, acc 1
2016-11-12T19:15:49.746905: step 8576, loss 0.000567403, acc 1
2016-11-12T19:15:49.804429: step 8577, loss 0.00104252, acc 1
2016-11-12T19:15:49.861937: step 8578, loss 0.000854743, acc 1
2016-11-12T19:15:49.918933: step 8579, loss 0.000232366, acc 1
2016-11-12T19:15:49.976376: step 8580, loss 0.00518958, acc 1
2016-11-12T19:15:50.033427: step 8581, loss 0.000349937, acc 1
2016-11-12T19:15:50.092710: step 8582, loss 0.0545553, acc 0.984375
2016-11-12T19:15:50.153321: step 8583, loss 0.00152626, acc 1
2016-11-12T19:15:50.211236: step 8584, loss 0.0289513, acc 0.984375
2016-11-12T19:15:50.268766: step 8585, loss 0.000230412, acc 1
2016-11-12T19:15:50.329146: step 8586, loss 0.0487386, acc 0.984375
2016-11-12T19:15:50.388235: step 8587, loss 0.0794362, acc 0.984375
2016-11-12T19:15:50.449379: step 8588, loss 0.0634999, acc 0.96875
2016-11-12T19:15:50.508850: step 8589, loss 0.00811475, acc 1
2016-11-12T19:15:50.568239: step 8590, loss 0.00495044, acc 1
2016-11-12T19:15:50.606871: step 8591, loss 0.00105181, acc 1
2016-11-12T19:15:50.667567: step 8592, loss 0.00136943, acc 1
2016-11-12T19:15:50.726560: step 8593, loss 0.000452633, acc 1
2016-11-12T19:15:50.785537: step 8594, loss 0.000101041, acc 1
2016-11-12T19:15:50.841674: step 8595, loss 0.000144328, acc 1
2016-11-12T19:15:50.900165: step 8596, loss 0.045119, acc 0.984375
2016-11-12T19:15:50.960156: step 8597, loss 0.00478801, acc 1
2016-11-12T19:15:51.021439: step 8598, loss 0.000348889, acc 1
2016-11-12T19:15:51.081123: step 8599, loss 0.0214129, acc 1
2016-11-12T19:15:51.137944: step 8600, loss 0.0110082, acc 1

Evaluation:
2016-11-12T19:15:51.209355: step 8600, loss 3.75759, acc 0.56

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-8600

2016-11-12T19:15:51.696284: step 8601, loss 0.00452583, acc 1
2016-11-12T19:15:51.756496: step 8602, loss 0.00062521, acc 1
2016-11-12T19:15:51.813122: step 8603, loss 0.0103365, acc 1
2016-11-12T19:15:51.873169: step 8604, loss 0.000270446, acc 1
2016-11-12T19:15:51.933139: step 8605, loss 0.00587648, acc 1
2016-11-12T19:15:51.991023: step 8606, loss 0.0468675, acc 0.984375
2016-11-12T19:15:52.049349: step 8607, loss 0.00585927, acc 1
2016-11-12T19:15:52.108888: step 8608, loss 0.000859243, acc 1
2016-11-12T19:15:52.165884: step 8609, loss 0.00173474, acc 1
2016-11-12T19:15:52.224635: step 8610, loss 0.000202585, acc 1
2016-11-12T19:15:52.281785: step 8611, loss 0.0245281, acc 0.984375
2016-11-12T19:15:52.340345: step 8612, loss 0.11025, acc 0.984375
2016-11-12T19:15:52.400915: step 8613, loss 0.00167258, acc 1
2016-11-12T19:15:52.460904: step 8614, loss 0.00539864, acc 1
2016-11-12T19:15:52.518592: step 8615, loss 0.000896066, acc 1
2016-11-12T19:15:52.576836: step 8616, loss 0.000409385, acc 1
2016-11-12T19:15:52.633243: step 8617, loss 0.00262033, acc 1
2016-11-12T19:15:52.690522: step 8618, loss 0.0148477, acc 0.984375
2016-11-12T19:15:52.747971: step 8619, loss 0.000135068, acc 1
2016-11-12T19:15:52.804692: step 8620, loss 0.000117514, acc 1
2016-11-12T19:15:52.863020: step 8621, loss 0.000191807, acc 1
2016-11-12T19:15:52.921362: step 8622, loss 0.00205126, acc 1
2016-11-12T19:15:52.984198: step 8623, loss 0.0280174, acc 0.984375
2016-11-12T19:15:53.045541: step 8624, loss 0.0298458, acc 0.984375
2016-11-12T19:15:53.105059: step 8625, loss 0.000456523, acc 1
2016-11-12T19:15:53.162454: step 8626, loss 0.00488448, acc 1
2016-11-12T19:15:53.220712: step 8627, loss 0.0111965, acc 1
2016-11-12T19:15:53.279625: step 8628, loss 0.00512166, acc 1
2016-11-12T19:15:53.339711: step 8629, loss 0.000734297, acc 1
2016-11-12T19:15:53.397796: step 8630, loss 0.00106249, acc 1
2016-11-12T19:15:53.455857: step 8631, loss 0.0280372, acc 0.984375
2016-11-12T19:15:53.512935: step 8632, loss 0.000214677, acc 1
2016-11-12T19:15:53.571570: step 8633, loss 0.00282411, acc 1
2016-11-12T19:15:53.632955: step 8634, loss 0.0124006, acc 0.984375
2016-11-12T19:15:53.690849: step 8635, loss 0.00212688, acc 1
2016-11-12T19:15:53.748714: step 8636, loss 0.000573287, acc 1
2016-11-12T19:15:53.806602: step 8637, loss 0.0773206, acc 0.96875
2016-11-12T19:15:53.866124: step 8638, loss 0.00108913, acc 1
2016-11-12T19:15:53.922514: step 8639, loss 0.00150375, acc 1
2016-11-12T19:15:53.982085: step 8640, loss 0.171797, acc 0.984375
2016-11-12T19:15:54.039509: step 8641, loss 0.0167504, acc 0.984375
2016-11-12T19:15:54.097416: step 8642, loss 0.00387712, acc 1
2016-11-12T19:15:54.156768: step 8643, loss 0.00282177, acc 1
2016-11-12T19:15:54.214094: step 8644, loss 0.000456906, acc 1
2016-11-12T19:15:54.271478: step 8645, loss 0.000673912, acc 1
2016-11-12T19:15:54.328591: step 8646, loss 0.000578913, acc 1
2016-11-12T19:15:54.385276: step 8647, loss 0.000515925, acc 1
2016-11-12T19:15:54.444644: step 8648, loss 0.00193649, acc 1
2016-11-12T19:15:54.504049: step 8649, loss 0.000571534, acc 1
2016-11-12T19:15:54.564023: step 8650, loss 0.000876119, acc 1
2016-11-12T19:15:54.621364: step 8651, loss 0.00221238, acc 1
2016-11-12T19:15:54.679733: step 8652, loss 0.00356083, acc 1
2016-11-12T19:15:54.736758: step 8653, loss 0.00165212, acc 1
2016-11-12T19:15:54.794410: step 8654, loss 0.000253619, acc 1
2016-11-12T19:15:54.850764: step 8655, loss 0.0165815, acc 0.984375
2016-11-12T19:15:54.908866: step 8656, loss 0.0257607, acc 0.984375
2016-11-12T19:15:54.966657: step 8657, loss 0.0100369, acc 1
2016-11-12T19:15:55.024449: step 8658, loss 0.0208496, acc 0.984375
2016-11-12T19:15:55.085379: step 8659, loss 0.000801075, acc 1
2016-11-12T19:15:55.142720: step 8660, loss 0.000857599, acc 1
2016-11-12T19:15:55.201461: step 8661, loss 0.00199042, acc 1
2016-11-12T19:15:55.240506: step 8662, loss 2.13383e-06, acc 1
2016-11-12T19:15:55.298510: step 8663, loss 0.000433567, acc 1
2016-11-12T19:15:55.354782: step 8664, loss 0.00542687, acc 1
2016-11-12T19:15:55.413119: step 8665, loss 5.7713e-05, acc 1
2016-11-12T19:15:55.469304: step 8666, loss 0.000533872, acc 1
2016-11-12T19:15:55.528895: step 8667, loss 0.00287666, acc 1
2016-11-12T19:15:55.585957: step 8668, loss 0.000540451, acc 1
2016-11-12T19:15:55.644661: step 8669, loss 0.0218046, acc 0.984375
2016-11-12T19:15:55.703628: step 8670, loss 0.0114731, acc 0.984375
2016-11-12T19:15:55.761550: step 8671, loss 0.00128585, acc 1
2016-11-12T19:15:55.818169: step 8672, loss 0.00142073, acc 1
2016-11-12T19:15:55.877012: step 8673, loss 0.0133278, acc 0.984375
2016-11-12T19:15:55.934483: step 8674, loss 0.012064, acc 0.984375
2016-11-12T19:15:55.991836: step 8675, loss 0.0116697, acc 1
2016-11-12T19:15:56.049851: step 8676, loss 0.00219345, acc 1
2016-11-12T19:15:56.111049: step 8677, loss 0.000660272, acc 1
2016-11-12T19:15:56.167420: step 8678, loss 0.00170822, acc 1
2016-11-12T19:15:56.227058: step 8679, loss 9.91706e-05, acc 1
2016-11-12T19:15:56.282672: step 8680, loss 0.000569359, acc 1
2016-11-12T19:15:56.341975: step 8681, loss 0.0572503, acc 0.984375
2016-11-12T19:15:56.400097: step 8682, loss 0.0155226, acc 1
2016-11-12T19:15:56.460489: step 8683, loss 0.000965211, acc 1
2016-11-12T19:15:56.520961: step 8684, loss 4.87957e-05, acc 1
2016-11-12T19:15:56.576661: step 8685, loss 0.000462407, acc 1
2016-11-12T19:15:56.634335: step 8686, loss 0.100202, acc 0.96875
2016-11-12T19:15:56.693081: step 8687, loss 0.0137964, acc 0.984375
2016-11-12T19:15:56.751840: step 8688, loss 0.0056282, acc 1
2016-11-12T19:15:56.810455: step 8689, loss 0.000100442, acc 1
2016-11-12T19:15:56.868101: step 8690, loss 0.00369428, acc 1
2016-11-12T19:15:56.925416: step 8691, loss 0.00623736, acc 1
2016-11-12T19:15:56.983457: step 8692, loss 0.0225612, acc 0.984375
2016-11-12T19:15:57.041053: step 8693, loss 0.00388145, acc 1
2016-11-12T19:15:57.101347: step 8694, loss 0.000494266, acc 1
2016-11-12T19:15:57.160867: step 8695, loss 0.00773814, acc 1
2016-11-12T19:15:57.220732: step 8696, loss 0.00112503, acc 1
2016-11-12T19:15:57.279092: step 8697, loss 0.00125474, acc 1
2016-11-12T19:15:57.338305: step 8698, loss 0.0195622, acc 0.984375
2016-11-12T19:15:57.401755: step 8699, loss 0.000895691, acc 1
2016-11-12T19:15:57.461054: step 8700, loss 0.000100288, acc 1

Evaluation:
2016-11-12T19:15:57.531442: step 8700, loss 3.81956, acc 0.568

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-8700

2016-11-12T19:15:58.015962: step 8701, loss 0.00144172, acc 1
2016-11-12T19:15:58.072054: step 8702, loss 0.00238584, acc 1
2016-11-12T19:15:58.132953: step 8703, loss 0.000352566, acc 1
2016-11-12T19:15:58.189575: step 8704, loss 0.00492117, acc 1
2016-11-12T19:15:58.249286: step 8705, loss 0.000814115, acc 1
2016-11-12T19:15:58.306326: step 8706, loss 0.0021156, acc 1
2016-11-12T19:15:58.363462: step 8707, loss 0.0025442, acc 1
2016-11-12T19:15:58.420951: step 8708, loss 0.0397261, acc 0.984375
2016-11-12T19:15:58.481686: step 8709, loss 0.00176524, acc 1
2016-11-12T19:15:58.539184: step 8710, loss 0.012484, acc 1
2016-11-12T19:15:58.596653: step 8711, loss 0.000494034, acc 1
2016-11-12T19:15:58.653210: step 8712, loss 0.000453162, acc 1
2016-11-12T19:15:58.712125: step 8713, loss 0.022117, acc 0.984375
2016-11-12T19:15:58.772056: step 8714, loss 0.00291658, acc 1
2016-11-12T19:15:58.829032: step 8715, loss 0.000120385, acc 1
2016-11-12T19:15:58.888001: step 8716, loss 0.00028443, acc 1
2016-11-12T19:15:58.944257: step 8717, loss 0.00211643, acc 1
2016-11-12T19:15:59.002430: step 8718, loss 0.00611401, acc 1
2016-11-12T19:15:59.061283: step 8719, loss 0.00161686, acc 1
2016-11-12T19:15:59.117988: step 8720, loss 0.0110299, acc 1
2016-11-12T19:15:59.174431: step 8721, loss 0.000127831, acc 1
2016-11-12T19:15:59.233845: step 8722, loss 0.000166662, acc 1
2016-11-12T19:15:59.292888: step 8723, loss 0.0231716, acc 0.96875
2016-11-12T19:15:59.353020: step 8724, loss 0.0088149, acc 1
2016-11-12T19:15:59.410668: step 8725, loss 0.18115, acc 0.953125
2016-11-12T19:15:59.468673: step 8726, loss 0.000976232, acc 1
2016-11-12T19:15:59.525806: step 8727, loss 0.000121948, acc 1
2016-11-12T19:15:59.581709: step 8728, loss 0.0149589, acc 0.984375
2016-11-12T19:15:59.640920: step 8729, loss 0.0612459, acc 0.984375
2016-11-12T19:15:59.702613: step 8730, loss 0.00324678, acc 1
2016-11-12T19:15:59.759875: step 8731, loss 0.00154207, acc 1
2016-11-12T19:15:59.821817: step 8732, loss 0.00058819, acc 1
2016-11-12T19:15:59.862707: step 8733, loss 0.000281375, acc 1
2016-11-12T19:15:59.922189: step 8734, loss 7.86107e-05, acc 1
2016-11-12T19:15:59.978290: step 8735, loss 0.000667266, acc 1
2016-11-12T19:16:00.035656: step 8736, loss 7.47684e-05, acc 1
2016-11-12T19:16:00.092532: step 8737, loss 0.00263485, acc 1
2016-11-12T19:16:00.150963: step 8738, loss 0.0234035, acc 0.984375
2016-11-12T19:16:00.208299: step 8739, loss 0.00110367, acc 1
2016-11-12T19:16:00.266980: step 8740, loss 0.0028368, acc 1
2016-11-12T19:16:00.324610: step 8741, loss 0.00097775, acc 1
2016-11-12T19:16:00.382556: step 8742, loss 0.00204599, acc 1
2016-11-12T19:16:00.439601: step 8743, loss 0.000172512, acc 1
2016-11-12T19:16:00.497533: step 8744, loss 0.0106696, acc 1
2016-11-12T19:16:00.556434: step 8745, loss 0.00354018, acc 1
2016-11-12T19:16:00.614104: step 8746, loss 0.0141793, acc 0.984375
2016-11-12T19:16:00.673190: step 8747, loss 0.0131326, acc 1
2016-11-12T19:16:00.733035: step 8748, loss 0.000832749, acc 1
2016-11-12T19:16:00.791483: step 8749, loss 0.0212004, acc 0.984375
2016-11-12T19:16:00.848801: step 8750, loss 0.00291085, acc 1
2016-11-12T19:16:00.909557: step 8751, loss 0.00511495, acc 1
2016-11-12T19:16:00.966884: step 8752, loss 0.000134711, acc 1
2016-11-12T19:16:01.025666: step 8753, loss 0.0588457, acc 0.984375
2016-11-12T19:16:01.085639: step 8754, loss 0.000353638, acc 1
2016-11-12T19:16:01.144820: step 8755, loss 9.681e-05, acc 1
2016-11-12T19:16:01.201225: step 8756, loss 0.172841, acc 0.984375
2016-11-12T19:16:01.261403: step 8757, loss 0.0131749, acc 0.984375
2016-11-12T19:16:01.321755: step 8758, loss 0.0100277, acc 1
2016-11-12T19:16:01.379190: step 8759, loss 0.000316003, acc 1
2016-11-12T19:16:01.436304: step 8760, loss 0.0158404, acc 0.984375
2016-11-12T19:16:01.494981: step 8761, loss 0.000781289, acc 1
2016-11-12T19:16:01.552817: step 8762, loss 7.82459e-06, acc 1
2016-11-12T19:16:01.609123: step 8763, loss 0.00053404, acc 1
2016-11-12T19:16:01.664940: step 8764, loss 0.0502181, acc 0.96875
2016-11-12T19:16:01.725254: step 8765, loss 0.00150141, acc 1
2016-11-12T19:16:01.783703: step 8766, loss 0.00963197, acc 1
2016-11-12T19:16:01.841414: step 8767, loss 0.000880804, acc 1
2016-11-12T19:16:01.900907: step 8768, loss 0.0155339, acc 0.984375
2016-11-12T19:16:01.960963: step 8769, loss 0.000915016, acc 1
2016-11-12T19:16:02.019556: step 8770, loss 0.000116256, acc 1
2016-11-12T19:16:02.076182: step 8771, loss 0.00122546, acc 1
2016-11-12T19:16:02.134835: step 8772, loss 0.00800811, acc 1
2016-11-12T19:16:02.192994: step 8773, loss 0.00209603, acc 1
2016-11-12T19:16:02.251360: step 8774, loss 0.00413767, acc 1
2016-11-12T19:16:02.308807: step 8775, loss 0.00398286, acc 1
2016-11-12T19:16:02.367303: step 8776, loss 0.00237902, acc 1
2016-11-12T19:16:02.424877: step 8777, loss 0.000451648, acc 1
2016-11-12T19:16:02.485327: step 8778, loss 0.000291065, acc 1
2016-11-12T19:16:02.542238: step 8779, loss 0.0257186, acc 0.984375
2016-11-12T19:16:02.599809: step 8780, loss 0.00126091, acc 1
2016-11-12T19:16:02.657910: step 8781, loss 0.000466671, acc 1
2016-11-12T19:16:02.715149: step 8782, loss 0.00438471, acc 1
2016-11-12T19:16:02.774119: step 8783, loss 0.00602437, acc 1
2016-11-12T19:16:02.831792: step 8784, loss 0.000623332, acc 1
2016-11-12T19:16:02.888163: step 8785, loss 0.000135183, acc 1
2016-11-12T19:16:02.945583: step 8786, loss 0.000126286, acc 1
2016-11-12T19:16:03.004964: step 8787, loss 0.0620458, acc 0.984375
2016-11-12T19:16:03.063824: step 8788, loss 0.00348884, acc 1
2016-11-12T19:16:03.124466: step 8789, loss 0.00021338, acc 1
2016-11-12T19:16:03.182591: step 8790, loss 0.0225772, acc 0.984375
2016-11-12T19:16:03.241160: step 8791, loss 0.0137308, acc 0.984375
2016-11-12T19:16:03.298265: step 8792, loss 0.0171502, acc 0.984375
2016-11-12T19:16:03.357027: step 8793, loss 0.00103758, acc 1
2016-11-12T19:16:03.417302: step 8794, loss 0.013906, acc 1
2016-11-12T19:16:03.475535: step 8795, loss 0.000321234, acc 1
2016-11-12T19:16:03.536186: step 8796, loss 0.000618809, acc 1
2016-11-12T19:16:03.595177: step 8797, loss 0.000111496, acc 1
2016-11-12T19:16:03.653004: step 8798, loss 0.00266205, acc 1
2016-11-12T19:16:03.713202: step 8799, loss 0.00149036, acc 1
2016-11-12T19:16:03.770772: step 8800, loss 0.000173039, acc 1

Evaluation:
2016-11-12T19:16:03.842761: step 8800, loss 3.79229, acc 0.566

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-8800

2016-11-12T19:16:04.328086: step 8801, loss 0.0285183, acc 0.984375
2016-11-12T19:16:04.386435: step 8802, loss 0.0595128, acc 0.96875
2016-11-12T19:16:04.446657: step 8803, loss 0.000152077, acc 1
2016-11-12T19:16:04.484931: step 8804, loss 0.000132882, acc 1
2016-11-12T19:16:04.543702: step 8805, loss 0.00265031, acc 1
2016-11-12T19:16:04.600969: step 8806, loss 0.00100308, acc 1
2016-11-12T19:16:04.660640: step 8807, loss 0.00269501, acc 1
2016-11-12T19:16:04.721915: step 8808, loss 0.000169475, acc 1
2016-11-12T19:16:04.779627: step 8809, loss 0.00411961, acc 1
2016-11-12T19:16:04.840774: step 8810, loss 0.00018175, acc 1
2016-11-12T19:16:04.896680: step 8811, loss 0.000296785, acc 1
2016-11-12T19:16:04.954635: step 8812, loss 0.00360549, acc 1
2016-11-12T19:16:05.012200: step 8813, loss 0.00115065, acc 1
2016-11-12T19:16:05.069355: step 8814, loss 0.038896, acc 0.984375
2016-11-12T19:16:05.130251: step 8815, loss 0.0164313, acc 0.984375
2016-11-12T19:16:05.188707: step 8816, loss 0.00107532, acc 1
2016-11-12T19:16:05.248937: step 8817, loss 0.000212803, acc 1
2016-11-12T19:16:05.308254: step 8818, loss 0.00682208, acc 1
2016-11-12T19:16:05.366533: step 8819, loss 0.00873345, acc 1
2016-11-12T19:16:05.426064: step 8820, loss 0.0491507, acc 0.984375
2016-11-12T19:16:05.488725: step 8821, loss 0.019082, acc 0.984375
2016-11-12T19:16:05.548710: step 8822, loss 0.00132519, acc 1
2016-11-12T19:16:05.610931: step 8823, loss 0.0164932, acc 0.984375
2016-11-12T19:16:05.671482: step 8824, loss 0.00967383, acc 1
2016-11-12T19:16:05.730189: step 8825, loss 0.000518567, acc 1
2016-11-12T19:16:05.788727: step 8826, loss 0.000250245, acc 1
2016-11-12T19:16:05.845098: step 8827, loss 0.000283888, acc 1
2016-11-12T19:16:05.901331: step 8828, loss 0.00397404, acc 1
2016-11-12T19:16:05.961263: step 8829, loss 0.0312264, acc 0.984375
2016-11-12T19:16:06.020979: step 8830, loss 0.00158683, acc 1
2016-11-12T19:16:06.079286: step 8831, loss 0.000883838, acc 1
2016-11-12T19:16:06.136580: step 8832, loss 0.0207608, acc 0.984375
2016-11-12T19:16:06.202366: step 8833, loss 0.000897155, acc 1
2016-11-12T19:16:06.264939: step 8834, loss 0.00121481, acc 1
2016-11-12T19:16:06.322572: step 8835, loss 0.000214606, acc 1
2016-11-12T19:16:06.384349: step 8836, loss 0.15951, acc 0.984375
2016-11-12T19:16:06.443120: step 8837, loss 0.00111146, acc 1
2016-11-12T19:16:06.501201: step 8838, loss 0.00208672, acc 1
2016-11-12T19:16:06.558338: step 8839, loss 0.000393279, acc 1
2016-11-12T19:16:06.615418: step 8840, loss 0.0079829, acc 1
2016-11-12T19:16:06.676625: step 8841, loss 0.000524125, acc 1
2016-11-12T19:16:06.732834: step 8842, loss 1.62207e-05, acc 1
2016-11-12T19:16:06.789462: step 8843, loss 0.0403669, acc 0.984375
2016-11-12T19:16:06.848129: step 8844, loss 0.000259144, acc 1
2016-11-12T19:16:06.905737: step 8845, loss 0.14864, acc 0.984375
2016-11-12T19:16:06.965844: step 8846, loss 0.020275, acc 0.984375
2016-11-12T19:16:07.025621: step 8847, loss 0.0164073, acc 0.984375
2016-11-12T19:16:07.082908: step 8848, loss 0.0412693, acc 0.96875
2016-11-12T19:16:07.142320: step 8849, loss 0.00035141, acc 1
2016-11-12T19:16:07.198686: step 8850, loss 0.0270466, acc 0.984375
2016-11-12T19:16:07.256713: step 8851, loss 0.0225694, acc 0.984375
2016-11-12T19:16:07.315258: step 8852, loss 0.00288453, acc 1
2016-11-12T19:16:07.374459: step 8853, loss 0.00821426, acc 1
2016-11-12T19:16:07.433903: step 8854, loss 0.00396224, acc 1
2016-11-12T19:16:07.493234: step 8855, loss 0.035132, acc 0.96875
2016-11-12T19:16:07.551275: step 8856, loss 0.00161828, acc 1
2016-11-12T19:16:07.609221: step 8857, loss 0.00129228, acc 1
2016-11-12T19:16:07.667656: step 8858, loss 0.00161102, acc 1
2016-11-12T19:16:07.726625: step 8859, loss 0.00686513, acc 1
2016-11-12T19:16:07.784924: step 8860, loss 0.00540857, acc 1
2016-11-12T19:16:07.844821: step 8861, loss 0.000965047, acc 1
2016-11-12T19:16:07.901034: step 8862, loss 0.0332412, acc 0.984375
2016-11-12T19:16:07.958997: step 8863, loss 0.0061908, acc 1
2016-11-12T19:16:08.016841: step 8864, loss 0.0112078, acc 1
2016-11-12T19:16:08.077495: step 8865, loss 0.013941, acc 1
2016-11-12T19:16:08.137831: step 8866, loss 0.0153272, acc 0.984375
2016-11-12T19:16:08.196901: step 8867, loss 0.000594606, acc 1
2016-11-12T19:16:08.252462: step 8868, loss 0.000913943, acc 1
2016-11-12T19:16:08.312294: step 8869, loss 0.0202177, acc 0.984375
2016-11-12T19:16:08.371314: step 8870, loss 0.0197508, acc 0.984375
2016-11-12T19:16:08.428759: step 8871, loss 0.000979513, acc 1
2016-11-12T19:16:08.487743: step 8872, loss 0.0697854, acc 0.984375
2016-11-12T19:16:08.545212: step 8873, loss 0.00247719, acc 1
2016-11-12T19:16:08.602887: step 8874, loss 4.01717e-05, acc 1
2016-11-12T19:16:08.644003: step 8875, loss 0.00281921, acc 1
2016-11-12T19:16:08.703290: step 8876, loss 0.000141369, acc 1
2016-11-12T19:16:08.760849: step 8877, loss 0.000455156, acc 1
2016-11-12T19:16:08.818178: step 8878, loss 0.000250104, acc 1
2016-11-12T19:16:08.875248: step 8879, loss 0.000110705, acc 1
2016-11-12T19:16:08.933211: step 8880, loss 0.000387531, acc 1
2016-11-12T19:16:08.992556: step 8881, loss 0.0150578, acc 0.984375
2016-11-12T19:16:09.051300: step 8882, loss 0.000212303, acc 1
2016-11-12T19:16:09.106940: step 8883, loss 0.0143093, acc 0.984375
2016-11-12T19:16:09.165825: step 8884, loss 0.00523586, acc 1
2016-11-12T19:16:09.224969: step 8885, loss 0.00894324, acc 1
2016-11-12T19:16:09.282453: step 8886, loss 0.000791317, acc 1
2016-11-12T19:16:09.338562: step 8887, loss 0.0275826, acc 0.984375
2016-11-12T19:16:09.397446: step 8888, loss 0.0288405, acc 0.96875
2016-11-12T19:16:09.455223: step 8889, loss 0.00681425, acc 1
2016-11-12T19:16:09.511953: step 8890, loss 0.00227724, acc 1
2016-11-12T19:16:09.568657: step 8891, loss 6.2766e-05, acc 1
2016-11-12T19:16:09.626082: step 8892, loss 0.000133017, acc 1
2016-11-12T19:16:09.684357: step 8893, loss 0.00417569, acc 1
2016-11-12T19:16:09.744598: step 8894, loss 4.48642e-05, acc 1
2016-11-12T19:16:09.801116: step 8895, loss 0.00465351, acc 1
2016-11-12T19:16:09.858845: step 8896, loss 0.0214355, acc 1
2016-11-12T19:16:09.916288: step 8897, loss 0.000327071, acc 1
2016-11-12T19:16:09.973434: step 8898, loss 0.000371224, acc 1
2016-11-12T19:16:10.031434: step 8899, loss 0.000424966, acc 1
2016-11-12T19:16:10.088073: step 8900, loss 0.017125, acc 1

Evaluation:
2016-11-12T19:16:10.160249: step 8900, loss 3.95574, acc 0.566

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-8900

2016-11-12T19:16:10.649247: step 8901, loss 0.000907319, acc 1
2016-11-12T19:16:10.707018: step 8902, loss 0.014467, acc 0.984375
2016-11-12T19:16:10.767071: step 8903, loss 0.00134697, acc 1
2016-11-12T19:16:10.825270: step 8904, loss 0.000239866, acc 1
2016-11-12T19:16:10.881263: step 8905, loss 0.0166107, acc 0.984375
2016-11-12T19:16:10.939577: step 8906, loss 0.002352, acc 1
2016-11-12T19:16:10.996766: step 8907, loss 0.00107438, acc 1
2016-11-12T19:16:11.055554: step 8908, loss 0.00386961, acc 1
2016-11-12T19:16:11.114442: step 8909, loss 0.000635356, acc 1
2016-11-12T19:16:11.172921: step 8910, loss 0.0473439, acc 0.96875
2016-11-12T19:16:11.230909: step 8911, loss 0.00167801, acc 1
2016-11-12T19:16:11.289120: step 8912, loss 0.00924389, acc 1
2016-11-12T19:16:11.352079: step 8913, loss 0.00129157, acc 1
2016-11-12T19:16:11.410976: step 8914, loss 0.00107399, acc 1
2016-11-12T19:16:11.468952: step 8915, loss 0.0077438, acc 1
2016-11-12T19:16:11.529891: step 8916, loss 0.00315247, acc 1
2016-11-12T19:16:11.589104: step 8917, loss 0.000771288, acc 1
2016-11-12T19:16:11.647475: step 8918, loss 0.00021385, acc 1
2016-11-12T19:16:11.704987: step 8919, loss 0.0374725, acc 0.984375
2016-11-12T19:16:11.765030: step 8920, loss 0.0123253, acc 0.984375
2016-11-12T19:16:11.823638: step 8921, loss 0.000758904, acc 1
2016-11-12T19:16:11.881076: step 8922, loss 0.00339235, acc 1
2016-11-12T19:16:11.940610: step 8923, loss 0.00521006, acc 1
2016-11-12T19:16:11.998158: step 8924, loss 0.00395039, acc 1
2016-11-12T19:16:12.057124: step 8925, loss 0.00293835, acc 1
2016-11-12T19:16:12.113451: step 8926, loss 0.0144338, acc 0.984375
2016-11-12T19:16:12.171875: step 8927, loss 0.00115275, acc 1
2016-11-12T19:16:12.228359: step 8928, loss 0.0202874, acc 0.984375
2016-11-12T19:16:12.285852: step 8929, loss 0.0153753, acc 0.984375
2016-11-12T19:16:12.344586: step 8930, loss 0.0019649, acc 1
2016-11-12T19:16:12.402487: step 8931, loss 3.8145e-05, acc 1
2016-11-12T19:16:12.459672: step 8932, loss 0.00132575, acc 1
2016-11-12T19:16:12.517771: step 8933, loss 0.000357951, acc 1
2016-11-12T19:16:12.576772: step 8934, loss 0.0349603, acc 0.984375
2016-11-12T19:16:12.640732: step 8935, loss 0.000664818, acc 1
2016-11-12T19:16:12.698819: step 8936, loss 0.00197728, acc 1
2016-11-12T19:16:12.757693: step 8937, loss 0.032592, acc 0.984375
2016-11-12T19:16:12.816059: step 8938, loss 0.029565, acc 0.984375
2016-11-12T19:16:12.873651: step 8939, loss 0.0103665, acc 1
2016-11-12T19:16:12.932642: step 8940, loss 0.00364159, acc 1
2016-11-12T19:16:12.992468: step 8941, loss 0.0130076, acc 0.984375
2016-11-12T19:16:13.050345: step 8942, loss 0.000375006, acc 1
2016-11-12T19:16:13.109105: step 8943, loss 0.016879, acc 1
2016-11-12T19:16:13.166773: step 8944, loss 1.77664e-05, acc 1
2016-11-12T19:16:13.223495: step 8945, loss 0.0440031, acc 0.984375
2016-11-12T19:16:13.261283: step 8946, loss 0.0112373, acc 1
2016-11-12T19:16:13.321924: step 8947, loss 1.3885e-05, acc 1
2016-11-12T19:16:13.380600: step 8948, loss 0.000897301, acc 1
2016-11-12T19:16:13.437686: step 8949, loss 0.0012784, acc 1
2016-11-12T19:16:13.497156: step 8950, loss 0.000823016, acc 1
2016-11-12T19:16:13.555507: step 8951, loss 0.0153041, acc 1
2016-11-12T19:16:13.614776: step 8952, loss 0.00017702, acc 1
2016-11-12T19:16:13.673043: step 8953, loss 0.019373, acc 0.984375
2016-11-12T19:16:13.735044: step 8954, loss 0.0150269, acc 0.984375
2016-11-12T19:16:13.792900: step 8955, loss 0.000335281, acc 1
2016-11-12T19:16:13.849850: step 8956, loss 0.0899114, acc 0.96875
2016-11-12T19:16:13.909675: step 8957, loss 0.0429488, acc 0.96875
2016-11-12T19:16:13.971963: step 8958, loss 0.0211586, acc 0.984375
2016-11-12T19:16:14.033668: step 8959, loss 0.025192, acc 0.984375
2016-11-12T19:16:14.093559: step 8960, loss 0.00108853, acc 1
2016-11-12T19:16:14.149921: step 8961, loss 0.00172442, acc 1
2016-11-12T19:16:14.210898: step 8962, loss 0.018597, acc 0.984375
2016-11-12T19:16:14.268729: step 8963, loss 0.00667045, acc 1
2016-11-12T19:16:14.329165: step 8964, loss 0.000169102, acc 1
2016-11-12T19:16:14.385096: step 8965, loss 0.000780565, acc 1
2016-11-12T19:16:14.445673: step 8966, loss 8.75123e-05, acc 1
2016-11-12T19:16:14.505765: step 8967, loss 0.000602899, acc 1
2016-11-12T19:16:14.563855: step 8968, loss 8.70809e-05, acc 1
2016-11-12T19:16:14.620832: step 8969, loss 0.00237088, acc 1
2016-11-12T19:16:14.681174: step 8970, loss 0.000384743, acc 1
2016-11-12T19:16:14.737008: step 8971, loss 0.0340036, acc 0.984375
2016-11-12T19:16:14.795657: step 8972, loss 0.000266436, acc 1
2016-11-12T19:16:14.852809: step 8973, loss 0.00549823, acc 1
2016-11-12T19:16:14.911723: step 8974, loss 0.000461339, acc 1
2016-11-12T19:16:14.968971: step 8975, loss 0.00153264, acc 1
2016-11-12T19:16:15.029668: step 8976, loss 0.019831, acc 1
2016-11-12T19:16:15.088842: step 8977, loss 0.00698808, acc 1
2016-11-12T19:16:15.146280: step 8978, loss 0.000590082, acc 1
2016-11-12T19:16:15.207982: step 8979, loss 0.0108437, acc 1
2016-11-12T19:16:15.268631: step 8980, loss 0.0266346, acc 0.984375
2016-11-12T19:16:15.327698: step 8981, loss 1.53023e-05, acc 1
2016-11-12T19:16:15.387175: step 8982, loss 0.000220566, acc 1
2016-11-12T19:16:15.445598: step 8983, loss 0.00011894, acc 1
2016-11-12T19:16:15.502774: step 8984, loss 0.00103948, acc 1
2016-11-12T19:16:15.560211: step 8985, loss 0.000580934, acc 1
2016-11-12T19:16:15.617305: step 8986, loss 0.00182068, acc 1
2016-11-12T19:16:15.678802: step 8987, loss 0.0225077, acc 0.984375
2016-11-12T19:16:15.736628: step 8988, loss 0.000576227, acc 1
2016-11-12T19:16:15.794840: step 8989, loss 0.0319576, acc 0.984375
2016-11-12T19:16:15.851944: step 8990, loss 0.012489, acc 0.984375
2016-11-12T19:16:15.910843: step 8991, loss 0.0042266, acc 1
2016-11-12T19:16:15.968171: step 8992, loss 0.0119442, acc 0.984375
2016-11-12T19:16:16.029654: step 8993, loss 0.00014795, acc 1
2016-11-12T19:16:16.086414: step 8994, loss 0.0195047, acc 0.984375
2016-11-12T19:16:16.145568: step 8995, loss 0.00907409, acc 1
2016-11-12T19:16:16.205458: step 8996, loss 0.000814654, acc 1
2016-11-12T19:16:16.262115: step 8997, loss 0.000908146, acc 1
2016-11-12T19:16:16.321100: step 8998, loss 0.00283772, acc 1
2016-11-12T19:16:16.380336: step 8999, loss 0.0346644, acc 0.984375
2016-11-12T19:16:16.438750: step 9000, loss 0.000521736, acc 1

Evaluation:
2016-11-12T19:16:16.511219: step 9000, loss 4.00727, acc 0.56

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-9000

2016-11-12T19:16:16.994191: step 9001, loss 0.000238928, acc 1
2016-11-12T19:16:17.051841: step 9002, loss 0.0175977, acc 0.984375
2016-11-12T19:16:17.113992: step 9003, loss 0.0064478, acc 1
2016-11-12T19:16:17.172909: step 9004, loss 0.00459488, acc 1
2016-11-12T19:16:17.234008: step 9005, loss 0.00100959, acc 1
2016-11-12T19:16:17.294724: step 9006, loss 0.0289247, acc 0.96875
2016-11-12T19:16:17.352861: step 9007, loss 0.000138312, acc 1
2016-11-12T19:16:17.409072: step 9008, loss 0.0237426, acc 0.984375
2016-11-12T19:16:17.467100: step 9009, loss 0.000884789, acc 1
2016-11-12T19:16:17.524522: step 9010, loss 0.105152, acc 0.984375
2016-11-12T19:16:17.587800: step 9011, loss 0.000254304, acc 1
2016-11-12T19:16:17.644007: step 9012, loss 0.0038176, acc 1
2016-11-12T19:16:17.703956: step 9013, loss 0.000231862, acc 1
2016-11-12T19:16:17.760486: step 9014, loss 0.00290215, acc 1
2016-11-12T19:16:17.819619: step 9015, loss 0.000798515, acc 1
2016-11-12T19:16:17.880335: step 9016, loss 0.00250857, acc 1
2016-11-12T19:16:17.920379: step 9017, loss 0.0597028, acc 0.95
2016-11-12T19:16:17.981891: step 9018, loss 0.000164455, acc 1
2016-11-12T19:16:18.039288: step 9019, loss 0.00219315, acc 1
2016-11-12T19:16:18.096891: step 9020, loss 0.00066752, acc 1
2016-11-12T19:16:18.154506: step 9021, loss 0.000135282, acc 1
2016-11-12T19:16:18.211873: step 9022, loss 0.0219468, acc 0.984375
2016-11-12T19:16:18.271628: step 9023, loss 0.0288562, acc 0.984375
2016-11-12T19:16:18.331811: step 9024, loss 0.0238854, acc 0.984375
2016-11-12T19:16:18.393127: step 9025, loss 0.00337136, acc 1
2016-11-12T19:16:18.452269: step 9026, loss 0.000918394, acc 1
2016-11-12T19:16:18.509224: step 9027, loss 0.000507369, acc 1
2016-11-12T19:16:18.569089: step 9028, loss 8.85653e-05, acc 1
2016-11-12T19:16:18.625335: step 9029, loss 0.000385652, acc 1
2016-11-12T19:16:18.682034: step 9030, loss 0.000418363, acc 1
2016-11-12T19:16:18.740434: step 9031, loss 0.00151826, acc 1
2016-11-12T19:16:18.798400: step 9032, loss 0.0160124, acc 1
2016-11-12T19:16:18.856924: step 9033, loss 0.00362559, acc 1
2016-11-12T19:16:18.914714: step 9034, loss 0.00255784, acc 1
2016-11-12T19:16:18.971571: step 9035, loss 0.00794587, acc 1
2016-11-12T19:16:19.030569: step 9036, loss 0.00370698, acc 1
2016-11-12T19:16:19.088503: step 9037, loss 0.0132348, acc 1
2016-11-12T19:16:19.145849: step 9038, loss 0.000196377, acc 1
2016-11-12T19:16:19.204120: step 9039, loss 5.44315e-05, acc 1
2016-11-12T19:16:19.260989: step 9040, loss 0.00143946, acc 1
2016-11-12T19:16:19.318928: step 9041, loss 0.000241844, acc 1
2016-11-12T19:16:19.376239: step 9042, loss 0.00149637, acc 1
2016-11-12T19:16:19.433753: step 9043, loss 0.0028111, acc 1
2016-11-12T19:16:19.492384: step 9044, loss 0.00969725, acc 1
2016-11-12T19:16:19.554074: step 9045, loss 0.000713558, acc 1
2016-11-12T19:16:19.612854: step 9046, loss 0.00211856, acc 1
2016-11-12T19:16:19.673170: step 9047, loss 0.000489117, acc 1
2016-11-12T19:16:19.730033: step 9048, loss 0.000125453, acc 1
2016-11-12T19:16:19.789081: step 9049, loss 0.000334367, acc 1
2016-11-12T19:16:19.848607: step 9050, loss 0.000326853, acc 1
2016-11-12T19:16:19.910877: step 9051, loss 0.0287197, acc 0.984375
2016-11-12T19:16:19.970618: step 9052, loss 5.85073e-05, acc 1
2016-11-12T19:16:20.028817: step 9053, loss 0.00022502, acc 1
2016-11-12T19:16:20.085683: step 9054, loss 0.00197071, acc 1
2016-11-12T19:16:20.144796: step 9055, loss 0.0357496, acc 0.984375
2016-11-12T19:16:20.203857: step 9056, loss 0.00217369, acc 1
2016-11-12T19:16:20.261110: step 9057, loss 0.0151897, acc 0.984375
2016-11-12T19:16:20.318966: step 9058, loss 0.00294322, acc 1
2016-11-12T19:16:20.376747: step 9059, loss 0.0124919, acc 0.984375
2016-11-12T19:16:20.434304: step 9060, loss 0.000683391, acc 1
2016-11-12T19:16:20.491721: step 9061, loss 0.00064918, acc 1
2016-11-12T19:16:20.548447: step 9062, loss 0.00311528, acc 1
2016-11-12T19:16:20.608959: step 9063, loss 0.0132755, acc 1
2016-11-12T19:16:20.671709: step 9064, loss 6.42474e-05, acc 1
2016-11-12T19:16:20.729327: step 9065, loss 0.00100119, acc 1
2016-11-12T19:16:20.786752: step 9066, loss 0.000244439, acc 1
2016-11-12T19:16:20.844364: step 9067, loss 0.000104129, acc 1
2016-11-12T19:16:20.901212: step 9068, loss 0.000681881, acc 1
2016-11-12T19:16:20.960494: step 9069, loss 0.000730776, acc 1
2016-11-12T19:16:21.017677: step 9070, loss 0.0427095, acc 0.984375
2016-11-12T19:16:21.075782: step 9071, loss 0.02974, acc 0.984375
2016-11-12T19:16:21.134220: step 9072, loss 0.00449575, acc 1
2016-11-12T19:16:21.192330: step 9073, loss 0.000213793, acc 1
2016-11-12T19:16:21.248928: step 9074, loss 0.00191851, acc 1
2016-11-12T19:16:21.312625: step 9075, loss 0.00242282, acc 1
2016-11-12T19:16:21.370926: step 9076, loss 0.0135814, acc 0.984375
2016-11-12T19:16:21.428916: step 9077, loss 0.0368637, acc 0.984375
2016-11-12T19:16:21.489763: step 9078, loss 0.0171507, acc 0.984375
2016-11-12T19:16:21.549684: step 9079, loss 0.0023778, acc 1
2016-11-12T19:16:21.606338: step 9080, loss 0.0129605, acc 0.984375
2016-11-12T19:16:21.668085: step 9081, loss 0.00102236, acc 1
2016-11-12T19:16:21.726245: step 9082, loss 0.0257038, acc 0.96875
2016-11-12T19:16:21.785032: step 9083, loss 0.00127003, acc 1
2016-11-12T19:16:21.843460: step 9084, loss 0.00049779, acc 1
2016-11-12T19:16:21.901243: step 9085, loss 0.0220194, acc 0.984375
2016-11-12T19:16:21.959024: step 9086, loss 0.130293, acc 0.984375
2016-11-12T19:16:22.018992: step 9087, loss 0.0163823, acc 1
2016-11-12T19:16:22.060387: step 9088, loss 0.00773991, acc 1
2016-11-12T19:16:22.123980: step 9089, loss 0.000778056, acc 1
2016-11-12T19:16:22.182190: step 9090, loss 0.000420967, acc 1
2016-11-12T19:16:22.242575: step 9091, loss 0.0236435, acc 0.984375
2016-11-12T19:16:22.302081: step 9092, loss 0.00228544, acc 1
2016-11-12T19:16:22.359788: step 9093, loss 0.00180331, acc 1
2016-11-12T19:16:22.420479: step 9094, loss 0.000634846, acc 1
2016-11-12T19:16:22.476712: step 9095, loss 0.00127866, acc 1
2016-11-12T19:16:22.534771: step 9096, loss 0.000955936, acc 1
2016-11-12T19:16:22.593122: step 9097, loss 0.00166726, acc 1
2016-11-12T19:16:22.653046: step 9098, loss 7.39153e-05, acc 1
2016-11-12T19:16:22.709199: step 9099, loss 0.00566985, acc 1
2016-11-12T19:16:22.767445: step 9100, loss 0.0586934, acc 0.96875

Evaluation:
2016-11-12T19:16:22.840076: step 9100, loss 3.99364, acc 0.572

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-9100

2016-11-12T19:16:23.374201: step 9101, loss 0.00114088, acc 1
2016-11-12T19:16:23.433517: step 9102, loss 0.0180382, acc 0.984375
2016-11-12T19:16:23.491884: step 9103, loss 0.000943693, acc 1
2016-11-12T19:16:23.550067: step 9104, loss 0.000221555, acc 1
2016-11-12T19:16:23.608787: step 9105, loss 0.00107154, acc 1
2016-11-12T19:16:23.666681: step 9106, loss 0.0543536, acc 0.96875
2016-11-12T19:16:23.729132: step 9107, loss 0.023985, acc 0.984375
2016-11-12T19:16:23.786515: step 9108, loss 0.00525277, acc 1
2016-11-12T19:16:23.845389: step 9109, loss 0.0044892, acc 1
2016-11-12T19:16:23.905280: step 9110, loss 0.0358137, acc 0.984375
2016-11-12T19:16:23.964433: step 9111, loss 0.00139123, acc 1
2016-11-12T19:16:24.025162: step 9112, loss 0.00128437, acc 1
2016-11-12T19:16:24.084358: step 9113, loss 0.00606925, acc 1
2016-11-12T19:16:24.145482: step 9114, loss 0.00687964, acc 1
2016-11-12T19:16:24.202904: step 9115, loss 0.0127386, acc 0.984375
2016-11-12T19:16:24.262252: step 9116, loss 0.0153782, acc 0.984375
2016-11-12T19:16:24.322725: step 9117, loss 0.000931641, acc 1
2016-11-12T19:16:24.380557: step 9118, loss 0.000193781, acc 1
2016-11-12T19:16:24.438533: step 9119, loss 0.000117201, acc 1
2016-11-12T19:16:24.494513: step 9120, loss 0.000593463, acc 1
2016-11-12T19:16:24.552691: step 9121, loss 0.000186374, acc 1
2016-11-12T19:16:24.608801: step 9122, loss 0.00240814, acc 1
2016-11-12T19:16:24.667056: step 9123, loss 0.00583293, acc 1
2016-11-12T19:16:24.724808: step 9124, loss 7.92259e-05, acc 1
2016-11-12T19:16:24.782234: step 9125, loss 0.000293786, acc 1
2016-11-12T19:16:24.840523: step 9126, loss 0.000617463, acc 1
2016-11-12T19:16:24.898262: step 9127, loss 0.00904793, acc 1
2016-11-12T19:16:24.956819: step 9128, loss 8.87862e-06, acc 1
2016-11-12T19:16:25.011916: step 9129, loss 0.00358095, acc 1
2016-11-12T19:16:25.070311: step 9130, loss 0.00854615, acc 1
2016-11-12T19:16:25.129032: step 9131, loss 0.000476825, acc 1
2016-11-12T19:16:25.190160: step 9132, loss 0.000348529, acc 1
2016-11-12T19:16:25.247658: step 9133, loss 0.00685035, acc 1
2016-11-12T19:16:25.308716: step 9134, loss 0.00594942, acc 1
2016-11-12T19:16:25.366328: step 9135, loss 0.0125464, acc 0.984375
2016-11-12T19:16:25.424011: step 9136, loss 0.00169788, acc 1
2016-11-12T19:16:25.480094: step 9137, loss 0.0170021, acc 0.984375
2016-11-12T19:16:25.540834: step 9138, loss 3.17015e-06, acc 1
2016-11-12T19:16:25.596385: step 9139, loss 0.00212432, acc 1
2016-11-12T19:16:25.652938: step 9140, loss 0.000110604, acc 1
2016-11-12T19:16:25.709329: step 9141, loss 0.00180563, acc 1
2016-11-12T19:16:25.768938: step 9142, loss 0.000127367, acc 1
2016-11-12T19:16:25.824262: step 9143, loss 0.00386907, acc 1
2016-11-12T19:16:25.883895: step 9144, loss 0.0132183, acc 0.984375
2016-11-12T19:16:25.945857: step 9145, loss 0.00047415, acc 1
2016-11-12T19:16:26.002594: step 9146, loss 0.000102431, acc 1
2016-11-12T19:16:26.058209: step 9147, loss 0.00081654, acc 1
2016-11-12T19:16:26.115008: step 9148, loss 0.00300345, acc 1
2016-11-12T19:16:26.172835: step 9149, loss 0.000211361, acc 1
2016-11-12T19:16:26.230208: step 9150, loss 0.00113634, acc 1
2016-11-12T19:16:26.288497: step 9151, loss 0.00700032, acc 1
2016-11-12T19:16:26.348161: step 9152, loss 0.000983764, acc 1
2016-11-12T19:16:26.411653: step 9153, loss 0.000529277, acc 1
2016-11-12T19:16:26.468925: step 9154, loss 0.000192211, acc 1
2016-11-12T19:16:26.529684: step 9155, loss 0.0604988, acc 0.984375
2016-11-12T19:16:26.588005: step 9156, loss 9.46078e-05, acc 1
2016-11-12T19:16:26.648395: step 9157, loss 0.00140941, acc 1
2016-11-12T19:16:26.707397: step 9158, loss 0.00018589, acc 1
2016-11-12T19:16:26.746570: step 9159, loss 0.000411106, acc 1
2016-11-12T19:16:26.805087: step 9160, loss 0.00925209, acc 1
2016-11-12T19:16:26.862738: step 9161, loss 0.0206352, acc 0.984375
2016-11-12T19:16:26.919753: step 9162, loss 0.00300597, acc 1
2016-11-12T19:16:26.977202: step 9163, loss 0.00390765, acc 1
2016-11-12T19:16:27.033719: step 9164, loss 0.000493289, acc 1
2016-11-12T19:16:27.093335: step 9165, loss 7.68875e-05, acc 1
2016-11-12T19:16:27.148880: step 9166, loss 0.0679752, acc 0.96875
2016-11-12T19:16:27.207847: step 9167, loss 0.000233555, acc 1
2016-11-12T19:16:27.268420: step 9168, loss 0.000426522, acc 1
2016-11-12T19:16:27.324556: step 9169, loss 0.0111866, acc 0.984375
2016-11-12T19:16:27.383435: step 9170, loss 5.42081e-05, acc 1
2016-11-12T19:16:27.443768: step 9171, loss 0.000186918, acc 1
2016-11-12T19:16:27.501207: step 9172, loss 0.000253513, acc 1
2016-11-12T19:16:27.557815: step 9173, loss 0.0011733, acc 1
2016-11-12T19:16:27.619423: step 9174, loss 0.0275949, acc 0.984375
2016-11-12T19:16:27.680981: step 9175, loss 0.00628517, acc 1
2016-11-12T19:16:27.740640: step 9176, loss 0.000716957, acc 1
2016-11-12T19:16:27.798916: step 9177, loss 0.0093387, acc 1
2016-11-12T19:16:27.857028: step 9178, loss 0.000377143, acc 1
2016-11-12T19:16:27.917471: step 9179, loss 0.0111923, acc 1
2016-11-12T19:16:27.974287: step 9180, loss 0.00338637, acc 1
2016-11-12T19:16:28.032098: step 9181, loss 8.31721e-05, acc 1
2016-11-12T19:16:28.088579: step 9182, loss 1.21757e-05, acc 1
2016-11-12T19:16:28.145685: step 9183, loss 0.00648685, acc 1
2016-11-12T19:16:28.205017: step 9184, loss 0.00216239, acc 1
2016-11-12T19:16:28.266547: step 9185, loss 0.000116239, acc 1
2016-11-12T19:16:28.322454: step 9186, loss 0.0156804, acc 0.984375
2016-11-12T19:16:28.381163: step 9187, loss 0.0150139, acc 0.984375
2016-11-12T19:16:28.438977: step 9188, loss 0.000740391, acc 1
2016-11-12T19:16:28.495937: step 9189, loss 0.00116274, acc 1
2016-11-12T19:16:28.554206: step 9190, loss 0.00928584, acc 1
2016-11-12T19:16:28.612171: step 9191, loss 0.0669818, acc 0.984375
2016-11-12T19:16:28.670701: step 9192, loss 0.000473919, acc 1
2016-11-12T19:16:28.727341: step 9193, loss 0.00218147, acc 1
2016-11-12T19:16:28.787331: step 9194, loss 0.000633043, acc 1
2016-11-12T19:16:28.843536: step 9195, loss 0.0382627, acc 0.984375
2016-11-12T19:16:28.901931: step 9196, loss 0.0036578, acc 1
2016-11-12T19:16:28.960235: step 9197, loss 0.000431889, acc 1
2016-11-12T19:16:29.016730: step 9198, loss 0.00202339, acc 1
2016-11-12T19:16:29.074067: step 9199, loss 0.00272811, acc 1
2016-11-12T19:16:29.134763: step 9200, loss 0.000474328, acc 1

Evaluation:
2016-11-12T19:16:29.207520: step 9200, loss 4.08454, acc 0.558

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-9200

2016-11-12T19:16:29.707188: step 9201, loss 0.00798312, acc 1
2016-11-12T19:16:29.766443: step 9202, loss 0.0221769, acc 0.984375
2016-11-12T19:16:29.825129: step 9203, loss 0.000623717, acc 1
2016-11-12T19:16:29.882516: step 9204, loss 2.09461e-05, acc 1
2016-11-12T19:16:29.940670: step 9205, loss 0.00654922, acc 1
2016-11-12T19:16:29.998567: step 9206, loss 0.015157, acc 1
2016-11-12T19:16:30.057405: step 9207, loss 0.0079678, acc 1
2016-11-12T19:16:30.116789: step 9208, loss 0.00054447, acc 1
2016-11-12T19:16:30.174051: step 9209, loss 0.00314939, acc 1
2016-11-12T19:16:30.231976: step 9210, loss 0.000842677, acc 1
2016-11-12T19:16:30.288298: step 9211, loss 0.00376762, acc 1
2016-11-12T19:16:30.346987: step 9212, loss 3.68643e-05, acc 1
2016-11-12T19:16:30.403233: step 9213, loss 3.15661e-05, acc 1
2016-11-12T19:16:30.460341: step 9214, loss 0.0148974, acc 0.984375
2016-11-12T19:16:30.520832: step 9215, loss 0.000174905, acc 1
2016-11-12T19:16:30.577484: step 9216, loss 0.000169208, acc 1
2016-11-12T19:16:30.636193: step 9217, loss 0.0465768, acc 0.984375
2016-11-12T19:16:30.695441: step 9218, loss 0.0191835, acc 0.984375
2016-11-12T19:16:30.752625: step 9219, loss 0.0178293, acc 0.984375
2016-11-12T19:16:30.811862: step 9220, loss 0.000559415, acc 1
2016-11-12T19:16:30.872241: step 9221, loss 0.00120086, acc 1
2016-11-12T19:16:30.928500: step 9222, loss 0.000356227, acc 1
2016-11-12T19:16:30.987483: step 9223, loss 0.0111291, acc 1
2016-11-12T19:16:31.043660: step 9224, loss 0.00369549, acc 1
2016-11-12T19:16:31.103772: step 9225, loss 0.000188455, acc 1
2016-11-12T19:16:31.161604: step 9226, loss 0.00611182, acc 1
2016-11-12T19:16:31.220958: step 9227, loss 0.00354162, acc 1
2016-11-12T19:16:31.277319: step 9228, loss 0.000540712, acc 1
2016-11-12T19:16:31.334649: step 9229, loss 0.0306011, acc 0.984375
2016-11-12T19:16:31.372567: step 9230, loss 2.89101e-05, acc 1
2016-11-12T19:16:31.430001: step 9231, loss 0.046481, acc 0.96875
2016-11-12T19:16:31.489404: step 9232, loss 3.25699e-05, acc 1
2016-11-12T19:16:31.545902: step 9233, loss 0.000411685, acc 1
2016-11-12T19:16:31.602542: step 9234, loss 6.52844e-05, acc 1
2016-11-12T19:16:31.658684: step 9235, loss 0.0345696, acc 0.984375
2016-11-12T19:16:31.717046: step 9236, loss 0.000361415, acc 1
2016-11-12T19:16:31.776957: step 9237, loss 0.000248749, acc 1
2016-11-12T19:16:31.833215: step 9238, loss 0.0173954, acc 0.984375
2016-11-12T19:16:31.894567: step 9239, loss 0.000384662, acc 1
2016-11-12T19:16:31.953040: step 9240, loss 7.77864e-05, acc 1
2016-11-12T19:16:32.009828: step 9241, loss 0.0001929, acc 1
2016-11-12T19:16:32.068057: step 9242, loss 0.000702101, acc 1
2016-11-12T19:16:32.124595: step 9243, loss 0.0014713, acc 1
2016-11-12T19:16:32.182108: step 9244, loss 0.000587437, acc 1
2016-11-12T19:16:32.239948: step 9245, loss 6.2532e-05, acc 1
2016-11-12T19:16:32.295866: step 9246, loss 0.00030489, acc 1
2016-11-12T19:16:32.356420: step 9247, loss 0.0120079, acc 0.984375
2016-11-12T19:16:32.412949: step 9248, loss 0.0087453, acc 1
2016-11-12T19:16:32.472415: step 9249, loss 0.0200126, acc 0.984375
2016-11-12T19:16:32.529416: step 9250, loss 0.00345494, acc 1
2016-11-12T19:16:32.588948: step 9251, loss 0.000324476, acc 1
2016-11-12T19:16:32.647927: step 9252, loss 0.0143074, acc 0.984375
2016-11-12T19:16:32.706118: step 9253, loss 0.000635635, acc 1
2016-11-12T19:16:32.764217: step 9254, loss 8.991e-05, acc 1
2016-11-12T19:16:32.821604: step 9255, loss 0.000349023, acc 1
2016-11-12T19:16:32.880390: step 9256, loss 0.0194336, acc 0.984375
2016-11-12T19:16:32.938978: step 9257, loss 0.0101466, acc 1
2016-11-12T19:16:33.001193: step 9258, loss 0.0142181, acc 0.984375
2016-11-12T19:16:33.061076: step 9259, loss 8.46992e-05, acc 1
2016-11-12T19:16:33.117149: step 9260, loss 8.21801e-05, acc 1
2016-11-12T19:16:33.172864: step 9261, loss 9.03251e-05, acc 1
2016-11-12T19:16:33.231973: step 9262, loss 0.00046838, acc 1
2016-11-12T19:16:33.288507: step 9263, loss 0.0195433, acc 0.984375
2016-11-12T19:16:33.348913: step 9264, loss 0.2479, acc 0.984375
2016-11-12T19:16:33.409735: step 9265, loss 0.000247023, acc 1
2016-11-12T19:16:33.469089: step 9266, loss 0.000253871, acc 1
2016-11-12T19:16:33.529042: step 9267, loss 0.000321616, acc 1
2016-11-12T19:16:33.587730: step 9268, loss 1.76338e-05, acc 1
2016-11-12T19:16:33.643918: step 9269, loss 0.018374, acc 0.984375
2016-11-12T19:16:33.701757: step 9270, loss 0.00197022, acc 1
2016-11-12T19:16:33.760329: step 9271, loss 0.000250455, acc 1
2016-11-12T19:16:33.818530: step 9272, loss 0.0884697, acc 0.984375
2016-11-12T19:16:33.879552: step 9273, loss 1.44644e-05, acc 1
2016-11-12T19:16:33.937098: step 9274, loss 0.000147966, acc 1
2016-11-12T19:16:33.998203: step 9275, loss 0.00297668, acc 1
2016-11-12T19:16:34.060591: step 9276, loss 3.26762e-05, acc 1
2016-11-12T19:16:34.117572: step 9277, loss 0.000580122, acc 1
2016-11-12T19:16:34.176432: step 9278, loss 0.0219659, acc 0.984375
2016-11-12T19:16:34.237361: step 9279, loss 0.0192994, acc 0.984375
2016-11-12T19:16:34.295046: step 9280, loss 0.000147386, acc 1
2016-11-12T19:16:34.350614: step 9281, loss 0.000404731, acc 1
2016-11-12T19:16:34.413716: step 9282, loss 0.000416965, acc 1
2016-11-12T19:16:34.473175: step 9283, loss 0.00802465, acc 1
2016-11-12T19:16:34.530103: step 9284, loss 0.000119851, acc 1
2016-11-12T19:16:34.589416: step 9285, loss 4.64826e-05, acc 1
2016-11-12T19:16:34.649546: step 9286, loss 0.00602574, acc 1
2016-11-12T19:16:34.707819: step 9287, loss 0.0162472, acc 0.984375
2016-11-12T19:16:34.766031: step 9288, loss 0.180317, acc 0.96875
2016-11-12T19:16:34.828800: step 9289, loss 0.00352331, acc 1
2016-11-12T19:16:34.885951: step 9290, loss 6.73766e-05, acc 1
2016-11-12T19:16:34.943940: step 9291, loss 0.000752704, acc 1
2016-11-12T19:16:35.001380: step 9292, loss 0.000490409, acc 1
2016-11-12T19:16:35.059247: step 9293, loss 0.00150327, acc 1
2016-11-12T19:16:35.117858: step 9294, loss 0.000418543, acc 1
2016-11-12T19:16:35.175014: step 9295, loss 0.0168041, acc 0.984375
2016-11-12T19:16:35.233637: step 9296, loss 0.0174105, acc 0.984375
2016-11-12T19:16:35.292372: step 9297, loss 0.00177992, acc 1
2016-11-12T19:16:35.350146: step 9298, loss 0.00158093, acc 1
2016-11-12T19:16:35.409404: step 9299, loss 0.0176194, acc 0.984375
2016-11-12T19:16:35.472102: step 9300, loss 0.000742041, acc 1

Evaluation:
2016-11-12T19:16:35.543825: step 9300, loss 4.03585, acc 0.562

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-9300

2016-11-12T19:16:36.024162: step 9301, loss 0.000693173, acc 1
2016-11-12T19:16:36.082844: step 9302, loss 3.59712e-05, acc 1
2016-11-12T19:16:36.138834: step 9303, loss 3.74021e-05, acc 1
2016-11-12T19:16:36.196022: step 9304, loss 0.00238766, acc 1
2016-11-12T19:16:36.253888: step 9305, loss 0.00873194, acc 1
2016-11-12T19:16:36.312592: step 9306, loss 0.000237057, acc 1
2016-11-12T19:16:36.369299: step 9307, loss 5.50517e-05, acc 1
2016-11-12T19:16:36.425803: step 9308, loss 0.00722192, acc 1
2016-11-12T19:16:36.483571: step 9309, loss 0.00156961, acc 1
2016-11-12T19:16:36.541955: step 9310, loss 0.00016711, acc 1
2016-11-12T19:16:36.600022: step 9311, loss 0.00764186, acc 1
2016-11-12T19:16:36.660945: step 9312, loss 0.0416954, acc 0.984375
2016-11-12T19:16:36.718585: step 9313, loss 0.0263235, acc 0.984375
2016-11-12T19:16:36.776465: step 9314, loss 0.000205448, acc 1
2016-11-12T19:16:36.833370: step 9315, loss 0.00167389, acc 1
2016-11-12T19:16:36.896564: step 9316, loss 0.000285863, acc 1
2016-11-12T19:16:36.956366: step 9317, loss 0.00355969, acc 1
2016-11-12T19:16:37.014555: step 9318, loss 0.000138362, acc 1
2016-11-12T19:16:37.072394: step 9319, loss 0.0632956, acc 0.96875
2016-11-12T19:16:37.131862: step 9320, loss 0.000779635, acc 1
2016-11-12T19:16:37.188766: step 9321, loss 0.000384024, acc 1
2016-11-12T19:16:37.245492: step 9322, loss 0.012023, acc 0.984375
2016-11-12T19:16:37.302765: step 9323, loss 0.0303802, acc 0.984375
2016-11-12T19:16:37.360534: step 9324, loss 0.000192485, acc 1
2016-11-12T19:16:37.418403: step 9325, loss 0.00443127, acc 1
2016-11-12T19:16:37.475588: step 9326, loss 0.0102111, acc 1
2016-11-12T19:16:37.537007: step 9327, loss 0.00254239, acc 1
2016-11-12T19:16:37.594229: step 9328, loss 8.02211e-06, acc 1
2016-11-12T19:16:37.649331: step 9329, loss 0.000136453, acc 1
2016-11-12T19:16:37.705359: step 9330, loss 0.00878757, acc 1
2016-11-12T19:16:37.762119: step 9331, loss 0.000545761, acc 1
2016-11-12T19:16:37.820180: step 9332, loss 0.000105438, acc 1
2016-11-12T19:16:37.880275: step 9333, loss 5.1435e-05, acc 1
2016-11-12T19:16:37.937718: step 9334, loss 0.00509926, acc 1
2016-11-12T19:16:37.996965: step 9335, loss 0.000169487, acc 1
2016-11-12T19:16:38.054665: step 9336, loss 0.218746, acc 0.984375
2016-11-12T19:16:38.113658: step 9337, loss 0.0569909, acc 0.984375
2016-11-12T19:16:38.172597: step 9338, loss 7.09176e-05, acc 1
2016-11-12T19:16:38.229855: step 9339, loss 0.000949693, acc 1
2016-11-12T19:16:38.285972: step 9340, loss 0.000102216, acc 1
2016-11-12T19:16:38.342186: step 9341, loss 0.00031453, acc 1
2016-11-12T19:16:38.398253: step 9342, loss 1.72159e-05, acc 1
2016-11-12T19:16:38.453983: step 9343, loss 0.0216649, acc 0.984375
2016-11-12T19:16:38.513766: step 9344, loss 7.13847e-05, acc 1
2016-11-12T19:16:38.569918: step 9345, loss 0.00101244, acc 1
2016-11-12T19:16:38.630768: step 9346, loss 0.00621993, acc 1
2016-11-12T19:16:38.688923: step 9347, loss 4.38721e-05, acc 1
2016-11-12T19:16:38.745579: step 9348, loss 0.0215618, acc 1
2016-11-12T19:16:38.805120: step 9349, loss 0.000249632, acc 1
2016-11-12T19:16:38.862363: step 9350, loss 0.00249892, acc 1
2016-11-12T19:16:38.922201: step 9351, loss 0.00153497, acc 1
2016-11-12T19:16:38.982935: step 9352, loss 0.000928834, acc 1
2016-11-12T19:16:39.041042: step 9353, loss 0.00977011, acc 1
2016-11-12T19:16:39.098710: step 9354, loss 0.000651361, acc 1
2016-11-12T19:16:39.157842: step 9355, loss 0.000281825, acc 1
2016-11-12T19:16:39.215118: step 9356, loss 3.64021e-05, acc 1
2016-11-12T19:16:39.271006: step 9357, loss 0.00012733, acc 1
2016-11-12T19:16:39.333172: step 9358, loss 0.00261199, acc 1
2016-11-12T19:16:39.390582: step 9359, loss 0.000663362, acc 1
2016-11-12T19:16:39.447785: step 9360, loss 0.0023497, acc 1
2016-11-12T19:16:39.508158: step 9361, loss 0.000236497, acc 1
2016-11-12T19:16:39.565330: step 9362, loss 0.0147082, acc 1
2016-11-12T19:16:39.625037: step 9363, loss 0.00242857, acc 1
2016-11-12T19:16:39.685213: step 9364, loss 2.63759e-05, acc 1
2016-11-12T19:16:39.740982: step 9365, loss 0.00447604, acc 1
2016-11-12T19:16:39.798115: step 9366, loss 0.0438055, acc 0.984375
2016-11-12T19:16:39.856967: step 9367, loss 0.0105489, acc 1
2016-11-12T19:16:39.916723: step 9368, loss 0.0159021, acc 0.984375
2016-11-12T19:16:39.976137: step 9369, loss 0.013501, acc 0.984375
2016-11-12T19:16:40.033599: step 9370, loss 0.0173639, acc 0.984375
2016-11-12T19:16:40.094451: step 9371, loss 0.0271085, acc 0.984375
2016-11-12T19:16:40.134348: step 9372, loss 5.49549e-06, acc 1
2016-11-12T19:16:40.190488: step 9373, loss 0.000901567, acc 1
2016-11-12T19:16:40.249439: step 9374, loss 0.0249594, acc 0.984375
2016-11-12T19:16:40.306172: step 9375, loss 0.031395, acc 0.984375
2016-11-12T19:16:40.365034: step 9376, loss 0.000157099, acc 1
2016-11-12T19:16:40.423095: step 9377, loss 0.00824541, acc 1
2016-11-12T19:16:40.484520: step 9378, loss 0.0123798, acc 0.984375
2016-11-12T19:16:40.542813: step 9379, loss 0.00177922, acc 1
2016-11-12T19:16:40.600938: step 9380, loss 0.000504312, acc 1
2016-11-12T19:16:40.659769: step 9381, loss 0.0007798, acc 1
2016-11-12T19:16:40.715941: step 9382, loss 0.000286543, acc 1
2016-11-12T19:16:40.775292: step 9383, loss 0.0220084, acc 1
2016-11-12T19:16:40.833908: step 9384, loss 0.000497218, acc 1
2016-11-12T19:16:40.890727: step 9385, loss 0.000110087, acc 1
2016-11-12T19:16:40.948480: step 9386, loss 0.0220349, acc 0.984375
2016-11-12T19:16:41.012008: step 9387, loss 0.00710424, acc 1
2016-11-12T19:16:41.068831: step 9388, loss 0.000301166, acc 1
2016-11-12T19:16:41.125082: step 9389, loss 0.0104535, acc 1
2016-11-12T19:16:41.183759: step 9390, loss 6.41285e-05, acc 1
2016-11-12T19:16:41.241580: step 9391, loss 0.000216639, acc 1
2016-11-12T19:16:41.299942: step 9392, loss 0.000119396, acc 1
2016-11-12T19:16:41.359111: step 9393, loss 0.0334994, acc 0.96875
2016-11-12T19:16:41.416248: step 9394, loss 0.0131673, acc 0.984375
2016-11-12T19:16:41.474400: step 9395, loss 4.70919e-05, acc 1
2016-11-12T19:16:41.533097: step 9396, loss 0.0188259, acc 0.984375
2016-11-12T19:16:41.591919: step 9397, loss 0.00774375, acc 1
2016-11-12T19:16:41.649690: step 9398, loss 0.050302, acc 0.984375
2016-11-12T19:16:41.708048: step 9399, loss 0.00597073, acc 1
2016-11-12T19:16:41.768274: step 9400, loss 0.0257584, acc 0.984375

Evaluation:
2016-11-12T19:16:41.840321: step 9400, loss 4.1043, acc 0.572

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-9400

2016-11-12T19:16:42.336992: step 9401, loss 1.42855e-05, acc 1
2016-11-12T19:16:42.395804: step 9402, loss 3.70672e-05, acc 1
2016-11-12T19:16:42.451858: step 9403, loss 0.0313524, acc 0.984375
2016-11-12T19:16:42.513153: step 9404, loss 4.59087e-05, acc 1
2016-11-12T19:16:42.568794: step 9405, loss 0.258505, acc 0.984375
2016-11-12T19:16:42.632825: step 9406, loss 0.000113259, acc 1
2016-11-12T19:16:42.690833: step 9407, loss 0.00011354, acc 1
2016-11-12T19:16:42.748259: step 9408, loss 0.034086, acc 0.96875
2016-11-12T19:16:42.808858: step 9409, loss 0.000201147, acc 1
2016-11-12T19:16:42.867147: step 9410, loss 0.00575686, acc 1
2016-11-12T19:16:42.924923: step 9411, loss 0.0020636, acc 1
2016-11-12T19:16:42.983012: step 9412, loss 0.000242297, acc 1
2016-11-12T19:16:43.040704: step 9413, loss 6.96282e-05, acc 1
2016-11-12T19:16:43.097686: step 9414, loss 0.000137359, acc 1
2016-11-12T19:16:43.153792: step 9415, loss 0.000388366, acc 1
2016-11-12T19:16:43.212816: step 9416, loss 8.44055e-05, acc 1
2016-11-12T19:16:43.269434: step 9417, loss 0.000582464, acc 1
2016-11-12T19:16:43.327023: step 9418, loss 0.0051735, acc 1
2016-11-12T19:16:43.385138: step 9419, loss 0.00329437, acc 1
2016-11-12T19:16:43.443969: step 9420, loss 0.000228161, acc 1
2016-11-12T19:16:43.502916: step 9421, loss 0.0125201, acc 0.984375
2016-11-12T19:16:43.561711: step 9422, loss 0.00209227, acc 1
2016-11-12T19:16:43.619764: step 9423, loss 0.00129661, acc 1
2016-11-12T19:16:43.677545: step 9424, loss 0.000193117, acc 1
2016-11-12T19:16:43.736885: step 9425, loss 0.0152878, acc 0.984375
2016-11-12T19:16:43.797126: step 9426, loss 7.23574e-05, acc 1
2016-11-12T19:16:43.854234: step 9427, loss 0.00160855, acc 1
2016-11-12T19:16:43.912314: step 9428, loss 0.000238282, acc 1
2016-11-12T19:16:43.969608: step 9429, loss 0.00303807, acc 1
2016-11-12T19:16:44.027401: step 9430, loss 0.000916285, acc 1
2016-11-12T19:16:44.084778: step 9431, loss 0.000596722, acc 1
2016-11-12T19:16:44.142011: step 9432, loss 0.00277486, acc 1
2016-11-12T19:16:44.200667: step 9433, loss 0.000271899, acc 1
2016-11-12T19:16:44.260418: step 9434, loss 0.00219496, acc 1
2016-11-12T19:16:44.319244: step 9435, loss 0.00462479, acc 1
2016-11-12T19:16:44.379409: step 9436, loss 0.00150067, acc 1
2016-11-12T19:16:44.437085: step 9437, loss 2.01153e-05, acc 1
2016-11-12T19:16:44.495680: step 9438, loss 0.0231066, acc 0.984375
2016-11-12T19:16:44.554351: step 9439, loss 1.96207e-05, acc 1
2016-11-12T19:16:44.609569: step 9440, loss 0.000624703, acc 1
2016-11-12T19:16:44.669115: step 9441, loss 0.00414332, acc 1
2016-11-12T19:16:44.726552: step 9442, loss 0.00226188, acc 1
2016-11-12T19:16:44.766618: step 9443, loss 0.000472324, acc 1
2016-11-12T19:16:44.824938: step 9444, loss 0.000552358, acc 1
2016-11-12T19:16:44.882023: step 9445, loss 1.51067e-05, acc 1
2016-11-12T19:16:44.938286: step 9446, loss 0.000674266, acc 1
2016-11-12T19:16:44.996867: step 9447, loss 0.000780712, acc 1
2016-11-12T19:16:45.056141: step 9448, loss 0.00217414, acc 1
2016-11-12T19:16:45.113770: step 9449, loss 0.000118917, acc 1
2016-11-12T19:16:45.172241: step 9450, loss 0.0026463, acc 1
2016-11-12T19:16:45.231486: step 9451, loss 0.00143715, acc 1
2016-11-12T19:16:45.289810: step 9452, loss 0.0224574, acc 0.984375
2016-11-12T19:16:45.348918: step 9453, loss 0.000922649, acc 1
2016-11-12T19:16:45.407373: step 9454, loss 0.000234926, acc 1
2016-11-12T19:16:45.464821: step 9455, loss 3.32603e-05, acc 1
2016-11-12T19:16:45.521934: step 9456, loss 0.0090301, acc 1
2016-11-12T19:16:45.579927: step 9457, loss 0.00316726, acc 1
2016-11-12T19:16:45.637281: step 9458, loss 0.00031923, acc 1
2016-11-12T19:16:45.694846: step 9459, loss 0.000786288, acc 1
2016-11-12T19:16:45.752200: step 9460, loss 0.00482738, acc 1
2016-11-12T19:16:45.813169: step 9461, loss 0.0452664, acc 0.984375
2016-11-12T19:16:45.871418: step 9462, loss 0.0011711, acc 1
2016-11-12T19:16:45.928901: step 9463, loss 0.00055595, acc 1
2016-11-12T19:16:45.989229: step 9464, loss 0.0250226, acc 0.984375
2016-11-12T19:16:46.048642: step 9465, loss 2.70708e-05, acc 1
2016-11-12T19:16:46.108002: step 9466, loss 0.000202256, acc 1
2016-11-12T19:16:46.166170: step 9467, loss 0.00133177, acc 1
2016-11-12T19:16:46.225380: step 9468, loss 0.00107317, acc 1
2016-11-12T19:16:46.285135: step 9469, loss 0.0310685, acc 0.984375
2016-11-12T19:16:46.343287: step 9470, loss 0.0115058, acc 0.984375
2016-11-12T19:16:46.400638: step 9471, loss 0.0283238, acc 0.984375
2016-11-12T19:16:46.459153: step 9472, loss 0.00532614, acc 1
2016-11-12T19:16:46.517223: step 9473, loss 0.000225753, acc 1
2016-11-12T19:16:46.573222: step 9474, loss 0.00028796, acc 1
2016-11-12T19:16:46.629480: step 9475, loss 0.0156271, acc 0.984375
2016-11-12T19:16:46.687943: step 9476, loss 0.0056177, acc 1
2016-11-12T19:16:46.746042: step 9477, loss 0.00239756, acc 1
2016-11-12T19:16:46.803105: step 9478, loss 0.0147981, acc 0.984375
2016-11-12T19:16:46.861200: step 9479, loss 4.57351e-05, acc 1
2016-11-12T19:16:46.917245: step 9480, loss 0.000513001, acc 1
2016-11-12T19:16:46.974925: step 9481, loss 0.000132186, acc 1
2016-11-12T19:16:47.033214: step 9482, loss 0.00194663, acc 1
2016-11-12T19:16:47.095275: step 9483, loss 0.000105318, acc 1
2016-11-12T19:16:47.153521: step 9484, loss 0.00174358, acc 1
2016-11-12T19:16:47.210920: step 9485, loss 0.0314732, acc 1
2016-11-12T19:16:47.268655: step 9486, loss 0.0138522, acc 0.984375
2016-11-12T19:16:47.325988: step 9487, loss 0.0342607, acc 0.984375
2016-11-12T19:16:47.385103: step 9488, loss 0.025636, acc 0.984375
2016-11-12T19:16:47.446747: step 9489, loss 0.0325235, acc 0.96875
2016-11-12T19:16:47.507324: step 9490, loss 0.0338311, acc 0.984375
2016-11-12T19:16:47.567149: step 9491, loss 0.00193989, acc 1
2016-11-12T19:16:47.624921: step 9492, loss 0.000790613, acc 1
2016-11-12T19:16:47.684354: step 9493, loss 0.000339138, acc 1
2016-11-12T19:16:47.743284: step 9494, loss 0.000338815, acc 1
2016-11-12T19:16:47.801162: step 9495, loss 0.000100164, acc 1
2016-11-12T19:16:47.858785: step 9496, loss 0.00147611, acc 1
2016-11-12T19:16:47.916815: step 9497, loss 0.000565973, acc 1
2016-11-12T19:16:47.975528: step 9498, loss 0.00618966, acc 1
2016-11-12T19:16:48.033242: step 9499, loss 2.71615e-05, acc 1
2016-11-12T19:16:48.091348: step 9500, loss 8.05107e-05, acc 1

Evaluation:
2016-11-12T19:16:48.161789: step 9500, loss 4.07059, acc 0.572

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-9500

2016-11-12T19:16:48.660240: step 9501, loss 0.000347034, acc 1
2016-11-12T19:16:48.719021: step 9502, loss 0.00166091, acc 1
2016-11-12T19:16:48.775585: step 9503, loss 0.000258119, acc 1
2016-11-12T19:16:48.834788: step 9504, loss 0.00179401, acc 1
2016-11-12T19:16:48.893091: step 9505, loss 0.0119464, acc 0.984375
2016-11-12T19:16:48.950164: step 9506, loss 0.000581473, acc 1
2016-11-12T19:16:49.006927: step 9507, loss 0.0130133, acc 1
2016-11-12T19:16:49.065396: step 9508, loss 0.000651022, acc 1
2016-11-12T19:16:49.124523: step 9509, loss 0.00804818, acc 1
2016-11-12T19:16:49.184156: step 9510, loss 0.0259976, acc 0.96875
2016-11-12T19:16:49.241556: step 9511, loss 0.000180961, acc 1
2016-11-12T19:16:49.298046: step 9512, loss 0.0169464, acc 1
2016-11-12T19:16:49.356977: step 9513, loss 0.0763317, acc 0.984375
2016-11-12T19:16:49.396608: step 9514, loss 7.69451e-06, acc 1
2016-11-12T19:16:49.453755: step 9515, loss 0.000476942, acc 1
2016-11-12T19:16:49.511522: step 9516, loss 0.00758079, acc 1
2016-11-12T19:16:49.568695: step 9517, loss 0.000414358, acc 1
2016-11-12T19:16:49.625762: step 9518, loss 0.011981, acc 0.984375
2016-11-12T19:16:49.685185: step 9519, loss 0.0487568, acc 0.984375
2016-11-12T19:16:49.744820: step 9520, loss 0.0137328, acc 1
2016-11-12T19:16:49.802553: step 9521, loss 0.000229472, acc 1
2016-11-12T19:16:49.860911: step 9522, loss 0.00173261, acc 1
2016-11-12T19:16:49.918027: step 9523, loss 0.0164461, acc 0.984375
2016-11-12T19:16:49.977904: step 9524, loss 0.0105485, acc 1
2016-11-12T19:16:50.037568: step 9525, loss 0.000112463, acc 1
2016-11-12T19:16:50.096563: step 9526, loss 0.001824, acc 1
2016-11-12T19:16:50.156558: step 9527, loss 0.00870611, acc 1
2016-11-12T19:16:50.217673: step 9528, loss 0.000250214, acc 1
2016-11-12T19:16:50.276523: step 9529, loss 0.00898032, acc 1
2016-11-12T19:16:50.334056: step 9530, loss 0.0067583, acc 1
2016-11-12T19:16:50.393087: step 9531, loss 0.0145639, acc 0.984375
2016-11-12T19:16:50.456007: step 9532, loss 0.0652978, acc 0.984375
2016-11-12T19:16:50.515464: step 9533, loss 0.000281343, acc 1
2016-11-12T19:16:50.572619: step 9534, loss 0.00223474, acc 1
2016-11-12T19:16:50.630856: step 9535, loss 0.0494137, acc 0.984375
2016-11-12T19:16:50.690939: step 9536, loss 0.000141953, acc 1
2016-11-12T19:16:50.748155: step 9537, loss 0.000709407, acc 1
2016-11-12T19:16:50.804319: step 9538, loss 0.000428641, acc 1
2016-11-12T19:16:50.863932: step 9539, loss 0.000167329, acc 1
2016-11-12T19:16:50.924656: step 9540, loss 0.0222055, acc 0.984375
2016-11-12T19:16:50.984466: step 9541, loss 0.000414045, acc 1
2016-11-12T19:16:51.045518: step 9542, loss 0.000850059, acc 1
2016-11-12T19:16:51.103077: step 9543, loss 0.0237812, acc 0.984375
2016-11-12T19:16:51.161810: step 9544, loss 0.0103147, acc 1
2016-11-12T19:16:51.218912: step 9545, loss 0.0336761, acc 0.984375
2016-11-12T19:16:51.276758: step 9546, loss 0.00268189, acc 1
2016-11-12T19:16:51.333817: step 9547, loss 0.0162835, acc 0.984375
2016-11-12T19:16:51.392617: step 9548, loss 0.0188452, acc 0.984375
2016-11-12T19:16:51.453676: step 9549, loss 0.352982, acc 0.96875
2016-11-12T19:16:51.513552: step 9550, loss 0.000291995, acc 1
2016-11-12T19:16:51.570917: step 9551, loss 0.0648954, acc 0.96875
2016-11-12T19:16:51.629482: step 9552, loss 0.010687, acc 1
2016-11-12T19:16:51.688023: step 9553, loss 0.000511868, acc 1
2016-11-12T19:16:51.746129: step 9554, loss 0.00143654, acc 1
2016-11-12T19:16:51.803983: step 9555, loss 0.0164892, acc 0.984375
2016-11-12T19:16:51.864191: step 9556, loss 0.00606047, acc 1
2016-11-12T19:16:51.923346: step 9557, loss 0.017846, acc 0.984375
2016-11-12T19:16:51.981146: step 9558, loss 0.0209386, acc 1
2016-11-12T19:16:52.039380: step 9559, loss 0.03272, acc 0.984375
2016-11-12T19:16:52.098421: step 9560, loss 0.0228372, acc 0.984375
2016-11-12T19:16:52.157686: step 9561, loss 0.0108925, acc 1
2016-11-12T19:16:52.214839: step 9562, loss 0.00880675, acc 1
2016-11-12T19:16:52.273652: step 9563, loss 0.000528697, acc 1
2016-11-12T19:16:52.332944: step 9564, loss 0.00108859, acc 1
2016-11-12T19:16:52.391489: step 9565, loss 0.00708313, acc 1
2016-11-12T19:16:52.453050: step 9566, loss 0.000905922, acc 1
2016-11-12T19:16:52.510699: step 9567, loss 0.0110997, acc 0.984375
2016-11-12T19:16:52.569809: step 9568, loss 0.00102036, acc 1
2016-11-12T19:16:52.626619: step 9569, loss 0.000543064, acc 1
2016-11-12T19:16:52.684834: step 9570, loss 0.000574181, acc 1
2016-11-12T19:16:52.742553: step 9571, loss 0.000127738, acc 1
2016-11-12T19:16:52.799297: step 9572, loss 0.00615771, acc 1
2016-11-12T19:16:52.857563: step 9573, loss 0.000999715, acc 1
2016-11-12T19:16:52.917243: step 9574, loss 0.000260252, acc 1
2016-11-12T19:16:52.973599: step 9575, loss 0.00162749, acc 1
2016-11-12T19:16:53.030558: step 9576, loss 0.00899189, acc 1
2016-11-12T19:16:53.092878: step 9577, loss 0.000214029, acc 1
2016-11-12T19:16:53.151869: step 9578, loss 0.00249442, acc 1
2016-11-12T19:16:53.210520: step 9579, loss 0.000314967, acc 1
2016-11-12T19:16:53.269148: step 9580, loss 0.00125106, acc 1
2016-11-12T19:16:53.328129: step 9581, loss 0.026515, acc 0.984375
2016-11-12T19:16:53.387634: step 9582, loss 0.000227551, acc 1
2016-11-12T19:16:53.444705: step 9583, loss 8.94812e-05, acc 1
2016-11-12T19:16:53.502216: step 9584, loss 8.1157e-05, acc 1
2016-11-12T19:16:53.541328: step 9585, loss 6.19279e-06, acc 1
2016-11-12T19:16:53.601135: step 9586, loss 0.00119411, acc 1
2016-11-12T19:16:53.658539: step 9587, loss 0.000609885, acc 1
2016-11-12T19:16:53.717180: step 9588, loss 0.0010439, acc 1
2016-11-12T19:16:53.774164: step 9589, loss 0.0155097, acc 0.984375
2016-11-12T19:16:53.835025: step 9590, loss 0.0110411, acc 1
2016-11-12T19:16:53.892844: step 9591, loss 0.00929244, acc 1
2016-11-12T19:16:53.950507: step 9592, loss 0.000416347, acc 1
2016-11-12T19:16:54.007694: step 9593, loss 0.0077821, acc 1
2016-11-12T19:16:54.064604: step 9594, loss 0.0102292, acc 1
2016-11-12T19:16:54.123731: step 9595, loss 0.00151287, acc 1
2016-11-12T19:16:54.184931: step 9596, loss 0.0276141, acc 0.984375
2016-11-12T19:16:54.244658: step 9597, loss 0.0265607, acc 0.984375
2016-11-12T19:16:54.303022: step 9598, loss 0.00106254, acc 1
2016-11-12T19:16:54.360610: step 9599, loss 0.0127024, acc 0.984375
2016-11-12T19:16:54.418587: step 9600, loss 0.0440552, acc 0.984375

Evaluation:
2016-11-12T19:16:54.491024: step 9600, loss 4.24205, acc 0.554

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-9600

2016-11-12T19:16:54.988881: step 9601, loss 2.64065e-05, acc 1
2016-11-12T19:16:55.048678: step 9602, loss 0.00158853, acc 1
2016-11-12T19:16:55.108271: step 9603, loss 0.000270899, acc 1
2016-11-12T19:16:55.165291: step 9604, loss 0.0099749, acc 1
2016-11-12T19:16:55.226107: step 9605, loss 0.000557264, acc 1
2016-11-12T19:16:55.285159: step 9606, loss 0.00334439, acc 1
2016-11-12T19:16:55.345010: step 9607, loss 0.0031661, acc 1
2016-11-12T19:16:55.405074: step 9608, loss 5.12902e-05, acc 1
2016-11-12T19:16:55.461728: step 9609, loss 0.0158906, acc 0.984375
2016-11-12T19:16:55.520885: step 9610, loss 0.00213295, acc 1
2016-11-12T19:16:55.580594: step 9611, loss 0.00062044, acc 1
2016-11-12T19:16:55.637733: step 9612, loss 0.0227992, acc 0.984375
2016-11-12T19:16:55.696195: step 9613, loss 0.0125682, acc 0.984375
2016-11-12T19:16:55.757363: step 9614, loss 0.000321226, acc 1
2016-11-12T19:16:55.814386: step 9615, loss 0.0530026, acc 0.96875
2016-11-12T19:16:55.872246: step 9616, loss 0.000891558, acc 1
2016-11-12T19:16:55.930837: step 9617, loss 0.000272988, acc 1
2016-11-12T19:16:55.989069: step 9618, loss 0.000529783, acc 1
2016-11-12T19:16:56.049347: step 9619, loss 0.000168086, acc 1
2016-11-12T19:16:56.105971: step 9620, loss 0.000421859, acc 1
2016-11-12T19:16:56.164564: step 9621, loss 0.00039117, acc 1
2016-11-12T19:16:56.224829: step 9622, loss 0.0144914, acc 0.984375
2016-11-12T19:16:56.283870: step 9623, loss 0.00185513, acc 1
2016-11-12T19:16:56.341081: step 9624, loss 0.00133901, acc 1
2016-11-12T19:16:56.399909: step 9625, loss 0.000143039, acc 1
2016-11-12T19:16:56.456803: step 9626, loss 0.019645, acc 0.984375
2016-11-12T19:16:56.514958: step 9627, loss 0.0233036, acc 0.984375
2016-11-12T19:16:56.572413: step 9628, loss 3.69755e-05, acc 1
2016-11-12T19:16:56.628911: step 9629, loss 0.085032, acc 0.984375
2016-11-12T19:16:56.689297: step 9630, loss 8.15817e-05, acc 1
2016-11-12T19:16:56.745115: step 9631, loss 0.00224468, acc 1
2016-11-12T19:16:56.803847: step 9632, loss 0.0016854, acc 1
2016-11-12T19:16:56.860911: step 9633, loss 0.00337545, acc 1
2016-11-12T19:16:56.920835: step 9634, loss 9.78563e-05, acc 1
2016-11-12T19:16:56.977378: step 9635, loss 2.2311e-05, acc 1
2016-11-12T19:16:57.034425: step 9636, loss 0.000323008, acc 1
2016-11-12T19:16:57.093228: step 9637, loss 0.00142226, acc 1
2016-11-12T19:16:57.151953: step 9638, loss 8.12618e-06, acc 1
2016-11-12T19:16:57.210205: step 9639, loss 0.0045064, acc 1
2016-11-12T19:16:57.269206: step 9640, loss 0.000766515, acc 1
2016-11-12T19:16:57.327126: step 9641, loss 0.000686897, acc 1
2016-11-12T19:16:57.384228: step 9642, loss 0.0492259, acc 0.984375
2016-11-12T19:16:57.443149: step 9643, loss 0.00288946, acc 1
2016-11-12T19:16:57.503532: step 9644, loss 0.00177195, acc 1
2016-11-12T19:16:57.560587: step 9645, loss 0.0004366, acc 1
2016-11-12T19:16:57.618640: step 9646, loss 0.000140079, acc 1
2016-11-12T19:16:57.675580: step 9647, loss 0.0172985, acc 0.984375
2016-11-12T19:16:57.736029: step 9648, loss 0.0147039, acc 0.984375
2016-11-12T19:16:57.794631: step 9649, loss 0.00320407, acc 1
2016-11-12T19:16:57.855094: step 9650, loss 0.0881935, acc 0.984375
2016-11-12T19:16:57.916663: step 9651, loss 0.000523523, acc 1
2016-11-12T19:16:57.977098: step 9652, loss 0.000513727, acc 1
2016-11-12T19:16:58.036929: step 9653, loss 0.000191564, acc 1
2016-11-12T19:16:58.093675: step 9654, loss 0.149168, acc 0.984375
2016-11-12T19:16:58.154296: step 9655, loss 0.0745718, acc 0.96875
2016-11-12T19:16:58.194117: step 9656, loss 0.000217947, acc 1
2016-11-12T19:16:58.252700: step 9657, loss 0.000213927, acc 1
2016-11-12T19:16:58.312922: step 9658, loss 0.000569151, acc 1
2016-11-12T19:16:58.372379: step 9659, loss 0.000324085, acc 1
2016-11-12T19:16:58.433409: step 9660, loss 0.00162025, acc 1
2016-11-12T19:16:58.493174: step 9661, loss 0.000981062, acc 1
2016-11-12T19:16:58.551472: step 9662, loss 0.00638567, acc 1
2016-11-12T19:16:58.609038: step 9663, loss 0.00677892, acc 1
2016-11-12T19:16:58.669098: step 9664, loss 0.0202277, acc 0.984375
2016-11-12T19:16:58.726614: step 9665, loss 0.00387732, acc 1
2016-11-12T19:16:58.786232: step 9666, loss 0.0161099, acc 0.984375
2016-11-12T19:16:58.846130: step 9667, loss 0.00027355, acc 1
2016-11-12T19:16:58.904878: step 9668, loss 0.0690416, acc 0.953125
2016-11-12T19:16:58.965236: step 9669, loss 2.04518e-05, acc 1
2016-11-12T19:16:59.025407: step 9670, loss 0.000183218, acc 1
2016-11-12T19:16:59.084337: step 9671, loss 0.00115417, acc 1
2016-11-12T19:16:59.141340: step 9672, loss 0.00667618, acc 1
2016-11-12T19:16:59.199513: step 9673, loss 0.0103108, acc 1
2016-11-12T19:16:59.259836: step 9674, loss 0.00750864, acc 1
2016-11-12T19:16:59.317073: step 9675, loss 0.00840376, acc 1
2016-11-12T19:16:59.376373: step 9676, loss 0.0168511, acc 0.984375
2016-11-12T19:16:59.437049: step 9677, loss 0.000696157, acc 1
2016-11-12T19:16:59.496772: step 9678, loss 0.00057516, acc 1
2016-11-12T19:16:59.556751: step 9679, loss 0.0165955, acc 0.984375
2016-11-12T19:16:59.615966: step 9680, loss 0.00677757, acc 1
2016-11-12T19:16:59.676634: step 9681, loss 6.71701e-05, acc 1
2016-11-12T19:16:59.733458: step 9682, loss 0.00981074, acc 1
2016-11-12T19:16:59.792995: step 9683, loss 0.00354199, acc 1
2016-11-12T19:16:59.853346: step 9684, loss 0.00363784, acc 1
2016-11-12T19:16:59.911678: step 9685, loss 0.0133551, acc 0.984375
2016-11-12T19:16:59.971130: step 9686, loss 0.000687496, acc 1
2016-11-12T19:17:00.028715: step 9687, loss 0.0101453, acc 1
2016-11-12T19:17:00.087765: step 9688, loss 0.00439264, acc 1
2016-11-12T19:17:00.144419: step 9689, loss 2.23276e-05, acc 1
2016-11-12T19:17:00.201416: step 9690, loss 0.00275914, acc 1
2016-11-12T19:17:00.260005: step 9691, loss 0.00662739, acc 1
2016-11-12T19:17:00.320767: step 9692, loss 0.00066875, acc 1
2016-11-12T19:17:00.379113: step 9693, loss 0.0207719, acc 0.984375
2016-11-12T19:17:00.441295: step 9694, loss 0.0389925, acc 0.984375
2016-11-12T19:17:00.499001: step 9695, loss 0.00403803, acc 1
2016-11-12T19:17:00.557773: step 9696, loss 0.000821206, acc 1
2016-11-12T19:17:00.616803: step 9697, loss 0.00160693, acc 1
2016-11-12T19:17:00.674650: step 9698, loss 0.000376127, acc 1
2016-11-12T19:17:00.730936: step 9699, loss 0.00279502, acc 1
2016-11-12T19:17:00.789012: step 9700, loss 0.00363248, acc 1

Evaluation:
2016-11-12T19:17:00.860471: step 9700, loss 4.37348, acc 0.562

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-9700

2016-11-12T19:17:01.360514: step 9701, loss 0.00109894, acc 1
2016-11-12T19:17:01.423237: step 9702, loss 0.000483731, acc 1
2016-11-12T19:17:01.481821: step 9703, loss 0.000570538, acc 1
2016-11-12T19:17:01.540308: step 9704, loss 0.000345185, acc 1
2016-11-12T19:17:01.600785: step 9705, loss 0.0663911, acc 0.984375
2016-11-12T19:17:01.661017: step 9706, loss 0.00188293, acc 1
2016-11-12T19:17:01.718065: step 9707, loss 0.0849246, acc 0.984375
2016-11-12T19:17:01.777156: step 9708, loss 5.09044e-06, acc 1
2016-11-12T19:17:01.832253: step 9709, loss 0.000635852, acc 1
2016-11-12T19:17:01.892746: step 9710, loss 0.000268926, acc 1
2016-11-12T19:17:01.952680: step 9711, loss 0.00932646, acc 1
2016-11-12T19:17:02.010328: step 9712, loss 0.000532362, acc 1
2016-11-12T19:17:02.069661: step 9713, loss 0.000205601, acc 1
2016-11-12T19:17:02.127142: step 9714, loss 0.000334288, acc 1
2016-11-12T19:17:02.186641: step 9715, loss 0.0287718, acc 0.984375
2016-11-12T19:17:02.244745: step 9716, loss 0.00108616, acc 1
2016-11-12T19:17:02.306291: step 9717, loss 0.042711, acc 0.984375
2016-11-12T19:17:02.364943: step 9718, loss 0.000910856, acc 1
2016-11-12T19:17:02.424080: step 9719, loss 0.019235, acc 0.984375
2016-11-12T19:17:02.484995: step 9720, loss 0.0210389, acc 0.984375
2016-11-12T19:17:02.544063: step 9721, loss 0.0213181, acc 1
2016-11-12T19:17:02.603170: step 9722, loss 0.000422049, acc 1
2016-11-12T19:17:02.659454: step 9723, loss 0.0764043, acc 0.96875
2016-11-12T19:17:02.720395: step 9724, loss 0.000128979, acc 1
2016-11-12T19:17:02.780449: step 9725, loss 0.0342221, acc 0.984375
2016-11-12T19:17:02.840920: step 9726, loss 0.0116308, acc 0.984375
2016-11-12T19:17:02.880661: step 9727, loss 0.00157767, acc 1
2016-11-12T19:17:02.939762: step 9728, loss 0.000626595, acc 1
2016-11-12T19:17:02.997206: step 9729, loss 0.00100877, acc 1
2016-11-12T19:17:03.054590: step 9730, loss 0.0199367, acc 0.984375
2016-11-12T19:17:03.112954: step 9731, loss 0.011859, acc 0.984375
2016-11-12T19:17:03.171808: step 9732, loss 0.00491312, acc 1
2016-11-12T19:17:03.229475: step 9733, loss 0.0194705, acc 0.984375
2016-11-12T19:17:03.288266: step 9734, loss 0.0284856, acc 0.96875
2016-11-12T19:17:03.346812: step 9735, loss 0.000660437, acc 1
2016-11-12T19:17:03.403136: step 9736, loss 0.00444978, acc 1
2016-11-12T19:17:03.463538: step 9737, loss 0.000190629, acc 1
2016-11-12T19:17:03.520336: step 9738, loss 0.00129701, acc 1
2016-11-12T19:17:03.579744: step 9739, loss 0.00012893, acc 1
2016-11-12T19:17:03.639700: step 9740, loss 0.012162, acc 0.984375
2016-11-12T19:17:03.697096: step 9741, loss 0.00644383, acc 1
2016-11-12T19:17:03.753913: step 9742, loss 0.0156053, acc 0.984375
2016-11-12T19:17:03.812811: step 9743, loss 0.0282605, acc 0.984375
2016-11-12T19:17:03.873132: step 9744, loss 0.00408877, acc 1
2016-11-12T19:17:03.933331: step 9745, loss 0.0287677, acc 0.96875
2016-11-12T19:17:03.990912: step 9746, loss 4.68319e-05, acc 1
2016-11-12T19:17:04.047006: step 9747, loss 0.000165701, acc 1
2016-11-12T19:17:04.104073: step 9748, loss 0.00141298, acc 1
2016-11-12T19:17:04.161270: step 9749, loss 0.000586237, acc 1
2016-11-12T19:17:04.218252: step 9750, loss 0.0151183, acc 0.984375
2016-11-12T19:17:04.277615: step 9751, loss 0.00168617, acc 1
2016-11-12T19:17:04.338220: step 9752, loss 0.0127834, acc 1
2016-11-12T19:17:04.398408: step 9753, loss 0.0142667, acc 0.984375
2016-11-12T19:17:04.461822: step 9754, loss 0.00159311, acc 1
2016-11-12T19:17:04.522946: step 9755, loss 0.00651195, acc 1
2016-11-12T19:17:04.584628: step 9756, loss 0.00471282, acc 1
2016-11-12T19:17:04.642254: step 9757, loss 0.000549017, acc 1
2016-11-12T19:17:04.701044: step 9758, loss 0.000156298, acc 1
2016-11-12T19:17:04.759713: step 9759, loss 0.000956238, acc 1
2016-11-12T19:17:04.817362: step 9760, loss 0.000228938, acc 1
2016-11-12T19:17:04.875980: step 9761, loss 0.000116531, acc 1
2016-11-12T19:17:04.932294: step 9762, loss 0.000671104, acc 1
2016-11-12T19:17:04.992508: step 9763, loss 0.00613156, acc 1
2016-11-12T19:17:05.050829: step 9764, loss 0.000288157, acc 1
2016-11-12T19:17:05.107541: step 9765, loss 0.000362414, acc 1
2016-11-12T19:17:05.167712: step 9766, loss 0.00302213, acc 1
2016-11-12T19:17:05.227866: step 9767, loss 0.0464368, acc 0.96875
2016-11-12T19:17:05.288772: step 9768, loss 0.000278915, acc 1
2016-11-12T19:17:05.346137: step 9769, loss 0.00104893, acc 1
2016-11-12T19:17:05.404438: step 9770, loss 0.000905159, acc 1
2016-11-12T19:17:05.461257: step 9771, loss 0.000340112, acc 1
2016-11-12T19:17:05.517794: step 9772, loss 0.000801549, acc 1
2016-11-12T19:17:05.575946: step 9773, loss 0.00103358, acc 1
2016-11-12T19:17:05.635199: step 9774, loss 0.00304253, acc 1
2016-11-12T19:17:05.694428: step 9775, loss 0.0158124, acc 1
2016-11-12T19:17:05.753177: step 9776, loss 3.48194e-05, acc 1
2016-11-12T19:17:05.812197: step 9777, loss 0.000823018, acc 1
2016-11-12T19:17:05.873056: step 9778, loss 0.299389, acc 0.984375
2016-11-12T19:17:05.931809: step 9779, loss 8.97086e-05, acc 1
2016-11-12T19:17:05.990159: step 9780, loss 0.0262915, acc 0.984375
2016-11-12T19:17:06.048018: step 9781, loss 0.000157468, acc 1
2016-11-12T19:17:06.104746: step 9782, loss 0.00101573, acc 1
2016-11-12T19:17:06.164445: step 9783, loss 0.00170063, acc 1
2016-11-12T19:17:06.222576: step 9784, loss 0.000616001, acc 1
2016-11-12T19:17:06.280309: step 9785, loss 0.000281705, acc 1
2016-11-12T19:17:06.336739: step 9786, loss 0.0223345, acc 0.984375
2016-11-12T19:17:06.396907: step 9787, loss 0.00144652, acc 1
2016-11-12T19:17:06.455498: step 9788, loss 0.0175449, acc 0.984375
2016-11-12T19:17:06.516509: step 9789, loss 0.00182801, acc 1
2016-11-12T19:17:06.575868: step 9790, loss 0.0028422, acc 1
2016-11-12T19:17:06.633282: step 9791, loss 0.000623506, acc 1
2016-11-12T19:17:06.690288: step 9792, loss 0.000159282, acc 1
2016-11-12T19:17:06.746777: step 9793, loss 0.022221, acc 0.984375
2016-11-12T19:17:06.804525: step 9794, loss 0.0384124, acc 0.984375
2016-11-12T19:17:06.864863: step 9795, loss 0.00227085, acc 1
2016-11-12T19:17:06.923168: step 9796, loss 0.000263904, acc 1
2016-11-12T19:17:06.980950: step 9797, loss 0.000142039, acc 1
2016-11-12T19:17:07.020566: step 9798, loss 0.000582856, acc 1
2016-11-12T19:17:07.081307: step 9799, loss 5.18519e-05, acc 1
2016-11-12T19:17:07.138405: step 9800, loss 0.00167416, acc 1

Evaluation:
2016-11-12T19:17:07.210180: step 9800, loss 4.36203, acc 0.552

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-9800

2016-11-12T19:17:07.708551: step 9801, loss 0.000735987, acc 1
2016-11-12T19:17:07.766618: step 9802, loss 0.000571219, acc 1
2016-11-12T19:17:07.828777: step 9803, loss 0.020554, acc 0.984375
2016-11-12T19:17:07.886461: step 9804, loss 0.00285367, acc 1
2016-11-12T19:17:07.946488: step 9805, loss 0.0101779, acc 1
2016-11-12T19:17:08.007652: step 9806, loss 0.000128556, acc 1
2016-11-12T19:17:08.065016: step 9807, loss 0.000512795, acc 1
2016-11-12T19:17:08.122905: step 9808, loss 0.0148977, acc 0.984375
2016-11-12T19:17:08.179930: step 9809, loss 0.00596269, acc 1
2016-11-12T19:17:08.237773: step 9810, loss 0.000166822, acc 1
2016-11-12T19:17:08.297118: step 9811, loss 0.114377, acc 0.984375
2016-11-12T19:17:08.358977: step 9812, loss 0.00050924, acc 1
2016-11-12T19:17:08.415715: step 9813, loss 0.0115498, acc 0.984375
2016-11-12T19:17:08.473863: step 9814, loss 0.0260807, acc 0.984375
2016-11-12T19:17:08.535736: step 9815, loss 0.00416542, acc 1
2016-11-12T19:17:08.592244: step 9816, loss 0.0264305, acc 0.984375
2016-11-12T19:17:08.649532: step 9817, loss 0.000295582, acc 1
2016-11-12T19:17:08.708350: step 9818, loss 0.000929627, acc 1
2016-11-12T19:17:08.766725: step 9819, loss 0.0120242, acc 1
2016-11-12T19:17:08.826836: step 9820, loss 0.000913801, acc 1
2016-11-12T19:17:08.886101: step 9821, loss 0.000493085, acc 1
2016-11-12T19:17:08.943510: step 9822, loss 0.000757793, acc 1
2016-11-12T19:17:09.001038: step 9823, loss 0.00195386, acc 1
2016-11-12T19:17:09.060746: step 9824, loss 0.0358676, acc 0.984375
2016-11-12T19:17:09.120462: step 9825, loss 0.000651883, acc 1
2016-11-12T19:17:09.180467: step 9826, loss 0.000361491, acc 1
2016-11-12T19:17:09.239031: step 9827, loss 0.235977, acc 0.984375
2016-11-12T19:17:09.296826: step 9828, loss 0.0616336, acc 0.96875
2016-11-12T19:17:09.356505: step 9829, loss 0.000393254, acc 1
2016-11-12T19:17:09.414220: step 9830, loss 0.0454555, acc 0.984375
2016-11-12T19:17:09.472853: step 9831, loss 8.47743e-05, acc 1
2016-11-12T19:17:09.533152: step 9832, loss 0.000947737, acc 1
2016-11-12T19:17:09.591209: step 9833, loss 0.000308387, acc 1
2016-11-12T19:17:09.648746: step 9834, loss 0.000204635, acc 1
2016-11-12T19:17:09.707470: step 9835, loss 4.24111e-05, acc 1
2016-11-12T19:17:09.764610: step 9836, loss 0.000526436, acc 1
2016-11-12T19:17:09.825583: step 9837, loss 0.00994442, acc 1
2016-11-12T19:17:09.882899: step 9838, loss 0.00870231, acc 1
2016-11-12T19:17:09.942464: step 9839, loss 0.000259309, acc 1
2016-11-12T19:17:09.999716: step 9840, loss 0.000221916, acc 1
2016-11-12T19:17:10.056208: step 9841, loss 0.0152567, acc 0.984375
2016-11-12T19:17:10.114849: step 9842, loss 0.00111269, acc 1
2016-11-12T19:17:10.173121: step 9843, loss 0.000260009, acc 1
2016-11-12T19:17:10.228956: step 9844, loss 0.000201568, acc 1
2016-11-12T19:17:10.289253: step 9845, loss 0.0146766, acc 0.984375
2016-11-12T19:17:10.346359: step 9846, loss 0.000209459, acc 1
2016-11-12T19:17:10.403064: step 9847, loss 5.27369e-05, acc 1
2016-11-12T19:17:10.461283: step 9848, loss 0.00366515, acc 1
2016-11-12T19:17:10.519529: step 9849, loss 0.0124761, acc 0.984375
2016-11-12T19:17:10.577676: step 9850, loss 0.00551319, acc 1
2016-11-12T19:17:10.635661: step 9851, loss 5.75624e-05, acc 1
2016-11-12T19:17:10.693767: step 9852, loss 0.0232015, acc 0.984375
2016-11-12T19:17:10.755576: step 9853, loss 0.0143503, acc 0.984375
2016-11-12T19:17:10.814324: step 9854, loss 0.0141068, acc 1
2016-11-12T19:17:10.872420: step 9855, loss 0.01114, acc 1
2016-11-12T19:17:10.930135: step 9856, loss 0.00138678, acc 1
2016-11-12T19:17:10.990930: step 9857, loss 0.0340789, acc 0.984375
2016-11-12T19:17:11.048845: step 9858, loss 0.000407373, acc 1
2016-11-12T19:17:11.106282: step 9859, loss 0.0483724, acc 0.96875
2016-11-12T19:17:11.164492: step 9860, loss 0.0223086, acc 0.984375
2016-11-12T19:17:11.223168: step 9861, loss 0.00304919, acc 1
2016-11-12T19:17:11.280014: step 9862, loss 0.0923823, acc 0.984375
2016-11-12T19:17:11.343100: step 9863, loss 0.000278047, acc 1
2016-11-12T19:17:11.404481: step 9864, loss 0.000444327, acc 1
2016-11-12T19:17:11.461871: step 9865, loss 0.000703368, acc 1
2016-11-12T19:17:11.518496: step 9866, loss 0.00443541, acc 1
2016-11-12T19:17:11.576300: step 9867, loss 0.000302331, acc 1
2016-11-12T19:17:11.633409: step 9868, loss 0.0148904, acc 0.984375
2016-11-12T19:17:11.672392: step 9869, loss 0.00185148, acc 1
2016-11-12T19:17:11.731744: step 9870, loss 0.000323197, acc 1
2016-11-12T19:17:11.792989: step 9871, loss 0.000152716, acc 1
2016-11-12T19:17:11.853915: step 9872, loss 0.0142512, acc 0.984375
2016-11-12T19:17:11.913000: step 9873, loss 0.000121157, acc 1
2016-11-12T19:17:11.969516: step 9874, loss 0.000687602, acc 1
2016-11-12T19:17:12.028403: step 9875, loss 0.00962542, acc 1
2016-11-12T19:17:12.086167: step 9876, loss 0.000972352, acc 1
2016-11-12T19:17:12.144636: step 9877, loss 0.0106184, acc 1
2016-11-12T19:17:12.201918: step 9878, loss 0.00558781, acc 1
2016-11-12T19:17:12.260796: step 9879, loss 7.57282e-05, acc 1
2016-11-12T19:17:12.317012: step 9880, loss 0.0184481, acc 0.984375
2016-11-12T19:17:12.373843: step 9881, loss 0.0143158, acc 1
2016-11-12T19:17:12.432997: step 9882, loss 0.0069395, acc 1
2016-11-12T19:17:12.491084: step 9883, loss 4.35868e-05, acc 1
2016-11-12T19:17:12.546649: step 9884, loss 0.0122437, acc 0.984375
2016-11-12T19:17:12.604933: step 9885, loss 0.0109348, acc 0.984375
2016-11-12T19:17:12.661149: step 9886, loss 0.00973098, acc 1
2016-11-12T19:17:12.719763: step 9887, loss 7.28642e-05, acc 1
2016-11-12T19:17:12.776269: step 9888, loss 0.000200629, acc 1
2016-11-12T19:17:12.831763: step 9889, loss 0.000137957, acc 1
2016-11-12T19:17:12.891819: step 9890, loss 0.000114586, acc 1
2016-11-12T19:17:12.948805: step 9891, loss 0.00122876, acc 1
2016-11-12T19:17:13.006957: step 9892, loss 0.0119573, acc 1
2016-11-12T19:17:13.066623: step 9893, loss 0.0405289, acc 0.984375
2016-11-12T19:17:13.125009: step 9894, loss 0.000790383, acc 1
2016-11-12T19:17:13.184593: step 9895, loss 0.000441845, acc 1
2016-11-12T19:17:13.241224: step 9896, loss 0.00388787, acc 1
2016-11-12T19:17:13.301083: step 9897, loss 0.0141639, acc 1
2016-11-12T19:17:13.361600: step 9898, loss 5.11056e-05, acc 1
2016-11-12T19:17:13.420079: step 9899, loss 0.0132763, acc 0.984375
2016-11-12T19:17:13.479785: step 9900, loss 0.00127868, acc 1

Evaluation:
2016-11-12T19:17:13.551679: step 9900, loss 4.37574, acc 0.55

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-9900

2016-11-12T19:17:14.066548: step 9901, loss 0.00112743, acc 1
2016-11-12T19:17:14.128980: step 9902, loss 0.00566704, acc 1
2016-11-12T19:17:14.188831: step 9903, loss 0.0023006, acc 1
2016-11-12T19:17:14.246514: step 9904, loss 0.0139447, acc 0.984375
2016-11-12T19:17:14.304739: step 9905, loss 0.00052027, acc 1
2016-11-12T19:17:14.365179: step 9906, loss 0.0162243, acc 0.984375
2016-11-12T19:17:14.422350: step 9907, loss 0.00939421, acc 1
2016-11-12T19:17:14.480201: step 9908, loss 0.00896984, acc 1
2016-11-12T19:17:14.537925: step 9909, loss 0.00677786, acc 1
2016-11-12T19:17:14.595256: step 9910, loss 5.36135e-05, acc 1
2016-11-12T19:17:14.652419: step 9911, loss 0.00029383, acc 1
2016-11-12T19:17:14.709602: step 9912, loss 0.0039222, acc 1
2016-11-12T19:17:14.767371: step 9913, loss 3.39555e-05, acc 1
2016-11-12T19:17:14.822732: step 9914, loss 0.00110478, acc 1
2016-11-12T19:17:14.880941: step 9915, loss 0.0219989, acc 0.984375
2016-11-12T19:17:14.939434: step 9916, loss 0.00254052, acc 1
2016-11-12T19:17:14.997637: step 9917, loss 4.6518e-05, acc 1
2016-11-12T19:17:15.053372: step 9918, loss 0.00232532, acc 1
2016-11-12T19:17:15.112433: step 9919, loss 0.00575577, acc 1
2016-11-12T19:17:15.169838: step 9920, loss 0.00477728, acc 1
2016-11-12T19:17:15.228364: step 9921, loss 0.00112739, acc 1
2016-11-12T19:17:15.286576: step 9922, loss 0.00227067, acc 1
2016-11-12T19:17:15.344166: step 9923, loss 0.0325523, acc 0.984375
2016-11-12T19:17:15.404607: step 9924, loss 7.29922e-05, acc 1
2016-11-12T19:17:15.462655: step 9925, loss 0.00463285, acc 1
2016-11-12T19:17:15.520613: step 9926, loss 0.0859861, acc 0.984375
2016-11-12T19:17:15.581556: step 9927, loss 0.00692193, acc 1
2016-11-12T19:17:15.639547: step 9928, loss 0.00413693, acc 1
2016-11-12T19:17:15.700788: step 9929, loss 0.000204922, acc 1
2016-11-12T19:17:15.760724: step 9930, loss 0.000415131, acc 1
2016-11-12T19:17:15.817090: step 9931, loss 0.000187157, acc 1
2016-11-12T19:17:15.875891: step 9932, loss 0.0106964, acc 1
2016-11-12T19:17:15.933604: step 9933, loss 0.0190179, acc 0.984375
2016-11-12T19:17:15.991149: step 9934, loss 0.0140062, acc 0.984375
2016-11-12T19:17:16.049307: step 9935, loss 0.0267288, acc 0.984375
2016-11-12T19:17:16.108291: step 9936, loss 0.016715, acc 0.984375
2016-11-12T19:17:16.171984: step 9937, loss 0.0501806, acc 0.984375
2016-11-12T19:17:16.232381: step 9938, loss 0.0241892, acc 0.984375
2016-11-12T19:17:16.291760: step 9939, loss 0.00874277, acc 1
2016-11-12T19:17:16.329999: step 9940, loss 2.78733e-05, acc 1
2016-11-12T19:17:16.388913: step 9941, loss 0.00146914, acc 1
2016-11-12T19:17:16.445149: step 9942, loss 0.0104475, acc 1
2016-11-12T19:17:16.504863: step 9943, loss 0.000263296, acc 1
2016-11-12T19:17:16.561700: step 9944, loss 0.000655097, acc 1
2016-11-12T19:17:16.620113: step 9945, loss 0.000212202, acc 1
2016-11-12T19:17:16.678490: step 9946, loss 0.000536574, acc 1
2016-11-12T19:17:16.738328: step 9947, loss 0.00721084, acc 1
2016-11-12T19:17:16.795703: step 9948, loss 0.000575332, acc 1
2016-11-12T19:17:16.853025: step 9949, loss 0.00258229, acc 1
2016-11-12T19:17:16.910768: step 9950, loss 0.000394173, acc 1
2016-11-12T19:17:16.967742: step 9951, loss 0.0466973, acc 0.984375
2016-11-12T19:17:17.025485: step 9952, loss 0.000567575, acc 1
2016-11-12T19:17:17.085078: step 9953, loss 0.00597028, acc 1
2016-11-12T19:17:17.145525: step 9954, loss 0.000284006, acc 1
2016-11-12T19:17:17.202010: step 9955, loss 5.93996e-05, acc 1
2016-11-12T19:17:17.260169: step 9956, loss 0.0553829, acc 0.984375
2016-11-12T19:17:17.317758: step 9957, loss 7.17717e-05, acc 1
2016-11-12T19:17:17.374248: step 9958, loss 0.000311833, acc 1
2016-11-12T19:17:17.432023: step 9959, loss 0.00641738, acc 1
2016-11-12T19:17:17.492341: step 9960, loss 0.000789115, acc 1
2016-11-12T19:17:17.552762: step 9961, loss 0.00562616, acc 1
2016-11-12T19:17:17.613177: step 9962, loss 0.000385404, acc 1
2016-11-12T19:17:17.669966: step 9963, loss 0.0122737, acc 0.984375
2016-11-12T19:17:17.727626: step 9964, loss 0.00139893, acc 1
2016-11-12T19:17:17.792246: step 9965, loss 0.00319077, acc 1
2016-11-12T19:17:17.848991: step 9966, loss 0.00214305, acc 1
2016-11-12T19:17:17.908685: step 9967, loss 4.21122e-05, acc 1
2016-11-12T19:17:17.964739: step 9968, loss 0.00553494, acc 1
2016-11-12T19:17:18.022991: step 9969, loss 0.00125456, acc 1
2016-11-12T19:17:18.080910: step 9970, loss 0.00193002, acc 1
2016-11-12T19:17:18.140371: step 9971, loss 0.000186567, acc 1
2016-11-12T19:17:18.197553: step 9972, loss 0.00263065, acc 1
2016-11-12T19:17:18.256913: step 9973, loss 0.00112889, acc 1
2016-11-12T19:17:18.314004: step 9974, loss 0.00764021, acc 1
2016-11-12T19:17:18.371820: step 9975, loss 0.000258674, acc 1
2016-11-12T19:17:18.429078: step 9976, loss 0.000114429, acc 1
2016-11-12T19:17:18.486340: step 9977, loss 5.45436e-05, acc 1
2016-11-12T19:17:18.544991: step 9978, loss 4.74434e-05, acc 1
2016-11-12T19:17:18.601761: step 9979, loss 0.00798008, acc 1
2016-11-12T19:17:18.659951: step 9980, loss 0.0233368, acc 0.984375
2016-11-12T19:17:18.717316: step 9981, loss 0.00316937, acc 1
2016-11-12T19:17:18.774436: step 9982, loss 0.000658417, acc 1
2016-11-12T19:17:18.833671: step 9983, loss 0.000104607, acc 1
2016-11-12T19:17:18.892764: step 9984, loss 0.022208, acc 0.984375
2016-11-12T19:17:18.952998: step 9985, loss 0.000437273, acc 1
2016-11-12T19:17:19.010671: step 9986, loss 0.00473338, acc 1
2016-11-12T19:17:19.068095: step 9987, loss 0.0132424, acc 1
2016-11-12T19:17:19.126399: step 9988, loss 0.0450787, acc 0.984375
2016-11-12T19:17:19.185182: step 9989, loss 0.000220428, acc 1
2016-11-12T19:17:19.242283: step 9990, loss 0.00469937, acc 1
2016-11-12T19:17:19.299683: step 9991, loss 0.000716709, acc 1
2016-11-12T19:17:19.357177: step 9992, loss 0.0650158, acc 0.984375
2016-11-12T19:17:19.415700: step 9993, loss 0.00210604, acc 1
2016-11-12T19:17:19.476532: step 9994, loss 0.0173052, acc 0.984375
2016-11-12T19:17:19.534140: step 9995, loss 0.0215294, acc 0.984375
2016-11-12T19:17:19.597602: step 9996, loss 0.00352617, acc 1
2016-11-12T19:17:19.656943: step 9997, loss 0.000893419, acc 1
2016-11-12T19:17:19.714372: step 9998, loss 0.000587407, acc 1
2016-11-12T19:17:19.772666: step 9999, loss 0.00609426, acc 1
2016-11-12T19:17:19.832926: step 10000, loss 5.04726e-05, acc 1

Evaluation:
2016-11-12T19:17:19.904137: step 10000, loss 4.30544, acc 0.54

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-10000

2016-11-12T19:17:20.394064: step 10001, loss 0.000144318, acc 1
2016-11-12T19:17:20.452914: step 10002, loss 0.0297136, acc 0.96875
2016-11-12T19:17:20.511135: step 10003, loss 0.00317469, acc 1
2016-11-12T19:17:20.570369: step 10004, loss 0.000253722, acc 1
2016-11-12T19:17:20.628886: step 10005, loss 0.00267219, acc 1
2016-11-12T19:17:20.685613: step 10006, loss 0.000318659, acc 1
2016-11-12T19:17:20.744375: step 10007, loss 0.0654949, acc 0.96875
2016-11-12T19:17:20.804456: step 10008, loss 0.0194725, acc 0.984375
2016-11-12T19:17:20.861776: step 10009, loss 0.00865387, acc 1
2016-11-12T19:17:20.920700: step 10010, loss 0.00807417, acc 1
2016-11-12T19:17:20.960428: step 10011, loss 7.7365e-06, acc 1
2016-11-12T19:17:21.023423: step 10012, loss 9.42793e-05, acc 1
2016-11-12T19:17:21.081495: step 10013, loss 0.00755004, acc 1
2016-11-12T19:17:21.139762: step 10014, loss 4.95712e-05, acc 1
2016-11-12T19:17:21.196916: step 10015, loss 0.00693058, acc 1
2016-11-12T19:17:21.256614: step 10016, loss 0.0117912, acc 0.984375
2016-11-12T19:17:21.317281: step 10017, loss 0.0110747, acc 1
2016-11-12T19:17:21.381710: step 10018, loss 0.00177888, acc 1
2016-11-12T19:17:21.438341: step 10019, loss 0.000487605, acc 1
2016-11-12T19:17:21.497713: step 10020, loss 0.000616458, acc 1
2016-11-12T19:17:21.552741: step 10021, loss 0.0171407, acc 0.984375
2016-11-12T19:17:21.610490: step 10022, loss 8.54083e-06, acc 1
2016-11-12T19:17:21.669008: step 10023, loss 0.00151259, acc 1
2016-11-12T19:17:21.725706: step 10024, loss 4.9723e-05, acc 1
2016-11-12T19:17:21.785205: step 10025, loss 0.007468, acc 1
2016-11-12T19:17:21.844532: step 10026, loss 0.00602754, acc 1
2016-11-12T19:17:21.901950: step 10027, loss 0.000998194, acc 1
2016-11-12T19:17:21.961217: step 10028, loss 0.0057071, acc 1
2016-11-12T19:17:22.021047: step 10029, loss 0.00232135, acc 1
2016-11-12T19:17:22.083869: step 10030, loss 0.000456904, acc 1
2016-11-12T19:17:22.142520: step 10031, loss 0.000279142, acc 1
2016-11-12T19:17:22.199499: step 10032, loss 0.00014669, acc 1
2016-11-12T19:17:22.258442: step 10033, loss 0.000406413, acc 1
2016-11-12T19:17:22.317490: step 10034, loss 0.0126125, acc 0.984375
2016-11-12T19:17:22.377291: step 10035, loss 0.000594923, acc 1
2016-11-12T19:17:22.436963: step 10036, loss 0.0399471, acc 0.984375
2016-11-12T19:17:22.495581: step 10037, loss 0.00190727, acc 1
2016-11-12T19:17:22.553873: step 10038, loss 0.00445522, acc 1
2016-11-12T19:17:22.612503: step 10039, loss 0.00264924, acc 1
2016-11-12T19:17:22.671000: step 10040, loss 0.00105607, acc 1
2016-11-12T19:17:22.731321: step 10041, loss 0.0191112, acc 0.984375
2016-11-12T19:17:22.791099: step 10042, loss 0.000390757, acc 1
2016-11-12T19:17:22.847891: step 10043, loss 0.000987484, acc 1
2016-11-12T19:17:22.905105: step 10044, loss 0.00856242, acc 1
2016-11-12T19:17:22.964920: step 10045, loss 0.0106395, acc 1
2016-11-12T19:17:23.023038: step 10046, loss 0.000579166, acc 1
2016-11-12T19:17:23.080335: step 10047, loss 0.00337657, acc 1
2016-11-12T19:17:23.138054: step 10048, loss 0.00108372, acc 1
2016-11-12T19:17:23.195693: step 10049, loss 0.000437884, acc 1
2016-11-12T19:17:23.255233: step 10050, loss 8.44192e-05, acc 1
2016-11-12T19:17:23.313648: step 10051, loss 0.0165644, acc 0.984375
2016-11-12T19:17:23.371300: step 10052, loss 0.00187051, acc 1
2016-11-12T19:17:23.428876: step 10053, loss 4.30741e-05, acc 1
2016-11-12T19:17:23.485526: step 10054, loss 0.336319, acc 0.984375
2016-11-12T19:17:23.545318: step 10055, loss 0.00253705, acc 1
2016-11-12T19:17:23.604149: step 10056, loss 0.00771748, acc 1
2016-11-12T19:17:23.661278: step 10057, loss 0.0167583, acc 1
2016-11-12T19:17:23.721693: step 10058, loss 5.3613e-05, acc 1
2016-11-12T19:17:23.778798: step 10059, loss 0.00147259, acc 1
2016-11-12T19:17:23.836634: step 10060, loss 0.00429679, acc 1
2016-11-12T19:17:23.894440: step 10061, loss 0.000282567, acc 1
2016-11-12T19:17:23.951479: step 10062, loss 0.000519181, acc 1
2016-11-12T19:17:24.008949: step 10063, loss 0.0233069, acc 0.984375
2016-11-12T19:17:24.068273: step 10064, loss 0.160441, acc 0.984375
2016-11-12T19:17:24.129184: step 10065, loss 0.0010051, acc 1
2016-11-12T19:17:24.186667: step 10066, loss 0.0103918, acc 1
2016-11-12T19:17:24.242800: step 10067, loss 0.0089144, acc 1
2016-11-12T19:17:24.301947: step 10068, loss 2.18354e-05, acc 1
2016-11-12T19:17:24.357252: step 10069, loss 0.00985519, acc 1
2016-11-12T19:17:24.415288: step 10070, loss 0.0147276, acc 0.984375
2016-11-12T19:17:24.474096: step 10071, loss 0.000179598, acc 1
2016-11-12T19:17:24.530515: step 10072, loss 0.00291098, acc 1
2016-11-12T19:17:24.590955: step 10073, loss 5.91258e-05, acc 1
2016-11-12T19:17:24.648207: step 10074, loss 0.000562464, acc 1
2016-11-12T19:17:24.704964: step 10075, loss 0.000347724, acc 1
2016-11-12T19:17:24.762768: step 10076, loss 3.62015e-05, acc 1
2016-11-12T19:17:24.821180: step 10077, loss 0.0391166, acc 0.984375
2016-11-12T19:17:24.880608: step 10078, loss 0.00677905, acc 1
2016-11-12T19:17:24.938678: step 10079, loss 0.000114162, acc 1
2016-11-12T19:17:24.998400: step 10080, loss 0.0138935, acc 1
2016-11-12T19:17:25.057495: step 10081, loss 0.00296932, acc 1
2016-11-12T19:17:25.096505: step 10082, loss 4.19005e-06, acc 1
2016-11-12T19:17:25.153550: step 10083, loss 0.00298963, acc 1
2016-11-12T19:17:25.213132: step 10084, loss 0.00296942, acc 1
2016-11-12T19:17:25.273067: step 10085, loss 0.000799483, acc 1
2016-11-12T19:17:25.331046: step 10086, loss 0.000778245, acc 1
2016-11-12T19:17:25.389666: step 10087, loss 0.00028143, acc 1
2016-11-12T19:17:25.447653: step 10088, loss 0.00141327, acc 1
2016-11-12T19:17:25.506086: step 10089, loss 0.000502533, acc 1
2016-11-12T19:17:25.564626: step 10090, loss 0.0024314, acc 1
2016-11-12T19:17:25.621658: step 10091, loss 0.000167115, acc 1
2016-11-12T19:17:25.679004: step 10092, loss 0.000410082, acc 1
2016-11-12T19:17:25.736203: step 10093, loss 0.010638, acc 1
2016-11-12T19:17:25.793072: step 10094, loss 0.0116027, acc 0.984375
2016-11-12T19:17:25.851992: step 10095, loss 0.0250094, acc 0.984375
2016-11-12T19:17:25.911977: step 10096, loss 0.000696156, acc 1
2016-11-12T19:17:25.972046: step 10097, loss 0.0160059, acc 1
2016-11-12T19:17:26.030023: step 10098, loss 0.0257771, acc 0.984375
2016-11-12T19:17:26.089102: step 10099, loss 0.0190692, acc 0.984375
2016-11-12T19:17:26.148150: step 10100, loss 0.00444431, acc 1

Evaluation:
2016-11-12T19:17:26.220690: step 10100, loss 4.31596, acc 0.544

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-10100

2016-11-12T19:17:26.709858: step 10101, loss 0.0263252, acc 0.984375
2016-11-12T19:17:26.768848: step 10102, loss 0.000223891, acc 1
2016-11-12T19:17:26.828811: step 10103, loss 0.00055807, acc 1
2016-11-12T19:17:26.887741: step 10104, loss 0.000928718, acc 1
2016-11-12T19:17:26.948748: step 10105, loss 0.00962231, acc 1
2016-11-12T19:17:27.007286: step 10106, loss 0.0207675, acc 0.984375
2016-11-12T19:17:27.065049: step 10107, loss 6.56363e-05, acc 1
2016-11-12T19:17:27.124261: step 10108, loss 0.00941482, acc 1
2016-11-12T19:17:27.186303: step 10109, loss 0.00258492, acc 1
2016-11-12T19:17:27.247896: step 10110, loss 0.000681073, acc 1
2016-11-12T19:17:27.304946: step 10111, loss 0.0200649, acc 0.984375
2016-11-12T19:17:27.365232: step 10112, loss 0.0850072, acc 0.984375
2016-11-12T19:17:27.424709: step 10113, loss 0.00299733, acc 1
2016-11-12T19:17:27.485332: step 10114, loss 9.70953e-05, acc 1
2016-11-12T19:17:27.541952: step 10115, loss 8.82212e-05, acc 1
2016-11-12T19:17:27.600206: step 10116, loss 0.0001287, acc 1
2016-11-12T19:17:27.658797: step 10117, loss 0.00885125, acc 1
2016-11-12T19:17:27.717060: step 10118, loss 0.0115395, acc 1
2016-11-12T19:17:27.775905: step 10119, loss 0.000597163, acc 1
2016-11-12T19:17:27.835459: step 10120, loss 7.52073e-05, acc 1
2016-11-12T19:17:27.892651: step 10121, loss 0.000410766, acc 1
2016-11-12T19:17:27.951446: step 10122, loss 5.34665e-05, acc 1
2016-11-12T19:17:28.008095: step 10123, loss 0.00473262, acc 1
2016-11-12T19:17:28.066116: step 10124, loss 0.000303915, acc 1
2016-11-12T19:17:28.126679: step 10125, loss 0.00988984, acc 1
2016-11-12T19:17:28.185927: step 10126, loss 0.00680538, acc 1
2016-11-12T19:17:28.244684: step 10127, loss 0.00495581, acc 1
2016-11-12T19:17:28.304448: step 10128, loss 0.0159385, acc 0.984375
2016-11-12T19:17:28.364217: step 10129, loss 2.90798e-05, acc 1
2016-11-12T19:17:28.422618: step 10130, loss 0.000711015, acc 1
2016-11-12T19:17:28.481072: step 10131, loss 0.00108894, acc 1
2016-11-12T19:17:28.538159: step 10132, loss 0.0197702, acc 0.984375
2016-11-12T19:17:28.595275: step 10133, loss 0.0127894, acc 0.984375
2016-11-12T19:17:28.653834: step 10134, loss 0.00120525, acc 1
2016-11-12T19:17:28.713711: step 10135, loss 0.00742191, acc 1
2016-11-12T19:17:28.771759: step 10136, loss 0.000778642, acc 1
2016-11-12T19:17:28.828673: step 10137, loss 0.0109427, acc 1
2016-11-12T19:17:28.887094: step 10138, loss 0.000321903, acc 1
2016-11-12T19:17:28.944862: step 10139, loss 0.000718025, acc 1
2016-11-12T19:17:29.005818: step 10140, loss 0.00046061, acc 1
2016-11-12T19:17:29.065280: step 10141, loss 0.00814815, acc 1
2016-11-12T19:17:29.122704: step 10142, loss 0.00014153, acc 1
2016-11-12T19:17:29.180755: step 10143, loss 0.00167434, acc 1
2016-11-12T19:17:29.238011: step 10144, loss 0.0650617, acc 0.984375
2016-11-12T19:17:29.297311: step 10145, loss 0.00662679, acc 1
2016-11-12T19:17:29.353900: step 10146, loss 0.00110516, acc 1
2016-11-12T19:17:29.410656: step 10147, loss 0.0598015, acc 0.96875
2016-11-12T19:17:29.472715: step 10148, loss 0.000344658, acc 1
2016-11-12T19:17:29.530148: step 10149, loss 0.0121818, acc 0.984375
2016-11-12T19:17:29.588995: step 10150, loss 0.0284832, acc 0.984375
2016-11-12T19:17:29.649603: step 10151, loss 0.00289569, acc 1
2016-11-12T19:17:29.708862: step 10152, loss 0.00214996, acc 1
2016-11-12T19:17:29.748523: step 10153, loss 0.00135562, acc 1
2016-11-12T19:17:29.812875: step 10154, loss 5.87448e-06, acc 1
2016-11-12T19:17:29.868801: step 10155, loss 0.00126168, acc 1
2016-11-12T19:17:29.925764: step 10156, loss 0.000883297, acc 1
2016-11-12T19:17:29.984205: step 10157, loss 0.000267875, acc 1
2016-11-12T19:17:30.044983: step 10158, loss 0.00740035, acc 1
2016-11-12T19:17:30.103692: step 10159, loss 0.010132, acc 1
2016-11-12T19:17:30.163635: step 10160, loss 0.0278901, acc 0.984375
2016-11-12T19:17:30.220741: step 10161, loss 0.000324341, acc 1
2016-11-12T19:17:30.280028: step 10162, loss 0.00356976, acc 1
2016-11-12T19:17:30.337840: step 10163, loss 0.00141111, acc 1
2016-11-12T19:17:30.396850: step 10164, loss 2.59168e-05, acc 1
2016-11-12T19:17:30.453563: step 10165, loss 0.010299, acc 1
2016-11-12T19:17:30.513126: step 10166, loss 0.0412879, acc 0.984375
2016-11-12T19:17:30.572899: step 10167, loss 0.00179938, acc 1
2016-11-12T19:17:30.632882: step 10168, loss 0.0149357, acc 0.984375
2016-11-12T19:17:30.690349: step 10169, loss 0.000791936, acc 1
2016-11-12T19:17:30.748647: step 10170, loss 0.0923355, acc 0.984375
2016-11-12T19:17:30.807825: step 10171, loss 0.000530412, acc 1
2016-11-12T19:17:30.865595: step 10172, loss 0.0103492, acc 1
2016-11-12T19:17:30.922855: step 10173, loss 0.0056875, acc 1
2016-11-12T19:17:30.983577: step 10174, loss 0.00133372, acc 1
2016-11-12T19:17:31.041759: step 10175, loss 0.003879, acc 1
2016-11-12T19:17:31.101449: step 10176, loss 0.000124764, acc 1
2016-11-12T19:17:31.156367: step 10177, loss 0.00155692, acc 1
2016-11-12T19:17:31.213019: step 10178, loss 0.00238768, acc 1
2016-11-12T19:17:31.270902: step 10179, loss 0.00027422, acc 1
2016-11-12T19:17:31.328452: step 10180, loss 0.00153736, acc 1
2016-11-12T19:17:31.387660: step 10181, loss 0.0962367, acc 0.96875
2016-11-12T19:17:31.447007: step 10182, loss 0.000396957, acc 1
2016-11-12T19:17:31.505026: step 10183, loss 0.00343566, acc 1
2016-11-12T19:17:31.562460: step 10184, loss 0.0329039, acc 0.984375
2016-11-12T19:17:31.620506: step 10185, loss 0.00381146, acc 1
2016-11-12T19:17:31.679226: step 10186, loss 0.000321467, acc 1
2016-11-12T19:17:31.735856: step 10187, loss 0.0100532, acc 1
2016-11-12T19:17:31.796598: step 10188, loss 0.03707, acc 0.984375
2016-11-12T19:17:31.854202: step 10189, loss 0.0301875, acc 0.984375
2016-11-12T19:17:31.912891: step 10190, loss 0.00970806, acc 1
2016-11-12T19:17:31.971283: step 10191, loss 0.000732428, acc 1
2016-11-12T19:17:32.028914: step 10192, loss 0.0844419, acc 0.984375
2016-11-12T19:17:32.088531: step 10193, loss 0.000258178, acc 1
2016-11-12T19:17:32.148059: step 10194, loss 5.05891e-05, acc 1
2016-11-12T19:17:32.204637: step 10195, loss 0.0139568, acc 0.984375
2016-11-12T19:17:32.263516: step 10196, loss 0.0326196, acc 0.984375
2016-11-12T19:17:32.320309: step 10197, loss 9.99841e-05, acc 1
2016-11-12T19:17:32.376980: step 10198, loss 0.0127264, acc 0.984375
2016-11-12T19:17:32.438968: step 10199, loss 0.000530583, acc 1
2016-11-12T19:17:32.496123: step 10200, loss 6.7631e-05, acc 1

Evaluation:
2016-11-12T19:17:32.570818: step 10200, loss 4.43262, acc 0.548

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-10200

2016-11-12T19:17:33.057341: step 10201, loss 5.69081e-05, acc 1
2016-11-12T19:17:33.116993: step 10202, loss 0.000207315, acc 1
2016-11-12T19:17:33.175157: step 10203, loss 0.00171055, acc 1
2016-11-12T19:17:33.234067: step 10204, loss 0.000560418, acc 1
2016-11-12T19:17:33.291308: step 10205, loss 0.0106003, acc 1
2016-11-12T19:17:33.352511: step 10206, loss 7.54029e-05, acc 1
2016-11-12T19:17:33.412136: step 10207, loss 0.000699849, acc 1
2016-11-12T19:17:33.469669: step 10208, loss 0.0128294, acc 0.984375
2016-11-12T19:17:33.529243: step 10209, loss 0.000144943, acc 1
2016-11-12T19:17:33.586085: step 10210, loss 0.000191109, acc 1
2016-11-12T19:17:33.642597: step 10211, loss 0.00662901, acc 1
2016-11-12T19:17:33.703390: step 10212, loss 0.0203257, acc 0.984375
2016-11-12T19:17:33.764546: step 10213, loss 0.0181242, acc 0.984375
2016-11-12T19:17:33.826416: step 10214, loss 3.77909e-05, acc 1
2016-11-12T19:17:33.884974: step 10215, loss 0.0166782, acc 0.984375
2016-11-12T19:17:33.943587: step 10216, loss 0.000138466, acc 1
2016-11-12T19:17:34.001389: step 10217, loss 0.0107577, acc 1
2016-11-12T19:17:34.060643: step 10218, loss 0.000570737, acc 1
2016-11-12T19:17:34.119389: step 10219, loss 5.58243e-05, acc 1
2016-11-12T19:17:34.175908: step 10220, loss 0.0114376, acc 0.984375
2016-11-12T19:17:34.235592: step 10221, loss 8.17712e-05, acc 1
2016-11-12T19:17:34.292492: step 10222, loss 0.000894238, acc 1
2016-11-12T19:17:34.349781: step 10223, loss 0.00567717, acc 1
2016-11-12T19:17:34.388282: step 10224, loss 4.19601e-06, acc 1
2016-11-12T19:17:34.446714: step 10225, loss 0.010368, acc 1
2016-11-12T19:17:34.505782: step 10226, loss 0.00264024, acc 1
2016-11-12T19:17:34.564327: step 10227, loss 0.000519527, acc 1
2016-11-12T19:17:34.622324: step 10228, loss 0.000421326, acc 1
2016-11-12T19:17:34.679965: step 10229, loss 0.00601917, acc 1
2016-11-12T19:17:34.740301: step 10230, loss 0.00689595, acc 1
2016-11-12T19:17:34.797051: step 10231, loss 0.000294037, acc 1
2016-11-12T19:17:34.853945: step 10232, loss 0.0129705, acc 0.984375
2016-11-12T19:17:34.914225: step 10233, loss 0.00117999, acc 1
2016-11-12T19:17:34.973011: step 10234, loss 0.000942213, acc 1
2016-11-12T19:17:35.032841: step 10235, loss 0.000106823, acc 1
2016-11-12T19:17:35.089316: step 10236, loss 0.0359198, acc 0.984375
2016-11-12T19:17:35.146960: step 10237, loss 9.32129e-05, acc 1
2016-11-12T19:17:35.204829: step 10238, loss 0.000241031, acc 1
2016-11-12T19:17:35.261485: step 10239, loss 0.00135464, acc 1
2016-11-12T19:17:35.320135: step 10240, loss 0.000433342, acc 1
2016-11-12T19:17:35.378706: step 10241, loss 9.68662e-05, acc 1
2016-11-12T19:17:35.434629: step 10242, loss 0.000667375, acc 1
2016-11-12T19:17:35.493211: step 10243, loss 0.00837899, acc 1
2016-11-12T19:17:35.552515: step 10244, loss 0.000838484, acc 1
2016-11-12T19:17:35.612383: step 10245, loss 0.0148844, acc 0.984375
2016-11-12T19:17:35.670867: step 10246, loss 0.012641, acc 1
2016-11-12T19:17:35.731564: step 10247, loss 0.000329863, acc 1
2016-11-12T19:17:35.787930: step 10248, loss 0.00354392, acc 1
2016-11-12T19:17:35.845312: step 10249, loss 0.000848222, acc 1
2016-11-12T19:17:35.902701: step 10250, loss 0.00201421, acc 1
2016-11-12T19:17:35.961267: step 10251, loss 5.87079e-05, acc 1
2016-11-12T19:17:36.019663: step 10252, loss 0.0264194, acc 0.984375
2016-11-12T19:17:36.080048: step 10253, loss 0.0316216, acc 0.984375
2016-11-12T19:17:36.137369: step 10254, loss 0.000235625, acc 1
2016-11-12T19:17:36.194921: step 10255, loss 3.04185e-05, acc 1
2016-11-12T19:17:36.253039: step 10256, loss 0.000167613, acc 1
2016-11-12T19:17:36.312161: step 10257, loss 0.000429865, acc 1
2016-11-12T19:17:36.369796: step 10258, loss 0.0101886, acc 1
2016-11-12T19:17:36.430660: step 10259, loss 9.24392e-05, acc 1
2016-11-12T19:17:36.491431: step 10260, loss 0.0124445, acc 0.984375
2016-11-12T19:17:36.551392: step 10261, loss 0.000773979, acc 1
2016-11-12T19:17:36.612108: step 10262, loss 0.00105141, acc 1
2016-11-12T19:17:36.669473: step 10263, loss 0.00438007, acc 1
2016-11-12T19:17:36.727821: step 10264, loss 0.0344135, acc 0.984375
2016-11-12T19:17:36.788898: step 10265, loss 0.0313317, acc 0.984375
2016-11-12T19:17:36.848822: step 10266, loss 0.000117257, acc 1
2016-11-12T19:17:36.905800: step 10267, loss 0.0209536, acc 0.984375
2016-11-12T19:17:36.964044: step 10268, loss 0.00108511, acc 1
2016-11-12T19:17:37.022182: step 10269, loss 0.00132728, acc 1
2016-11-12T19:17:37.081175: step 10270, loss 0.00323795, acc 1
2016-11-12T19:17:37.141121: step 10271, loss 0.00613477, acc 1
2016-11-12T19:17:37.198282: step 10272, loss 0.00233318, acc 1
2016-11-12T19:17:37.258363: step 10273, loss 0.0030879, acc 1
2016-11-12T19:17:37.317166: step 10274, loss 0.00818782, acc 1
2016-11-12T19:17:37.377662: step 10275, loss 0.00140538, acc 1
2016-11-12T19:17:37.435126: step 10276, loss 0.000501926, acc 1
2016-11-12T19:17:37.491797: step 10277, loss 0.000693635, acc 1
2016-11-12T19:17:37.549182: step 10278, loss 0.00143073, acc 1
2016-11-12T19:17:37.607814: step 10279, loss 0.0578213, acc 0.984375
2016-11-12T19:17:37.668562: step 10280, loss 0.00129862, acc 1
2016-11-12T19:17:37.726375: step 10281, loss 0.000186256, acc 1
2016-11-12T19:17:37.782832: step 10282, loss 0.00100701, acc 1
2016-11-12T19:17:37.844129: step 10283, loss 0.000560176, acc 1
2016-11-12T19:17:37.904504: step 10284, loss 0.030316, acc 0.984375
2016-11-12T19:17:37.966061: step 10285, loss 0.000341176, acc 1
2016-11-12T19:17:38.027873: step 10286, loss 0.11515, acc 0.984375
2016-11-12T19:17:38.088594: step 10287, loss 0.00903624, acc 1
2016-11-12T19:17:38.147991: step 10288, loss 8.27688e-05, acc 1
2016-11-12T19:17:38.205578: step 10289, loss 6.10674e-05, acc 1
2016-11-12T19:17:38.261712: step 10290, loss 0.0255752, acc 0.984375
2016-11-12T19:17:38.322502: step 10291, loss 0.0395679, acc 0.96875
2016-11-12T19:17:38.382279: step 10292, loss 0.0102184, acc 1
2016-11-12T19:17:38.442967: step 10293, loss 0.195359, acc 0.984375
2016-11-12T19:17:38.501460: step 10294, loss 0.00597675, acc 1
2016-11-12T19:17:38.540190: step 10295, loss 0.00082947, acc 1
2016-11-12T19:17:38.601168: step 10296, loss 0.0159257, acc 0.984375
2016-11-12T19:17:38.658554: step 10297, loss 0.00251243, acc 1
2016-11-12T19:17:38.717527: step 10298, loss 0.000184457, acc 1
2016-11-12T19:17:38.775470: step 10299, loss 0.000690854, acc 1
2016-11-12T19:17:38.833166: step 10300, loss 0.00336519, acc 1

Evaluation:
2016-11-12T19:17:38.904938: step 10300, loss 4.50895, acc 0.562

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-10300

2016-11-12T19:17:39.388874: step 10301, loss 0.000294543, acc 1
2016-11-12T19:17:39.452019: step 10302, loss 6.28801e-05, acc 1
2016-11-12T19:17:39.511476: step 10303, loss 0.00303586, acc 1
2016-11-12T19:17:39.569335: step 10304, loss 0.00204551, acc 1
2016-11-12T19:17:39.631554: step 10305, loss 0.00170499, acc 1
2016-11-12T19:17:39.690289: step 10306, loss 0.0130246, acc 0.984375
2016-11-12T19:17:39.749063: step 10307, loss 0.0173562, acc 0.984375
2016-11-12T19:17:39.807228: step 10308, loss 0.00242632, acc 1
2016-11-12T19:17:39.864860: step 10309, loss 0.000295191, acc 1
2016-11-12T19:17:39.922371: step 10310, loss 0.00175467, acc 1
2016-11-12T19:17:39.980291: step 10311, loss 0.00121673, acc 1
2016-11-12T19:17:40.037584: step 10312, loss 0.00769629, acc 1
2016-11-12T19:17:40.095909: step 10313, loss 0.0383049, acc 0.96875
2016-11-12T19:17:40.154446: step 10314, loss 0.000855835, acc 1
2016-11-12T19:17:40.212520: step 10315, loss 0.0109611, acc 0.984375
2016-11-12T19:17:40.269684: step 10316, loss 0.0102966, acc 1
2016-11-12T19:17:40.328613: step 10317, loss 0.127082, acc 0.984375
2016-11-12T19:17:40.387020: step 10318, loss 5.88441e-05, acc 1
2016-11-12T19:17:40.446414: step 10319, loss 0.0220824, acc 0.984375
2016-11-12T19:17:40.503998: step 10320, loss 0.000735904, acc 1
2016-11-12T19:17:40.561615: step 10321, loss 0.00775934, acc 1
2016-11-12T19:17:40.620699: step 10322, loss 0.000243701, acc 1
2016-11-12T19:17:40.677184: step 10323, loss 0.000100413, acc 1
2016-11-12T19:17:40.734045: step 10324, loss 0.00402003, acc 1
2016-11-12T19:17:40.792995: step 10325, loss 0.00295013, acc 1
2016-11-12T19:17:40.851354: step 10326, loss 0.000487495, acc 1
2016-11-12T19:17:40.912100: step 10327, loss 0.0148895, acc 1
2016-11-12T19:17:40.971538: step 10328, loss 0.0199499, acc 0.984375
2016-11-12T19:17:41.029392: step 10329, loss 0.00480463, acc 1
2016-11-12T19:17:41.086946: step 10330, loss 0.0181222, acc 0.984375
2016-11-12T19:17:41.144656: step 10331, loss 0.000933324, acc 1
2016-11-12T19:17:41.204848: step 10332, loss 0.0503964, acc 0.984375
2016-11-12T19:17:41.264770: step 10333, loss 0.000375567, acc 1
2016-11-12T19:17:41.323529: step 10334, loss 0.0203293, acc 0.984375
2016-11-12T19:17:41.381922: step 10335, loss 0.00134441, acc 1
2016-11-12T19:17:41.439996: step 10336, loss 0.0134998, acc 0.984375
2016-11-12T19:17:41.501139: step 10337, loss 0.000222033, acc 1
2016-11-12T19:17:41.560213: step 10338, loss 0.00331278, acc 1
2016-11-12T19:17:41.618028: step 10339, loss 0.00619043, acc 1
2016-11-12T19:17:41.677597: step 10340, loss 0.0035178, acc 1
2016-11-12T19:17:41.738281: step 10341, loss 3.39851e-05, acc 1
2016-11-12T19:17:41.797554: step 10342, loss 0.0050645, acc 1
2016-11-12T19:17:41.856703: step 10343, loss 0.00186262, acc 1
2016-11-12T19:17:41.915432: step 10344, loss 7.80887e-06, acc 1
2016-11-12T19:17:41.975114: step 10345, loss 0.00159767, acc 1
2016-11-12T19:17:42.033699: step 10346, loss 0.000317342, acc 1
2016-11-12T19:17:42.092056: step 10347, loss 0.0408486, acc 0.984375
2016-11-12T19:17:42.151275: step 10348, loss 0.00270768, acc 1
2016-11-12T19:17:42.208984: step 10349, loss 0.000122522, acc 1
2016-11-12T19:17:42.266034: step 10350, loss 8.07704e-05, acc 1
2016-11-12T19:17:42.324493: step 10351, loss 0.0293171, acc 0.984375
2016-11-12T19:17:42.382046: step 10352, loss 3.3246e-05, acc 1
2016-11-12T19:17:42.440706: step 10353, loss 0.0408812, acc 0.984375
2016-11-12T19:17:42.500946: step 10354, loss 0.0340371, acc 0.984375
2016-11-12T19:17:42.561105: step 10355, loss 0.00350249, acc 1
2016-11-12T19:17:42.620928: step 10356, loss 0.000756641, acc 1
2016-11-12T19:17:42.680085: step 10357, loss 0.00123053, acc 1
2016-11-12T19:17:42.737670: step 10358, loss 0.0449433, acc 0.96875
2016-11-12T19:17:42.797056: step 10359, loss 0.000692058, acc 1
2016-11-12T19:17:42.856830: step 10360, loss 0.32582, acc 0.953125
2016-11-12T19:17:42.918456: step 10361, loss 9.13411e-05, acc 1
2016-11-12T19:17:42.975415: step 10362, loss 0.00128043, acc 1
2016-11-12T19:17:43.032298: step 10363, loss 0.000127025, acc 1
2016-11-12T19:17:43.089558: step 10364, loss 0.000697672, acc 1
2016-11-12T19:17:43.147069: step 10365, loss 8.694e-05, acc 1
2016-11-12T19:17:43.186171: step 10366, loss 7.73985e-05, acc 1
2016-11-12T19:17:43.243373: step 10367, loss 0.000116919, acc 1
2016-11-12T19:17:43.300970: step 10368, loss 0.0749497, acc 0.96875
2016-11-12T19:17:43.360058: step 10369, loss 0.00159392, acc 1
2016-11-12T19:17:43.421306: step 10370, loss 6.25399e-05, acc 1
2016-11-12T19:17:43.481356: step 10371, loss 0.000317117, acc 1
2016-11-12T19:17:43.540775: step 10372, loss 0.00642538, acc 1
2016-11-12T19:17:43.603654: step 10373, loss 0.000624331, acc 1
2016-11-12T19:17:43.665701: step 10374, loss 0.0318515, acc 0.984375
2016-11-12T19:17:43.724043: step 10375, loss 0.0273266, acc 0.984375
2016-11-12T19:17:43.781605: step 10376, loss 0.000102901, acc 1
2016-11-12T19:17:43.840491: step 10377, loss 0.00731255, acc 1
2016-11-12T19:17:43.902723: step 10378, loss 0.00749348, acc 1
2016-11-12T19:17:43.961243: step 10379, loss 0.0151444, acc 0.984375
2016-11-12T19:17:44.020782: step 10380, loss 0.000111928, acc 1
2016-11-12T19:17:44.076845: step 10381, loss 0.000410865, acc 1
2016-11-12T19:17:44.136926: step 10382, loss 0.00212322, acc 1
2016-11-12T19:17:44.196241: step 10383, loss 9.3119e-05, acc 1
2016-11-12T19:17:44.252605: step 10384, loss 5.49191e-05, acc 1
2016-11-12T19:17:44.309118: step 10385, loss 0.0100372, acc 1
2016-11-12T19:17:44.368361: step 10386, loss 0.0143004, acc 0.984375
2016-11-12T19:17:44.424571: step 10387, loss 0.00020109, acc 1
2016-11-12T19:17:44.481437: step 10388, loss 0.00567996, acc 1
2016-11-12T19:17:44.540736: step 10389, loss 0.000196633, acc 1
2016-11-12T19:17:44.600878: step 10390, loss 0.00690001, acc 1
2016-11-12T19:17:44.658861: step 10391, loss 0.0102268, acc 1
2016-11-12T19:17:44.717409: step 10392, loss 5.12854e-05, acc 1
2016-11-12T19:17:44.776496: step 10393, loss 8.21872e-05, acc 1
2016-11-12T19:17:44.832953: step 10394, loss 0.0106882, acc 1
2016-11-12T19:17:44.892979: step 10395, loss 5.12806e-05, acc 1
2016-11-12T19:17:44.952604: step 10396, loss 0.000384246, acc 1
2016-11-12T19:17:45.008681: step 10397, loss 0.000587236, acc 1
2016-11-12T19:17:45.065000: step 10398, loss 0.030759, acc 0.984375
2016-11-12T19:17:45.123423: step 10399, loss 4.62799e-05, acc 1
2016-11-12T19:17:45.180982: step 10400, loss 0.00390853, acc 1

Evaluation:
2016-11-12T19:17:45.254329: step 10400, loss 4.58253, acc 0.546

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-10400

2016-11-12T19:17:45.745087: step 10401, loss 0.0140086, acc 0.984375
2016-11-12T19:17:45.804258: step 10402, loss 0.000158259, acc 1
2016-11-12T19:17:45.861309: step 10403, loss 0.000122835, acc 1
2016-11-12T19:17:45.920406: step 10404, loss 0.0126983, acc 0.984375
2016-11-12T19:17:45.981158: step 10405, loss 0.0121923, acc 0.984375
2016-11-12T19:17:46.040597: step 10406, loss 0.000124338, acc 1
2016-11-12T19:17:46.100003: step 10407, loss 0.000133804, acc 1
2016-11-12T19:17:46.156999: step 10408, loss 2.63044e-05, acc 1
2016-11-12T19:17:46.215402: step 10409, loss 0.000181081, acc 1
2016-11-12T19:17:46.274571: step 10410, loss 0.0276376, acc 0.96875
2016-11-12T19:17:46.333410: step 10411, loss 0.000282497, acc 1
2016-11-12T19:17:46.390037: step 10412, loss 0.000280343, acc 1
2016-11-12T19:17:46.448854: step 10413, loss 0.000119354, acc 1
2016-11-12T19:17:46.506880: step 10414, loss 0.00149631, acc 1
2016-11-12T19:17:46.567256: step 10415, loss 0.0482508, acc 0.984375
2016-11-12T19:17:46.627682: step 10416, loss 0.0145125, acc 0.984375
2016-11-12T19:17:46.684527: step 10417, loss 0.15485, acc 0.984375
2016-11-12T19:17:46.744924: step 10418, loss 0.00588282, acc 1
2016-11-12T19:17:46.804714: step 10419, loss 2.55532e-05, acc 1
2016-11-12T19:17:46.860757: step 10420, loss 0.00011089, acc 1
2016-11-12T19:17:46.917259: step 10421, loss 0.00323251, acc 1
2016-11-12T19:17:46.976459: step 10422, loss 0.00784889, acc 1
2016-11-12T19:17:47.034509: step 10423, loss 0.000471979, acc 1
2016-11-12T19:17:47.093564: step 10424, loss 0.00139638, acc 1
2016-11-12T19:17:47.151196: step 10425, loss 0.00030987, acc 1
2016-11-12T19:17:47.208057: step 10426, loss 0.0128572, acc 1
2016-11-12T19:17:47.265565: step 10427, loss 0.0136244, acc 0.984375
2016-11-12T19:17:47.323529: step 10428, loss 0.00110968, acc 1
2016-11-12T19:17:47.383218: step 10429, loss 0.0093472, acc 1
2016-11-12T19:17:47.445240: step 10430, loss 0.00169772, acc 1
2016-11-12T19:17:47.502938: step 10431, loss 0.00902468, acc 1
2016-11-12T19:17:47.561267: step 10432, loss 0.0176671, acc 0.984375
2016-11-12T19:17:47.619453: step 10433, loss 0.00196628, acc 1
2016-11-12T19:17:47.679345: step 10434, loss 0.00430965, acc 1
2016-11-12T19:17:47.738628: step 10435, loss 0.00847314, acc 1
2016-11-12T19:17:47.797517: step 10436, loss 0.00278603, acc 1
2016-11-12T19:17:47.838035: step 10437, loss 0.000712229, acc 1
2016-11-12T19:17:47.900882: step 10438, loss 0.0251136, acc 0.96875
2016-11-12T19:17:47.960665: step 10439, loss 9.19516e-05, acc 1
2016-11-12T19:17:48.017705: step 10440, loss 0.000120239, acc 1
2016-11-12T19:17:48.076770: step 10441, loss 0.00466793, acc 1
2016-11-12T19:17:48.134018: step 10442, loss 0.000930851, acc 1
2016-11-12T19:17:48.190992: step 10443, loss 0.000121755, acc 1
2016-11-12T19:17:48.249009: step 10444, loss 0.000477867, acc 1
2016-11-12T19:17:48.309705: step 10445, loss 0.00242977, acc 1
2016-11-12T19:17:48.367249: step 10446, loss 0.000503332, acc 1
2016-11-12T19:17:48.429043: step 10447, loss 0.0251768, acc 0.984375
2016-11-12T19:17:48.487037: step 10448, loss 0.013669, acc 0.984375
2016-11-12T19:17:48.546869: step 10449, loss 0.0311238, acc 0.984375
2016-11-12T19:17:48.606171: step 10450, loss 9.29294e-05, acc 1
2016-11-12T19:17:48.664456: step 10451, loss 0.0136847, acc 0.984375
2016-11-12T19:17:48.723139: step 10452, loss 0.036164, acc 0.984375
2016-11-12T19:17:48.781670: step 10453, loss 0.011681, acc 0.984375
2016-11-12T19:17:48.839795: step 10454, loss 0.00776223, acc 1
2016-11-12T19:17:48.898969: step 10455, loss 0.00990696, acc 1
2016-11-12T19:17:48.957358: step 10456, loss 8.08507e-05, acc 1
2016-11-12T19:17:49.014255: step 10457, loss 0.0213472, acc 0.984375
2016-11-12T19:17:49.073018: step 10458, loss 0.00722157, acc 1
2016-11-12T19:17:49.133248: step 10459, loss 0.023711, acc 0.984375
2016-11-12T19:17:49.196019: step 10460, loss 0.0135293, acc 0.984375
2016-11-12T19:17:49.256476: step 10461, loss 0.000281944, acc 1
2016-11-12T19:17:49.313514: step 10462, loss 0.00777975, acc 1
2016-11-12T19:17:49.373072: step 10463, loss 0.000236478, acc 1
2016-11-12T19:17:49.430020: step 10464, loss 0.000252087, acc 1
2016-11-12T19:17:49.487478: step 10465, loss 6.31226e-05, acc 1
2016-11-12T19:17:49.547901: step 10466, loss 0.000461473, acc 1
2016-11-12T19:17:49.604857: step 10467, loss 0.000175434, acc 1
2016-11-12T19:17:49.664399: step 10468, loss 0.000690388, acc 1
2016-11-12T19:17:49.720807: step 10469, loss 0.000326491, acc 1
2016-11-12T19:17:49.781447: step 10470, loss 0.00182878, acc 1
2016-11-12T19:17:49.840801: step 10471, loss 0.0148351, acc 1
2016-11-12T19:17:49.900904: step 10472, loss 0.000627267, acc 1
2016-11-12T19:17:49.960303: step 10473, loss 0.000456603, acc 1
2016-11-12T19:17:50.018004: step 10474, loss 0.0959432, acc 0.96875
2016-11-12T19:17:50.077781: step 10475, loss 4.8808e-05, acc 1
2016-11-12T19:17:50.133233: step 10476, loss 0.0239871, acc 0.984375
2016-11-12T19:17:50.191841: step 10477, loss 0.00147711, acc 1
2016-11-12T19:17:50.252746: step 10478, loss 0.000671512, acc 1
2016-11-12T19:17:50.311386: step 10479, loss 0.000639001, acc 1
2016-11-12T19:17:50.368636: step 10480, loss 3.08035e-05, acc 1
2016-11-12T19:17:50.424987: step 10481, loss 0.0278577, acc 0.984375
2016-11-12T19:17:50.484555: step 10482, loss 0.00163299, acc 1
2016-11-12T19:17:50.542473: step 10483, loss 0.00111741, acc 1
2016-11-12T19:17:50.600709: step 10484, loss 4.5071e-05, acc 1
2016-11-12T19:17:50.658274: step 10485, loss 8.89701e-06, acc 1
2016-11-12T19:17:50.714510: step 10486, loss 0.00040947, acc 1
2016-11-12T19:17:50.771965: step 10487, loss 0.00101094, acc 1
2016-11-12T19:17:50.831079: step 10488, loss 0.000514352, acc 1
2016-11-12T19:17:50.888495: step 10489, loss 0.000497098, acc 1
2016-11-12T19:17:50.946463: step 10490, loss 0.000322373, acc 1
2016-11-12T19:17:51.006301: step 10491, loss 0.00159941, acc 1
2016-11-12T19:17:51.064757: step 10492, loss 0.000426049, acc 1
2016-11-12T19:17:51.121790: step 10493, loss 0.000333435, acc 1
2016-11-12T19:17:51.178830: step 10494, loss 0.00083723, acc 1
2016-11-12T19:17:51.235945: step 10495, loss 4.88671e-05, acc 1
2016-11-12T19:17:51.292354: step 10496, loss 0.000128234, acc 1
2016-11-12T19:17:51.349170: step 10497, loss 0.0313333, acc 0.984375
2016-11-12T19:17:51.407252: step 10498, loss 0.00873254, acc 1
2016-11-12T19:17:51.465321: step 10499, loss 0.0253511, acc 0.984375
2016-11-12T19:17:51.525828: step 10500, loss 0.00822716, acc 1

Evaluation:
2016-11-12T19:17:51.597967: step 10500, loss 4.54702, acc 0.548

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-10500

2016-11-12T19:17:52.084959: step 10501, loss 0.183416, acc 0.984375
2016-11-12T19:17:52.144934: step 10502, loss 0.0056266, acc 1
2016-11-12T19:17:52.207133: step 10503, loss 0.0194646, acc 0.984375
2016-11-12T19:17:52.267906: step 10504, loss 0.0124144, acc 1
2016-11-12T19:17:52.329730: step 10505, loss 0.0147527, acc 0.984375
2016-11-12T19:17:52.388737: step 10506, loss 0.000757742, acc 1
2016-11-12T19:17:52.448989: step 10507, loss 0.011007, acc 0.984375
2016-11-12T19:17:52.489892: step 10508, loss 0.000155523, acc 1
2016-11-12T19:17:52.549178: step 10509, loss 8.64796e-05, acc 1
2016-11-12T19:17:52.605979: step 10510, loss 0.00998997, acc 1
2016-11-12T19:17:52.666034: step 10511, loss 3.42981e-05, acc 1
2016-11-12T19:17:52.723550: step 10512, loss 0.00911052, acc 1
2016-11-12T19:17:52.784149: step 10513, loss 0.00971367, acc 1
2016-11-12T19:17:52.844857: step 10514, loss 0.000612372, acc 1
2016-11-12T19:17:52.902612: step 10515, loss 0.00759722, acc 1
2016-11-12T19:17:52.965800: step 10516, loss 7.83301e-05, acc 1
2016-11-12T19:17:53.022578: step 10517, loss 0.0112505, acc 0.984375
2016-11-12T19:17:53.080528: step 10518, loss 0.000789614, acc 1
2016-11-12T19:17:53.140179: step 10519, loss 9.53352e-06, acc 1
2016-11-12T19:17:53.197283: step 10520, loss 0.000352367, acc 1
2016-11-12T19:17:53.258063: step 10521, loss 0.00015312, acc 1
2016-11-12T19:17:53.314976: step 10522, loss 0.000417318, acc 1
2016-11-12T19:17:53.375493: step 10523, loss 0.00833962, acc 1
2016-11-12T19:17:53.433821: step 10524, loss 0.0145679, acc 0.984375
2016-11-12T19:17:53.492528: step 10525, loss 4.24398e-05, acc 1
2016-11-12T19:17:53.549961: step 10526, loss 5.1241e-05, acc 1
2016-11-12T19:17:53.608241: step 10527, loss 2.94365e-05, acc 1
2016-11-12T19:17:53.666390: step 10528, loss 0.00693709, acc 1
2016-11-12T19:17:53.723517: step 10529, loss 0.0334086, acc 0.96875
2016-11-12T19:17:53.783346: step 10530, loss 0.0130242, acc 0.984375
2016-11-12T19:17:53.843365: step 10531, loss 0.000341032, acc 1
2016-11-12T19:17:53.900422: step 10532, loss 0.0913019, acc 0.984375
2016-11-12T19:17:53.959496: step 10533, loss 1.0269e-05, acc 1
2016-11-12T19:17:54.019558: step 10534, loss 0.000381365, acc 1
2016-11-12T19:17:54.082850: step 10535, loss 0.00799724, acc 1
2016-11-12T19:17:54.142999: step 10536, loss 0.0015018, acc 1
2016-11-12T19:17:54.201793: step 10537, loss 0.000351333, acc 1
2016-11-12T19:17:54.263116: step 10538, loss 0.000276228, acc 1
2016-11-12T19:17:54.320696: step 10539, loss 0.000514975, acc 1
2016-11-12T19:17:54.380056: step 10540, loss 9.45538e-05, acc 1
2016-11-12T19:17:54.440302: step 10541, loss 0.0164286, acc 0.984375
2016-11-12T19:17:54.497599: step 10542, loss 0.0606088, acc 0.96875
2016-11-12T19:17:54.555797: step 10543, loss 0.0138745, acc 0.984375
2016-11-12T19:17:54.614193: step 10544, loss 0.00128033, acc 1
2016-11-12T19:17:54.673881: step 10545, loss 0.0380897, acc 0.984375
2016-11-12T19:17:54.732931: step 10546, loss 0.00152196, acc 1
2016-11-12T19:17:54.792012: step 10547, loss 0.00173684, acc 1
2016-11-12T19:17:54.853712: step 10548, loss 6.06075e-06, acc 1
2016-11-12T19:17:54.910187: step 10549, loss 0.0618441, acc 0.984375
2016-11-12T19:17:54.968323: step 10550, loss 0.00116584, acc 1
2016-11-12T19:17:55.027229: step 10551, loss 3.84677e-05, acc 1
2016-11-12T19:17:55.084184: step 10552, loss 0.000675141, acc 1
2016-11-12T19:17:55.142378: step 10553, loss 0.00725287, acc 1
2016-11-12T19:17:55.201285: step 10554, loss 0.000632318, acc 1
2016-11-12T19:17:55.258440: step 10555, loss 0.000187394, acc 1
2016-11-12T19:17:55.316986: step 10556, loss 0.00170357, acc 1
2016-11-12T19:17:55.374179: step 10557, loss 0.000229474, acc 1
2016-11-12T19:17:55.432880: step 10558, loss 0.0113064, acc 0.984375
2016-11-12T19:17:55.491600: step 10559, loss 0.0164731, acc 0.984375
2016-11-12T19:17:55.551132: step 10560, loss 0.00620173, acc 1
2016-11-12T19:17:55.607692: step 10561, loss 0.00077307, acc 1
2016-11-12T19:17:55.664575: step 10562, loss 0.00361057, acc 1
2016-11-12T19:17:55.723707: step 10563, loss 0.0370889, acc 0.984375
2016-11-12T19:17:55.781351: step 10564, loss 0.0623655, acc 0.96875
2016-11-12T19:17:55.838902: step 10565, loss 0.0065794, acc 1
2016-11-12T19:17:55.898641: step 10566, loss 0.00762391, acc 1
2016-11-12T19:17:55.957567: step 10567, loss 0.000341886, acc 1
2016-11-12T19:17:56.016260: step 10568, loss 7.79803e-05, acc 1
2016-11-12T19:17:56.075216: step 10569, loss 0.000666642, acc 1
2016-11-12T19:17:56.132726: step 10570, loss 0.11282, acc 0.984375
2016-11-12T19:17:56.192582: step 10571, loss 0.00150225, acc 1
2016-11-12T19:17:56.261797: step 10572, loss 0.00024641, acc 1
2016-11-12T19:17:56.321029: step 10573, loss 0.00256164, acc 1
2016-11-12T19:17:56.384903: step 10574, loss 0.000318275, acc 1
2016-11-12T19:17:56.442315: step 10575, loss 0.0352676, acc 0.984375
2016-11-12T19:17:56.500449: step 10576, loss 0.0431379, acc 0.96875
2016-11-12T19:17:56.560630: step 10577, loss 0.0135092, acc 1
2016-11-12T19:17:56.619513: step 10578, loss 0.0192061, acc 0.984375
2016-11-12T19:17:56.660489: step 10579, loss 4.61831e-05, acc 1
2016-11-12T19:17:56.719120: step 10580, loss 0.0142123, acc 0.984375
2016-11-12T19:17:56.777072: step 10581, loss 1.10495e-05, acc 1
2016-11-12T19:17:56.835177: step 10582, loss 0.000528743, acc 1
2016-11-12T19:17:56.892509: step 10583, loss 0.017928, acc 0.984375
2016-11-12T19:17:56.952706: step 10584, loss 0.00639839, acc 1
2016-11-12T19:17:57.012706: step 10585, loss 0.000145138, acc 1
2016-11-12T19:17:57.069425: step 10586, loss 5.22003e-05, acc 1
2016-11-12T19:17:57.127877: step 10587, loss 0.000145365, acc 1
2016-11-12T19:17:57.188629: step 10588, loss 0.00749723, acc 1
2016-11-12T19:17:57.248882: step 10589, loss 0.00371445, acc 1
2016-11-12T19:17:57.306781: step 10590, loss 0.00241712, acc 1
2016-11-12T19:17:57.364510: step 10591, loss 0.0277187, acc 0.984375
2016-11-12T19:17:57.423348: step 10592, loss 0.00243334, acc 1
2016-11-12T19:17:57.480899: step 10593, loss 0.00156505, acc 1
2016-11-12T19:17:57.539823: step 10594, loss 0.023756, acc 0.984375
2016-11-12T19:17:57.599095: step 10595, loss 0.00685731, acc 1
2016-11-12T19:17:57.657021: step 10596, loss 0.00047726, acc 1
2016-11-12T19:17:57.715512: step 10597, loss 0.000308786, acc 1
2016-11-12T19:17:57.772995: step 10598, loss 0.00035344, acc 1
2016-11-12T19:17:57.830754: step 10599, loss 0.00164232, acc 1
2016-11-12T19:17:57.887736: step 10600, loss 0.000155387, acc 1

Evaluation:
2016-11-12T19:17:57.958512: step 10600, loss 4.43435, acc 0.546

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-10600

2016-11-12T19:17:58.442913: step 10601, loss 0.0230233, acc 0.984375
2016-11-12T19:17:58.501846: step 10602, loss 0.0148011, acc 1
2016-11-12T19:17:58.560898: step 10603, loss 0.000164371, acc 1
2016-11-12T19:17:58.620485: step 10604, loss 0.000297026, acc 1
2016-11-12T19:17:58.678471: step 10605, loss 0.00939392, acc 1
2016-11-12T19:17:58.737096: step 10606, loss 0.000794355, acc 1
2016-11-12T19:17:58.799098: step 10607, loss 0.00661052, acc 1
2016-11-12T19:17:58.859664: step 10608, loss 0.00126418, acc 1
2016-11-12T19:17:58.919813: step 10609, loss 0.0110382, acc 1
2016-11-12T19:17:58.978337: step 10610, loss 0.000649208, acc 1
2016-11-12T19:17:59.035510: step 10611, loss 0.00129553, acc 1
2016-11-12T19:17:59.093211: step 10612, loss 0.00033596, acc 1
2016-11-12T19:17:59.151403: step 10613, loss 0.000978932, acc 1
2016-11-12T19:17:59.208812: step 10614, loss 0.00894266, acc 1
2016-11-12T19:17:59.267642: step 10615, loss 0.0192594, acc 0.984375
2016-11-12T19:17:59.325315: step 10616, loss 1.56721e-05, acc 1
2016-11-12T19:17:59.381109: step 10617, loss 0.00176353, acc 1
2016-11-12T19:17:59.438984: step 10618, loss 0.0104544, acc 1
2016-11-12T19:17:59.498111: step 10619, loss 0.0070507, acc 1
2016-11-12T19:17:59.557333: step 10620, loss 0.000695037, acc 1
2016-11-12T19:17:59.616780: step 10621, loss 0.000476443, acc 1
2016-11-12T19:17:59.674361: step 10622, loss 0.00111527, acc 1
2016-11-12T19:17:59.737066: step 10623, loss 0.000447386, acc 1
2016-11-12T19:17:59.796499: step 10624, loss 0.0112809, acc 1
2016-11-12T19:17:59.855627: step 10625, loss 0.00190781, acc 1
2016-11-12T19:17:59.914091: step 10626, loss 0.011728, acc 0.984375
2016-11-12T19:17:59.972270: step 10627, loss 0.00830922, acc 1
2016-11-12T19:18:00.033672: step 10628, loss 0.00143654, acc 1
2016-11-12T19:18:00.092563: step 10629, loss 0.0207022, acc 0.984375
2016-11-12T19:18:00.150149: step 10630, loss 0.0293082, acc 0.984375
2016-11-12T19:18:00.208392: step 10631, loss 0.000247606, acc 1
2016-11-12T19:18:00.267924: step 10632, loss 0.000266868, acc 1
2016-11-12T19:18:00.326319: step 10633, loss 0.0258216, acc 0.984375
2016-11-12T19:18:00.384769: step 10634, loss 0.0171154, acc 0.984375
2016-11-12T19:18:00.445243: step 10635, loss 0.00204373, acc 1
2016-11-12T19:18:00.504382: step 10636, loss 0.000515732, acc 1
2016-11-12T19:18:00.563860: step 10637, loss 0.0126289, acc 0.984375
2016-11-12T19:18:00.623957: step 10638, loss 0.1891, acc 0.984375
2016-11-12T19:18:00.684281: step 10639, loss 0.0167625, acc 0.984375
2016-11-12T19:18:00.742341: step 10640, loss 0.00908925, acc 1
2016-11-12T19:18:00.799492: step 10641, loss 0.000815739, acc 1
2016-11-12T19:18:00.856858: step 10642, loss 0.00849468, acc 1
2016-11-12T19:18:00.917578: step 10643, loss 0.00223127, acc 1
2016-11-12T19:18:00.976782: step 10644, loss 0.0124401, acc 1
2016-11-12T19:18:01.038149: step 10645, loss 0.00084786, acc 1
2016-11-12T19:18:01.097084: step 10646, loss 0.000164406, acc 1
2016-11-12T19:18:01.156943: step 10647, loss 0.011956, acc 0.984375
2016-11-12T19:18:01.216626: step 10648, loss 1.74461e-05, acc 1
2016-11-12T19:18:01.272536: step 10649, loss 0.0165692, acc 0.984375
2016-11-12T19:18:01.311971: step 10650, loss 9.76346e-05, acc 1
2016-11-12T19:18:01.373030: step 10651, loss 0.000105047, acc 1
2016-11-12T19:18:01.431798: step 10652, loss 0.0222688, acc 0.984375
2016-11-12T19:18:01.489052: step 10653, loss 0.000275397, acc 1
2016-11-12T19:18:01.546227: step 10654, loss 0.0286746, acc 0.984375
2016-11-12T19:18:01.604833: step 10655, loss 0.0292151, acc 0.984375
2016-11-12T19:18:01.663139: step 10656, loss 9.29426e-05, acc 1
2016-11-12T19:18:01.723605: step 10657, loss 4.78961e-05, acc 1
2016-11-12T19:18:01.780906: step 10658, loss 0.0244326, acc 0.984375
2016-11-12T19:18:01.844094: step 10659, loss 0.000621631, acc 1
2016-11-12T19:18:01.901473: step 10660, loss 0.00223781, acc 1
2016-11-12T19:18:01.963471: step 10661, loss 0.0167705, acc 0.984375
2016-11-12T19:18:02.024917: step 10662, loss 0.00385873, acc 1
2016-11-12T19:18:02.084166: step 10663, loss 4.29435e-05, acc 1
2016-11-12T19:18:02.145394: step 10664, loss 0.00026214, acc 1
2016-11-12T19:18:02.202411: step 10665, loss 0.0180805, acc 0.984375
2016-11-12T19:18:02.260583: step 10666, loss 0.00151285, acc 1
2016-11-12T19:18:02.318020: step 10667, loss 0.00867475, acc 1
2016-11-12T19:18:02.376499: step 10668, loss 3.077e-05, acc 1
2016-11-12T19:18:02.436282: step 10669, loss 0.0206274, acc 0.984375
2016-11-12T19:18:02.494974: step 10670, loss 0.00114441, acc 1
2016-11-12T19:18:02.553337: step 10671, loss 0.00517555, acc 1
2016-11-12T19:18:02.613894: step 10672, loss 0.0104624, acc 1
2016-11-12T19:18:02.672818: step 10673, loss 0.000503648, acc 1
2016-11-12T19:18:02.733128: step 10674, loss 0.00437969, acc 1
2016-11-12T19:18:02.791578: step 10675, loss 0.0368359, acc 0.984375
2016-11-12T19:18:02.850799: step 10676, loss 0.00394526, acc 1
2016-11-12T19:18:02.908982: step 10677, loss 0.00869233, acc 1
2016-11-12T19:18:02.967737: step 10678, loss 0.000633521, acc 1
2016-11-12T19:18:03.024698: step 10679, loss 0.000271539, acc 1
2016-11-12T19:18:03.082307: step 10680, loss 0.000779607, acc 1
2016-11-12T19:18:03.139186: step 10681, loss 8.8334e-05, acc 1
2016-11-12T19:18:03.196719: step 10682, loss 0.0412307, acc 0.984375
2016-11-12T19:18:03.257144: step 10683, loss 0.0166716, acc 0.984375
2016-11-12T19:18:03.317009: step 10684, loss 0.000641275, acc 1
2016-11-12T19:18:03.374815: step 10685, loss 9.71751e-05, acc 1
2016-11-12T19:18:03.432370: step 10686, loss 1.10426e-05, acc 1
2016-11-12T19:18:03.488979: step 10687, loss 0.000199603, acc 1
2016-11-12T19:18:03.545985: step 10688, loss 0.0442679, acc 0.96875
2016-11-12T19:18:03.604048: step 10689, loss 0.000164823, acc 1
2016-11-12T19:18:03.662866: step 10690, loss 0.000355003, acc 1
2016-11-12T19:18:03.720198: step 10691, loss 0.000105166, acc 1
2016-11-12T19:18:03.776968: step 10692, loss 7.69523e-05, acc 1
2016-11-12T19:18:03.832112: step 10693, loss 0.000127011, acc 1
2016-11-12T19:18:03.888273: step 10694, loss 0.000483162, acc 1
2016-11-12T19:18:03.947804: step 10695, loss 0.000715123, acc 1
2016-11-12T19:18:04.005546: step 10696, loss 0.00054048, acc 1
2016-11-12T19:18:04.064336: step 10697, loss 3.42727e-05, acc 1
2016-11-12T19:18:04.121585: step 10698, loss 0.00606445, acc 1
2016-11-12T19:18:04.179325: step 10699, loss 0.0362353, acc 0.984375
2016-11-12T19:18:04.238508: step 10700, loss 0.00412797, acc 1

Evaluation:
2016-11-12T19:18:04.310646: step 10700, loss 4.50289, acc 0.55

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-10700

2016-11-12T19:18:04.796874: step 10701, loss 0.0102402, acc 1
2016-11-12T19:18:04.855551: step 10702, loss 0.00409209, acc 1
2016-11-12T19:18:04.912783: step 10703, loss 5.16426e-05, acc 1
2016-11-12T19:18:04.969665: step 10704, loss 0.000128233, acc 1
2016-11-12T19:18:05.028305: step 10705, loss 0.000107504, acc 1
2016-11-12T19:18:05.086229: step 10706, loss 0.00607101, acc 1
2016-11-12T19:18:05.143230: step 10707, loss 0.0216092, acc 0.984375
2016-11-12T19:18:05.201035: step 10708, loss 0.000115258, acc 1
2016-11-12T19:18:05.258254: step 10709, loss 0.00403506, acc 1
2016-11-12T19:18:05.316424: step 10710, loss 0.0125224, acc 0.984375
2016-11-12T19:18:05.374132: step 10711, loss 0.00893989, acc 1
2016-11-12T19:18:05.432762: step 10712, loss 3.96861e-05, acc 1
2016-11-12T19:18:05.489407: step 10713, loss 1.11693e-05, acc 1
2016-11-12T19:18:05.547072: step 10714, loss 0.000190043, acc 1
2016-11-12T19:18:05.604552: step 10715, loss 0.000124136, acc 1
2016-11-12T19:18:05.660014: step 10716, loss 0.000114072, acc 1
2016-11-12T19:18:05.719090: step 10717, loss 1.61751e-05, acc 1
2016-11-12T19:18:05.778329: step 10718, loss 0.00175703, acc 1
2016-11-12T19:18:05.838268: step 10719, loss 0.000986585, acc 1
2016-11-12T19:18:05.897195: step 10720, loss 0.016975, acc 0.984375
2016-11-12T19:18:05.939603: step 10721, loss 6.47281e-06, acc 1
2016-11-12T19:18:05.997014: step 10722, loss 0.00820547, acc 1
2016-11-12T19:18:06.057552: step 10723, loss 0.000538395, acc 1
2016-11-12T19:18:06.115705: step 10724, loss 0.000853912, acc 1
2016-11-12T19:18:06.173922: step 10725, loss 0.000741262, acc 1
2016-11-12T19:18:06.232654: step 10726, loss 0.00460358, acc 1
2016-11-12T19:18:06.292807: step 10727, loss 0.0048104, acc 1
2016-11-12T19:18:06.349663: step 10728, loss 0.000210855, acc 1
2016-11-12T19:18:06.407152: step 10729, loss 0.00204117, acc 1
2016-11-12T19:18:06.464657: step 10730, loss 0.0104248, acc 1
2016-11-12T19:18:06.522960: step 10731, loss 0.000217851, acc 1
2016-11-12T19:18:06.580586: step 10732, loss 0.0241987, acc 0.984375
2016-11-12T19:18:06.640470: step 10733, loss 0.00296077, acc 1
2016-11-12T19:18:06.698820: step 10734, loss 0.0168105, acc 0.984375
2016-11-12T19:18:06.761280: step 10735, loss 0.00884549, acc 1
2016-11-12T19:18:06.820846: step 10736, loss 5.78075e-05, acc 1
2016-11-12T19:18:06.877797: step 10737, loss 0.00748466, acc 1
2016-11-12T19:18:06.936383: step 10738, loss 0.00444743, acc 1
2016-11-12T19:18:06.993359: step 10739, loss 0.000747156, acc 1
2016-11-12T19:18:07.050565: step 10740, loss 0.00228009, acc 1
2016-11-12T19:18:07.108306: step 10741, loss 3.96991e-05, acc 1
2016-11-12T19:18:07.165397: step 10742, loss 0.00105201, acc 1
2016-11-12T19:18:07.223942: step 10743, loss 0.000150323, acc 1
2016-11-12T19:18:07.279606: step 10744, loss 0.0661115, acc 0.984375
2016-11-12T19:18:07.337710: step 10745, loss 0.00942689, acc 1
2016-11-12T19:18:07.396795: step 10746, loss 0.00394572, acc 1
2016-11-12T19:18:07.454543: step 10747, loss 9.88139e-05, acc 1
2016-11-12T19:18:07.511348: step 10748, loss 0.000195409, acc 1
2016-11-12T19:18:07.567257: step 10749, loss 0.0418757, acc 0.984375
2016-11-12T19:18:07.625418: step 10750, loss 5.66449e-05, acc 1
2016-11-12T19:18:07.682499: step 10751, loss 0.000374891, acc 1
2016-11-12T19:18:07.739232: step 10752, loss 0.0559972, acc 0.984375
2016-11-12T19:18:07.799727: step 10753, loss 0.000663317, acc 1
2016-11-12T19:18:07.857760: step 10754, loss 0.0416343, acc 0.96875
2016-11-12T19:18:07.916921: step 10755, loss 0.013063, acc 0.984375
2016-11-12T19:18:07.975933: step 10756, loss 0.00264222, acc 1
2016-11-12T19:18:08.032149: step 10757, loss 0.000340786, acc 1
2016-11-12T19:18:08.090070: step 10758, loss 0.00384349, acc 1
2016-11-12T19:18:08.148954: step 10759, loss 7.82958e-05, acc 1
2016-11-12T19:18:08.205481: step 10760, loss 0.0162322, acc 0.984375
2016-11-12T19:18:08.264485: step 10761, loss 0.00806592, acc 1
2016-11-12T19:18:08.322855: step 10762, loss 0.000272428, acc 1
2016-11-12T19:18:08.381235: step 10763, loss 1.23202e-05, acc 1
2016-11-12T19:18:08.437452: step 10764, loss 8.89244e-05, acc 1
2016-11-12T19:18:08.494755: step 10765, loss 0.000148456, acc 1
2016-11-12T19:18:08.552072: step 10766, loss 0.00871148, acc 1
2016-11-12T19:18:08.610015: step 10767, loss 0.0167712, acc 0.984375
2016-11-12T19:18:08.668282: step 10768, loss 0.0336014, acc 0.984375
2016-11-12T19:18:08.725296: step 10769, loss 0.000106617, acc 1
2016-11-12T19:18:08.783682: step 10770, loss 0.00939408, acc 1
2016-11-12T19:18:08.844438: step 10771, loss 0.019295, acc 0.984375
2016-11-12T19:18:08.903299: step 10772, loss 0.000116959, acc 1
2016-11-12T19:18:08.961066: step 10773, loss 0.00377604, acc 1
2016-11-12T19:18:09.019982: step 10774, loss 0.0122717, acc 0.984375
2016-11-12T19:18:09.083172: step 10775, loss 0.00808338, acc 1
2016-11-12T19:18:09.145149: step 10776, loss 0.0268644, acc 0.984375
2016-11-12T19:18:09.202961: step 10777, loss 0.000641992, acc 1
2016-11-12T19:18:09.260893: step 10778, loss 0.000178704, acc 1
2016-11-12T19:18:09.317233: step 10779, loss 0.00138319, acc 1
2016-11-12T19:18:09.377063: step 10780, loss 0.00806179, acc 1
2016-11-12T19:18:09.438468: step 10781, loss 0.000102207, acc 1
2016-11-12T19:18:09.496625: step 10782, loss 0.000108873, acc 1
2016-11-12T19:18:09.556557: step 10783, loss 0.0143187, acc 0.984375
2016-11-12T19:18:09.616278: step 10784, loss 7.8272e-05, acc 1
2016-11-12T19:18:09.674481: step 10785, loss 3.35129e-05, acc 1
2016-11-12T19:18:09.730183: step 10786, loss 0.0147821, acc 0.984375
2016-11-12T19:18:09.787010: step 10787, loss 0.00219774, acc 1
2016-11-12T19:18:09.845838: step 10788, loss 0.00895439, acc 1
2016-11-12T19:18:09.903182: step 10789, loss 0.000329135, acc 1
2016-11-12T19:18:09.961023: step 10790, loss 0.0116525, acc 1
2016-11-12T19:18:10.020658: step 10791, loss 0.119749, acc 0.984375
2016-11-12T19:18:10.060306: step 10792, loss 0.000261044, acc 1
2016-11-12T19:18:10.120409: step 10793, loss 0.00027241, acc 1
2016-11-12T19:18:10.177971: step 10794, loss 0.000267893, acc 1
2016-11-12T19:18:10.236241: step 10795, loss 0.00688704, acc 1
2016-11-12T19:18:10.296831: step 10796, loss 0.000552585, acc 1
2016-11-12T19:18:10.355734: step 10797, loss 0.00502914, acc 1
2016-11-12T19:18:10.413376: step 10798, loss 0.000504635, acc 1
2016-11-12T19:18:10.469667: step 10799, loss 0.000194948, acc 1
2016-11-12T19:18:10.526438: step 10800, loss 0.000222047, acc 1

Evaluation:
2016-11-12T19:18:10.598816: step 10800, loss 4.66473, acc 0.536

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-10800

2016-11-12T19:18:11.083809: step 10801, loss 0.00713959, acc 1
2016-11-12T19:18:11.142904: step 10802, loss 0.000116795, acc 1
2016-11-12T19:18:11.200977: step 10803, loss 0.0109539, acc 1
2016-11-12T19:18:11.258362: step 10804, loss 0.000739312, acc 1
2016-11-12T19:18:11.316887: step 10805, loss 0.000269326, acc 1
2016-11-12T19:18:11.376877: step 10806, loss 0.0153976, acc 0.984375
2016-11-12T19:18:11.436798: step 10807, loss 0.0293273, acc 0.984375
2016-11-12T19:18:11.495881: step 10808, loss 0.000169616, acc 1
2016-11-12T19:18:11.553377: step 10809, loss 0.0485841, acc 0.984375
2016-11-12T19:18:11.612519: step 10810, loss 4.23906e-05, acc 1
2016-11-12T19:18:11.671367: step 10811, loss 0.000451828, acc 1
2016-11-12T19:18:11.729380: step 10812, loss 0.0276615, acc 0.984375
2016-11-12T19:18:11.789218: step 10813, loss 0.0138104, acc 0.984375
2016-11-12T19:18:11.847970: step 10814, loss 0.0211773, acc 1
2016-11-12T19:18:11.907405: step 10815, loss 0.000578678, acc 1
2016-11-12T19:18:11.967438: step 10816, loss 0.000900503, acc 1
2016-11-12T19:18:12.025736: step 10817, loss 0.0200465, acc 0.984375
2016-11-12T19:18:12.085103: step 10818, loss 0.0107612, acc 1
2016-11-12T19:18:12.144488: step 10819, loss 0.00242677, acc 1
2016-11-12T19:18:12.204845: step 10820, loss 0.00193882, acc 1
2016-11-12T19:18:12.262290: step 10821, loss 2.12956e-05, acc 1
2016-11-12T19:18:12.318451: step 10822, loss 7.01136e-05, acc 1
2016-11-12T19:18:12.377786: step 10823, loss 0.0283276, acc 0.984375
2016-11-12T19:18:12.438494: step 10824, loss 0.0312093, acc 0.984375
2016-11-12T19:18:12.496945: step 10825, loss 0.0108907, acc 1
2016-11-12T19:18:12.555256: step 10826, loss 0.00379538, acc 1
2016-11-12T19:18:12.613468: step 10827, loss 0.0156081, acc 0.984375
2016-11-12T19:18:12.672909: step 10828, loss 0.00034414, acc 1
2016-11-12T19:18:12.731161: step 10829, loss 0.000100381, acc 1
2016-11-12T19:18:12.788754: step 10830, loss 0.0025245, acc 1
2016-11-12T19:18:12.849596: step 10831, loss 0.0145698, acc 1
2016-11-12T19:18:12.909241: step 10832, loss 0.0001898, acc 1
2016-11-12T19:18:12.968412: step 10833, loss 0.0241026, acc 0.984375
2016-11-12T19:18:13.026217: step 10834, loss 0.00436008, acc 1
2016-11-12T19:18:13.084165: step 10835, loss 0.000240769, acc 1
2016-11-12T19:18:13.140359: step 10836, loss 0.00934168, acc 1
2016-11-12T19:18:13.196729: step 10837, loss 0.00055185, acc 1
2016-11-12T19:18:13.252902: step 10838, loss 0.00191112, acc 1
2016-11-12T19:18:13.312695: step 10839, loss 0.000279271, acc 1
2016-11-12T19:18:13.370221: step 10840, loss 0.000201888, acc 1
2016-11-12T19:18:13.426036: step 10841, loss 0.0171248, acc 0.984375
2016-11-12T19:18:13.486995: step 10842, loss 0.0132815, acc 0.984375
2016-11-12T19:18:13.546891: step 10843, loss 6.3207e-05, acc 1
2016-11-12T19:18:13.603759: step 10844, loss 2.55642e-05, acc 1
2016-11-12T19:18:13.661489: step 10845, loss 8.96425e-05, acc 1
2016-11-12T19:18:13.720634: step 10846, loss 0.00749982, acc 1
2016-11-12T19:18:13.777651: step 10847, loss 0.00286857, acc 1
2016-11-12T19:18:13.837631: step 10848, loss 0.00376755, acc 1
2016-11-12T19:18:13.896027: step 10849, loss 3.27582e-05, acc 1
2016-11-12T19:18:13.954492: step 10850, loss 0.0190198, acc 0.984375
2016-11-12T19:18:14.012269: step 10851, loss 0.000163324, acc 1
2016-11-12T19:18:14.071068: step 10852, loss 0.00028177, acc 1
2016-11-12T19:18:14.131272: step 10853, loss 0.0176011, acc 0.984375
2016-11-12T19:18:14.189783: step 10854, loss 7.82073e-05, acc 1
2016-11-12T19:18:14.249586: step 10855, loss 0.000211248, acc 1
2016-11-12T19:18:14.308955: step 10856, loss 0.000477825, acc 1
2016-11-12T19:18:14.367447: step 10857, loss 0.000215441, acc 1
2016-11-12T19:18:14.425612: step 10858, loss 0.00316709, acc 1
2016-11-12T19:18:14.483506: step 10859, loss 0.000664329, acc 1
2016-11-12T19:18:14.544597: step 10860, loss 0.015075, acc 0.984375
2016-11-12T19:18:14.602599: step 10861, loss 0.0107828, acc 1
2016-11-12T19:18:14.661454: step 10862, loss 2.66613e-05, acc 1
2016-11-12T19:18:14.699476: step 10863, loss 0.00035623, acc 1
2016-11-12T19:18:14.760868: step 10864, loss 0.0315558, acc 0.96875
2016-11-12T19:18:14.820270: step 10865, loss 0.000450227, acc 1
2016-11-12T19:18:14.881747: step 10866, loss 0.0060335, acc 1
2016-11-12T19:18:14.940602: step 10867, loss 0.00028417, acc 1
2016-11-12T19:18:14.999076: step 10868, loss 0.00297393, acc 1
2016-11-12T19:18:15.056411: step 10869, loss 5.36498e-05, acc 1
2016-11-12T19:18:15.116456: step 10870, loss 0.000404062, acc 1
2016-11-12T19:18:15.174729: step 10871, loss 0.00770439, acc 1
2016-11-12T19:18:15.234811: step 10872, loss 0.000433138, acc 1
2016-11-12T19:18:15.292572: step 10873, loss 3.2588e-05, acc 1
2016-11-12T19:18:15.348877: step 10874, loss 0.014799, acc 0.984375
2016-11-12T19:18:15.410908: step 10875, loss 0.000302549, acc 1
2016-11-12T19:18:15.469427: step 10876, loss 0.000111805, acc 1
2016-11-12T19:18:15.527637: step 10877, loss 0.00124037, acc 1
2016-11-12T19:18:15.586479: step 10878, loss 0.00325029, acc 1
2016-11-12T19:18:15.645563: step 10879, loss 0.00283402, acc 1
2016-11-12T19:18:15.702844: step 10880, loss 0.0120388, acc 0.984375
2016-11-12T19:18:15.760985: step 10881, loss 0.0310952, acc 0.96875
2016-11-12T19:18:15.820400: step 10882, loss 0.000124567, acc 1
2016-11-12T19:18:15.877670: step 10883, loss 0.00719811, acc 1
2016-11-12T19:18:15.936858: step 10884, loss 0.00922271, acc 1
2016-11-12T19:18:15.996844: step 10885, loss 0.0297056, acc 0.984375
2016-11-12T19:18:16.057019: step 10886, loss 0.0135351, acc 0.984375
2016-11-12T19:18:16.116568: step 10887, loss 0.0129864, acc 1
2016-11-12T19:18:16.173726: step 10888, loss 0.000224716, acc 1
2016-11-12T19:18:16.231344: step 10889, loss 0.0199377, acc 0.984375
2016-11-12T19:18:16.291402: step 10890, loss 8.57616e-05, acc 1
2016-11-12T19:18:16.349251: step 10891, loss 0.00698777, acc 1
2016-11-12T19:18:16.409575: step 10892, loss 0.00035581, acc 1
2016-11-12T19:18:16.467421: step 10893, loss 0.00978073, acc 1
2016-11-12T19:18:16.525037: step 10894, loss 3.30797e-06, acc 1
2016-11-12T19:18:16.580763: step 10895, loss 0.0105383, acc 1
2016-11-12T19:18:16.637445: step 10896, loss 0.000184247, acc 1
2016-11-12T19:18:16.696619: step 10897, loss 0.00453396, acc 1
2016-11-12T19:18:16.760192: step 10898, loss 0.0209878, acc 0.984375
2016-11-12T19:18:16.818010: step 10899, loss 8.44639e-05, acc 1
2016-11-12T19:18:16.874744: step 10900, loss 9.97662e-05, acc 1

Evaluation:
2016-11-12T19:18:16.947245: step 10900, loss 4.52281, acc 0.558

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-10900

2016-11-12T19:18:17.431603: step 10901, loss 0.000317126, acc 1
2016-11-12T19:18:17.491754: step 10902, loss 0.000326154, acc 1
2016-11-12T19:18:17.549491: step 10903, loss 0.0155009, acc 0.984375
2016-11-12T19:18:17.608101: step 10904, loss 0.000117178, acc 1
2016-11-12T19:18:17.666646: step 10905, loss 0.0220688, acc 0.984375
2016-11-12T19:18:17.725167: step 10906, loss 4.16785e-05, acc 1
2016-11-12T19:18:17.782905: step 10907, loss 0.000204005, acc 1
2016-11-12T19:18:17.842912: step 10908, loss 0.0127598, acc 0.984375
2016-11-12T19:18:17.902904: step 10909, loss 0.00350683, acc 1
2016-11-12T19:18:17.962414: step 10910, loss 0.00650999, acc 1
2016-11-12T19:18:18.019666: step 10911, loss 0.0114502, acc 1
2016-11-12T19:18:18.077596: step 10912, loss 0.000454438, acc 1
2016-11-12T19:18:18.135990: step 10913, loss 0.000429662, acc 1
2016-11-12T19:18:18.193365: step 10914, loss 0.00263364, acc 1
2016-11-12T19:18:18.252542: step 10915, loss 0.0185373, acc 0.984375
2016-11-12T19:18:18.312160: step 10916, loss 0.000833171, acc 1
2016-11-12T19:18:18.370091: step 10917, loss 3.58912e-05, acc 1
2016-11-12T19:18:18.428108: step 10918, loss 0.000183552, acc 1
2016-11-12T19:18:18.485087: step 10919, loss 9.72094e-05, acc 1
2016-11-12T19:18:18.544483: step 10920, loss 0.00036742, acc 1
2016-11-12T19:18:18.602370: step 10921, loss 4.40196e-05, acc 1
2016-11-12T19:18:18.657621: step 10922, loss 0.0125522, acc 0.984375
2016-11-12T19:18:18.717729: step 10923, loss 0.00768972, acc 1
2016-11-12T19:18:18.777159: step 10924, loss 0.00197189, acc 1
2016-11-12T19:18:18.836371: step 10925, loss 0.000224956, acc 1
2016-11-12T19:18:18.893249: step 10926, loss 0.00414496, acc 1
2016-11-12T19:18:18.951443: step 10927, loss 0.0150753, acc 1
2016-11-12T19:18:19.012528: step 10928, loss 0.00137403, acc 1
2016-11-12T19:18:19.072835: step 10929, loss 9.99412e-05, acc 1
2016-11-12T19:18:19.130927: step 10930, loss 0.000203808, acc 1
2016-11-12T19:18:19.189195: step 10931, loss 0.000342498, acc 1
2016-11-12T19:18:19.246092: step 10932, loss 0.00576224, acc 1
2016-11-12T19:18:19.308855: step 10933, loss 0.0282572, acc 0.984375
2016-11-12T19:18:19.348027: step 10934, loss 1.43051e-07, acc 1
2016-11-12T19:18:19.403935: step 10935, loss 0.011131, acc 0.984375
2016-11-12T19:18:19.464568: step 10936, loss 0.000519714, acc 1
2016-11-12T19:18:19.522087: step 10937, loss 0.0279627, acc 0.984375
2016-11-12T19:18:19.580355: step 10938, loss 0.00129921, acc 1
2016-11-12T19:18:19.638299: step 10939, loss 0.0202826, acc 0.984375
2016-11-12T19:18:19.698007: step 10940, loss 0.000197753, acc 1
2016-11-12T19:18:19.756913: step 10941, loss 0.00300839, acc 1
2016-11-12T19:18:19.815106: step 10942, loss 0.00107349, acc 1
2016-11-12T19:18:19.872814: step 10943, loss 0.000831854, acc 1
2016-11-12T19:18:19.932939: step 10944, loss 0.00767283, acc 1
2016-11-12T19:18:19.991073: step 10945, loss 0.0112877, acc 1
2016-11-12T19:18:20.048946: step 10946, loss 0.0165065, acc 0.984375
2016-11-12T19:18:20.108857: step 10947, loss 0.017612, acc 0.984375
2016-11-12T19:18:20.167767: step 10948, loss 0.00464526, acc 1
2016-11-12T19:18:20.225476: step 10949, loss 6.25981e-05, acc 1
2016-11-12T19:18:20.285004: step 10950, loss 0.0364365, acc 0.984375
2016-11-12T19:18:20.344393: step 10951, loss 0.00677738, acc 1
2016-11-12T19:18:20.404996: step 10952, loss 0.000399368, acc 1
2016-11-12T19:18:20.462399: step 10953, loss 0.012856, acc 0.984375
2016-11-12T19:18:20.519496: step 10954, loss 0.00936245, acc 1
2016-11-12T19:18:20.577142: step 10955, loss 0.000434581, acc 1
2016-11-12T19:18:20.634979: step 10956, loss 0.000504483, acc 1
2016-11-12T19:18:20.691931: step 10957, loss 0.000716186, acc 1
2016-11-12T19:18:20.749716: step 10958, loss 0.000747652, acc 1
2016-11-12T19:18:20.808951: step 10959, loss 0.00747652, acc 1
2016-11-12T19:18:20.868037: step 10960, loss 0.000249447, acc 1
2016-11-12T19:18:20.926101: step 10961, loss 0.000290945, acc 1
2016-11-12T19:18:20.984806: step 10962, loss 0.000341253, acc 1
2016-11-12T19:18:21.044270: step 10963, loss 1.35391e-05, acc 1
2016-11-12T19:18:21.100402: step 10964, loss 0.00111109, acc 1
2016-11-12T19:18:21.160455: step 10965, loss 0.000711692, acc 1
2016-11-12T19:18:21.220714: step 10966, loss 0.000276312, acc 1
2016-11-12T19:18:21.280699: step 10967, loss 0.000199566, acc 1
2016-11-12T19:18:21.339804: step 10968, loss 1.93462e-05, acc 1
2016-11-12T19:18:21.397889: step 10969, loss 0.000170087, acc 1
2016-11-12T19:18:21.456909: step 10970, loss 0.000812254, acc 1
2016-11-12T19:18:21.514452: step 10971, loss 0.0976513, acc 0.984375
2016-11-12T19:18:21.572785: step 10972, loss 0.00102637, acc 1
2016-11-12T19:18:21.632303: step 10973, loss 0.0365837, acc 0.96875
2016-11-12T19:18:21.692884: step 10974, loss 0.00262477, acc 1
2016-11-12T19:18:21.751165: step 10975, loss 0.000166135, acc 1
2016-11-12T19:18:21.808664: step 10976, loss 0.000719159, acc 1
2016-11-12T19:18:21.867807: step 10977, loss 0.00389921, acc 1
2016-11-12T19:18:21.928823: step 10978, loss 0.00560722, acc 1
2016-11-12T19:18:21.987917: step 10979, loss 9.52382e-05, acc 1
2016-11-12T19:18:22.044485: step 10980, loss 0.00923341, acc 1
2016-11-12T19:18:22.104019: step 10981, loss 0.000268433, acc 1
2016-11-12T19:18:22.162146: step 10982, loss 0.00676501, acc 1
2016-11-12T19:18:22.221535: step 10983, loss 3.83862e-06, acc 1
2016-11-12T19:18:22.277731: step 10984, loss 0.000364944, acc 1
2016-11-12T19:18:22.336479: step 10985, loss 0.00117093, acc 1
2016-11-12T19:18:22.396656: step 10986, loss 0.0120756, acc 0.984375
2016-11-12T19:18:22.458123: step 10987, loss 0.000112714, acc 1
2016-11-12T19:18:22.517021: step 10988, loss 0.000362719, acc 1
2016-11-12T19:18:22.576610: step 10989, loss 0.0187221, acc 1
2016-11-12T19:18:22.636526: step 10990, loss 0.000541703, acc 1
2016-11-12T19:18:22.693240: step 10991, loss 0.0258836, acc 0.96875
2016-11-12T19:18:22.753334: step 10992, loss 0.000132271, acc 1
2016-11-12T19:18:22.812870: step 10993, loss 0.000987684, acc 1
2016-11-12T19:18:22.873335: step 10994, loss 0.000310926, acc 1
2016-11-12T19:18:22.932264: step 10995, loss 0.00767367, acc 1
2016-11-12T19:18:22.992967: step 10996, loss 0.000141511, acc 1
2016-11-12T19:18:23.052870: step 10997, loss 0.00193051, acc 1
2016-11-12T19:18:23.108928: step 10998, loss 0.00277798, acc 1
2016-11-12T19:18:23.166026: step 10999, loss 0.000477589, acc 1
2016-11-12T19:18:23.225066: step 11000, loss 0.0142964, acc 0.984375

Evaluation:
2016-11-12T19:18:23.301963: step 11000, loss 4.53659, acc 0.55

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-11000

2016-11-12T19:18:23.786316: step 11001, loss 0.000164899, acc 1
2016-11-12T19:18:23.843507: step 11002, loss 0.00199474, acc 1
2016-11-12T19:18:23.901786: step 11003, loss 3.85826e-05, acc 1
2016-11-12T19:18:23.959204: step 11004, loss 0.0205602, acc 1
2016-11-12T19:18:23.999601: step 11005, loss 0.000149517, acc 1
2016-11-12T19:18:24.059048: step 11006, loss 0.000142087, acc 1
2016-11-12T19:18:24.116732: step 11007, loss 3.83413e-05, acc 1
2016-11-12T19:18:24.172840: step 11008, loss 0.00946161, acc 1
2016-11-12T19:18:24.232733: step 11009, loss 0.00500427, acc 1
2016-11-12T19:18:24.290246: step 11010, loss 0.00615924, acc 1
2016-11-12T19:18:24.348633: step 11011, loss 0.000493993, acc 1
2016-11-12T19:18:24.407130: step 11012, loss 4.87604e-05, acc 1
2016-11-12T19:18:24.466612: step 11013, loss 0.011549, acc 0.984375
2016-11-12T19:18:24.526566: step 11014, loss 0.000131632, acc 1
2016-11-12T19:18:24.584991: step 11015, loss 0.0104455, acc 1
2016-11-12T19:18:24.645206: step 11016, loss 0.000700732, acc 1
2016-11-12T19:18:24.702607: step 11017, loss 0.00613995, acc 1
2016-11-12T19:18:24.759730: step 11018, loss 0.00583942, acc 1
2016-11-12T19:18:24.818949: step 11019, loss 5.48956e-05, acc 1
2016-11-12T19:18:24.875447: step 11020, loss 0.000126884, acc 1
2016-11-12T19:18:24.931998: step 11021, loss 0.00184984, acc 1
2016-11-12T19:18:24.988982: step 11022, loss 0.000230017, acc 1
2016-11-12T19:18:25.048519: step 11023, loss 0.00120461, acc 1
2016-11-12T19:18:25.108771: step 11024, loss 0.00287407, acc 1
2016-11-12T19:18:25.169185: step 11025, loss 3.34402e-05, acc 1
2016-11-12T19:18:25.225144: step 11026, loss 0.0103989, acc 1
2016-11-12T19:18:25.284280: step 11027, loss 0.000130221, acc 1
2016-11-12T19:18:25.341061: step 11028, loss 0.000376124, acc 1
2016-11-12T19:18:25.398826: step 11029, loss 0.000102526, acc 1
2016-11-12T19:18:25.455831: step 11030, loss 0.00146305, acc 1
2016-11-12T19:18:25.512991: step 11031, loss 0.000155117, acc 1
2016-11-12T19:18:25.571844: step 11032, loss 8.92579e-05, acc 1
2016-11-12T19:18:25.629159: step 11033, loss 4.11214e-05, acc 1
2016-11-12T19:18:25.688068: step 11034, loss 0.000203725, acc 1
2016-11-12T19:18:25.746480: step 11035, loss 0.00426848, acc 1
2016-11-12T19:18:25.804778: step 11036, loss 0.00206045, acc 1
2016-11-12T19:18:25.864680: step 11037, loss 3.78493e-05, acc 1
2016-11-12T19:18:25.922773: step 11038, loss 0.0012859, acc 1
2016-11-12T19:18:25.982309: step 11039, loss 0.0131233, acc 0.984375
2016-11-12T19:18:26.039550: step 11040, loss 0.000771108, acc 1
2016-11-12T19:18:26.097339: step 11041, loss 0.000380757, acc 1
2016-11-12T19:18:26.156957: step 11042, loss 1.23889e-05, acc 1
2016-11-12T19:18:26.212935: step 11043, loss 5.74159e-05, acc 1
2016-11-12T19:18:26.273161: step 11044, loss 6.63755e-05, acc 1
2016-11-12T19:18:26.332786: step 11045, loss 0.0220768, acc 0.984375
2016-11-12T19:18:26.393776: step 11046, loss 0.0381604, acc 0.96875
2016-11-12T19:18:26.452295: step 11047, loss 0.00408587, acc 1
2016-11-12T19:18:26.512194: step 11048, loss 0.000236244, acc 1
2016-11-12T19:18:26.568514: step 11049, loss 0.00260184, acc 1
2016-11-12T19:18:26.626518: step 11050, loss 0.000522499, acc 1
2016-11-12T19:18:26.683917: step 11051, loss 0.0359047, acc 0.96875
2016-11-12T19:18:26.743943: step 11052, loss 0.00230612, acc 1
2016-11-12T19:18:26.803209: step 11053, loss 0.00156209, acc 1
2016-11-12T19:18:26.864275: step 11054, loss 4.77425e-05, acc 1
2016-11-12T19:18:26.921237: step 11055, loss 0.000423656, acc 1
2016-11-12T19:18:26.980752: step 11056, loss 0.000396758, acc 1
2016-11-12T19:18:27.038218: step 11057, loss 3.36717e-05, acc 1
2016-11-12T19:18:27.096734: step 11058, loss 9.86071e-05, acc 1
2016-11-12T19:18:27.154536: step 11059, loss 0.00603548, acc 1
2016-11-12T19:18:27.215886: step 11060, loss 0.0139554, acc 0.984375
2016-11-12T19:18:27.273642: step 11061, loss 0.000454203, acc 1
2016-11-12T19:18:27.334774: step 11062, loss 0.00923016, acc 1
2016-11-12T19:18:27.392402: step 11063, loss 0.0227471, acc 0.984375
2016-11-12T19:18:27.449230: step 11064, loss 0.000867882, acc 1
2016-11-12T19:18:27.507905: step 11065, loss 1.26188e-05, acc 1
2016-11-12T19:18:27.564810: step 11066, loss 0.0134493, acc 1
2016-11-12T19:18:27.621997: step 11067, loss 8.77762e-06, acc 1
2016-11-12T19:18:27.677246: step 11068, loss 4.52202e-05, acc 1
2016-11-12T19:18:27.735451: step 11069, loss 0.0143428, acc 0.984375
2016-11-12T19:18:27.793272: step 11070, loss 0.00742804, acc 1
2016-11-12T19:18:27.851820: step 11071, loss 0.00648752, acc 1
2016-11-12T19:18:27.909228: step 11072, loss 0.00648098, acc 1
2016-11-12T19:18:27.968410: step 11073, loss 0.00428914, acc 1
2016-11-12T19:18:28.028733: step 11074, loss 0.00253986, acc 1
2016-11-12T19:18:28.087945: step 11075, loss 0.000322828, acc 1
2016-11-12T19:18:28.127476: step 11076, loss 0.0418188, acc 0.95
2016-11-12T19:18:28.186773: step 11077, loss 0.011832, acc 0.984375
2016-11-12T19:18:28.246078: step 11078, loss 0.00495544, acc 1
2016-11-12T19:18:28.303854: step 11079, loss 0.00209613, acc 1
2016-11-12T19:18:28.363422: step 11080, loss 9.13695e-06, acc 1
2016-11-12T19:18:28.420064: step 11081, loss 0.0223365, acc 0.984375
2016-11-12T19:18:28.479374: step 11082, loss 0.000200347, acc 1
2016-11-12T19:18:28.538082: step 11083, loss 0.000356252, acc 1
2016-11-12T19:18:28.596267: step 11084, loss 0.000380144, acc 1
2016-11-12T19:18:28.656733: step 11085, loss 0.00626787, acc 1
2016-11-12T19:18:28.717092: step 11086, loss 0.00879614, acc 1
2016-11-12T19:18:28.776470: step 11087, loss 0.0123817, acc 0.984375
2016-11-12T19:18:28.834740: step 11088, loss 0.00029433, acc 1
2016-11-12T19:18:28.892837: step 11089, loss 4.24776e-05, acc 1
2016-11-12T19:18:28.952727: step 11090, loss 0.0015413, acc 1
2016-11-12T19:18:29.012443: step 11091, loss 0.122546, acc 0.96875
2016-11-12T19:18:29.072668: step 11092, loss 0.00530297, acc 1
2016-11-12T19:18:29.132714: step 11093, loss 6.0277e-05, acc 1
2016-11-12T19:18:29.192859: step 11094, loss 0.000313164, acc 1
2016-11-12T19:18:29.252899: step 11095, loss 0.0063518, acc 1
2016-11-12T19:18:29.309819: step 11096, loss 0.00011828, acc 1
2016-11-12T19:18:29.371158: step 11097, loss 1.37663e-05, acc 1
2016-11-12T19:18:29.428776: step 11098, loss 0.0011715, acc 1
2016-11-12T19:18:29.485612: step 11099, loss 0.00773583, acc 1
2016-11-12T19:18:29.545586: step 11100, loss 0.000422221, acc 1

Evaluation:
2016-11-12T19:18:29.617445: step 11100, loss 4.57764, acc 0.55

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-11100

2016-11-12T19:18:30.108071: step 11101, loss 3.31008e-05, acc 1
2016-11-12T19:18:30.163811: step 11102, loss 0.0135584, acc 0.984375
2016-11-12T19:18:30.221211: step 11103, loss 3.00811e-06, acc 1
2016-11-12T19:18:30.279925: step 11104, loss 0.000338663, acc 1
2016-11-12T19:18:30.338054: step 11105, loss 0.0010005, acc 1
2016-11-12T19:18:30.396436: step 11106, loss 0.000173311, acc 1
2016-11-12T19:18:30.453380: step 11107, loss 0.000217313, acc 1
2016-11-12T19:18:30.513037: step 11108, loss 0.00113894, acc 1
2016-11-12T19:18:30.570332: step 11109, loss 0.000213833, acc 1
2016-11-12T19:18:30.629208: step 11110, loss 7.92755e-05, acc 1
2016-11-12T19:18:30.685921: step 11111, loss 0.000583909, acc 1
2016-11-12T19:18:30.743418: step 11112, loss 0.000131442, acc 1
2016-11-12T19:18:30.801039: step 11113, loss 0.000145879, acc 1
2016-11-12T19:18:30.860567: step 11114, loss 0.000408328, acc 1
2016-11-12T19:18:30.919816: step 11115, loss 0.000551841, acc 1
2016-11-12T19:18:30.979704: step 11116, loss 0.000103967, acc 1
2016-11-12T19:18:31.038958: step 11117, loss 0.0018925, acc 1
2016-11-12T19:18:31.097363: step 11118, loss 0.0180949, acc 0.984375
2016-11-12T19:18:31.155328: step 11119, loss 0.0128302, acc 1
2016-11-12T19:18:31.213048: step 11120, loss 0.000108636, acc 1
2016-11-12T19:18:31.272346: step 11121, loss 0.0140616, acc 0.984375
2016-11-12T19:18:31.331026: step 11122, loss 0.00175756, acc 1
2016-11-12T19:18:31.392495: step 11123, loss 0.000410166, acc 1
2016-11-12T19:18:31.449816: step 11124, loss 0.000573379, acc 1
2016-11-12T19:18:31.508460: step 11125, loss 0.00451811, acc 1
2016-11-12T19:18:31.567507: step 11126, loss 0.0724484, acc 0.96875
2016-11-12T19:18:31.625449: step 11127, loss 0.000365558, acc 1
2016-11-12T19:18:31.688733: step 11128, loss 0.00070775, acc 1
2016-11-12T19:18:31.745271: step 11129, loss 7.34263e-05, acc 1
2016-11-12T19:18:31.802801: step 11130, loss 0.0110905, acc 0.984375
2016-11-12T19:18:31.862236: step 11131, loss 0.00620362, acc 1
2016-11-12T19:18:31.920345: step 11132, loss 0.00420774, acc 1
2016-11-12T19:18:31.980598: step 11133, loss 0.00733182, acc 1
2016-11-12T19:18:32.039053: step 11134, loss 0.0120677, acc 0.984375
2016-11-12T19:18:32.100558: step 11135, loss 0.000153042, acc 1
2016-11-12T19:18:32.158083: step 11136, loss 0.000296772, acc 1
2016-11-12T19:18:32.215006: step 11137, loss 0.0259391, acc 0.96875
2016-11-12T19:18:32.276697: step 11138, loss 0.0215655, acc 0.984375
2016-11-12T19:18:32.336820: step 11139, loss 6.20164e-05, acc 1
2016-11-12T19:18:32.396428: step 11140, loss 0.0401822, acc 0.984375
2016-11-12T19:18:32.455252: step 11141, loss 0.00284891, acc 1
2016-11-12T19:18:32.516065: step 11142, loss 9.62913e-05, acc 1
2016-11-12T19:18:32.573695: step 11143, loss 0.0338997, acc 0.96875
2016-11-12T19:18:32.635576: step 11144, loss 0.000906418, acc 1
2016-11-12T19:18:32.693012: step 11145, loss 0.000520215, acc 1
2016-11-12T19:18:32.752418: step 11146, loss 2.85086e-05, acc 1
2016-11-12T19:18:32.789929: step 11147, loss 0.000296283, acc 1
2016-11-12T19:18:32.850416: step 11148, loss 0.00050935, acc 1
2016-11-12T19:18:32.907327: step 11149, loss 0.000758702, acc 1
2016-11-12T19:18:32.965080: step 11150, loss 0.000477072, acc 1
2016-11-12T19:18:33.021906: step 11151, loss 0.0216723, acc 0.984375
2016-11-12T19:18:33.080968: step 11152, loss 0.000432954, acc 1
2016-11-12T19:18:33.137529: step 11153, loss 0.0661362, acc 0.984375
2016-11-12T19:18:33.195246: step 11154, loss 0.0058883, acc 1
2016-11-12T19:18:33.256269: step 11155, loss 0.000218426, acc 1
2016-11-12T19:18:33.318188: step 11156, loss 0.000153947, acc 1
2016-11-12T19:18:33.377779: step 11157, loss 1.2534e-05, acc 1
2016-11-12T19:18:33.436926: step 11158, loss 7.43916e-05, acc 1
2016-11-12T19:18:33.492999: step 11159, loss 0.000156224, acc 1
2016-11-12T19:18:33.549915: step 11160, loss 0.0180456, acc 0.984375
2016-11-12T19:18:33.608711: step 11161, loss 0.00164043, acc 1
2016-11-12T19:18:33.668592: step 11162, loss 0.000175323, acc 1
2016-11-12T19:18:33.726132: step 11163, loss 0.000341481, acc 1
2016-11-12T19:18:33.783061: step 11164, loss 0.000264964, acc 1
2016-11-12T19:18:33.842696: step 11165, loss 0.000588151, acc 1
2016-11-12T19:18:33.900279: step 11166, loss 3.72864e-05, acc 1
2016-11-12T19:18:33.958868: step 11167, loss 2.92669e-05, acc 1
2016-11-12T19:18:34.016681: step 11168, loss 0.000579448, acc 1
2016-11-12T19:18:34.075723: step 11169, loss 0.000229524, acc 1
2016-11-12T19:18:34.132156: step 11170, loss 9.85675e-05, acc 1
2016-11-12T19:18:34.193993: step 11171, loss 0.0358847, acc 0.96875
2016-11-12T19:18:34.252983: step 11172, loss 0.0075098, acc 1
2016-11-12T19:18:34.312700: step 11173, loss 0.000338929, acc 1
2016-11-12T19:18:34.373349: step 11174, loss 0.00100924, acc 1
2016-11-12T19:18:34.433262: step 11175, loss 0.0172513, acc 0.984375
2016-11-12T19:18:34.490332: step 11176, loss 0.0110899, acc 1
2016-11-12T19:18:34.548563: step 11177, loss 0.0118547, acc 0.984375
2016-11-12T19:18:34.605091: step 11178, loss 0.00061977, acc 1
2016-11-12T19:18:34.661719: step 11179, loss 4.72267e-05, acc 1
2016-11-12T19:18:34.718465: step 11180, loss 0.000950959, acc 1
2016-11-12T19:18:34.775707: step 11181, loss 0.013838, acc 1
2016-11-12T19:18:34.838243: step 11182, loss 0.0198642, acc 0.984375
2016-11-12T19:18:34.896213: step 11183, loss 0.000242738, acc 1
2016-11-12T19:18:34.954748: step 11184, loss 9.41707e-05, acc 1
2016-11-12T19:18:35.011301: step 11185, loss 0.0125677, acc 0.984375
2016-11-12T19:18:35.069470: step 11186, loss 6.79087e-06, acc 1
2016-11-12T19:18:35.125658: step 11187, loss 0.000356407, acc 1
2016-11-12T19:18:35.184774: step 11188, loss 0.000209793, acc 1
2016-11-12T19:18:35.244129: step 11189, loss 4.59984e-05, acc 1
2016-11-12T19:18:35.300697: step 11190, loss 0.00320282, acc 1
2016-11-12T19:18:35.360735: step 11191, loss 7.95237e-05, acc 1
2016-11-12T19:18:35.420077: step 11192, loss 0.00775733, acc 1
2016-11-12T19:18:35.479934: step 11193, loss 0.000368712, acc 1
2016-11-12T19:18:35.537256: step 11194, loss 0.000174956, acc 1
2016-11-12T19:18:35.594849: step 11195, loss 0.00137336, acc 1
2016-11-12T19:18:35.652478: step 11196, loss 0.0104011, acc 1
2016-11-12T19:18:35.711318: step 11197, loss 0.000669131, acc 1
2016-11-12T19:18:35.769780: step 11198, loss 0.0105513, acc 1
2016-11-12T19:18:35.833681: step 11199, loss 0.00387457, acc 1
2016-11-12T19:18:35.892514: step 11200, loss 0.0151363, acc 0.984375

Evaluation:
2016-11-12T19:18:35.965232: step 11200, loss 4.76726, acc 0.55

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-11200

2016-11-12T19:18:36.450469: step 11201, loss 0.0198882, acc 0.984375
2016-11-12T19:18:36.508406: step 11202, loss 1.94856e-05, acc 1
2016-11-12T19:18:36.567798: step 11203, loss 0.0555408, acc 0.96875
2016-11-12T19:18:36.628960: step 11204, loss 0.0110232, acc 0.984375
2016-11-12T19:18:36.688748: step 11205, loss 0.0113959, acc 0.984375
2016-11-12T19:18:36.748863: step 11206, loss 0.0102408, acc 1
2016-11-12T19:18:36.807551: step 11207, loss 0.00011686, acc 1
2016-11-12T19:18:36.863681: step 11208, loss 0.00112825, acc 1
2016-11-12T19:18:36.923053: step 11209, loss 0.000123188, acc 1
2016-11-12T19:18:36.984024: step 11210, loss 0.231244, acc 0.984375
2016-11-12T19:18:37.043613: step 11211, loss 0.0016484, acc 1
2016-11-12T19:18:37.104599: step 11212, loss 0.00139845, acc 1
2016-11-12T19:18:37.164853: step 11213, loss 0.00150027, acc 1
2016-11-12T19:18:37.228935: step 11214, loss 0.000604354, acc 1
2016-11-12T19:18:37.292649: step 11215, loss 0.0156278, acc 1
2016-11-12T19:18:37.352852: step 11216, loss 0.00600502, acc 1
2016-11-12T19:18:37.411565: step 11217, loss 0.0142855, acc 0.984375
2016-11-12T19:18:37.450782: step 11218, loss 0.000670375, acc 1
2016-11-12T19:18:37.510482: step 11219, loss 8.62872e-05, acc 1
2016-11-12T19:18:37.571860: step 11220, loss 0.000303332, acc 1
2016-11-12T19:18:37.633388: step 11221, loss 0.00964056, acc 1
2016-11-12T19:18:37.690533: step 11222, loss 0.000341685, acc 1
2016-11-12T19:18:37.747848: step 11223, loss 0.000169185, acc 1
2016-11-12T19:18:37.804558: step 11224, loss 0.00883259, acc 1
2016-11-12T19:18:37.864342: step 11225, loss 0.000365955, acc 1
2016-11-12T19:18:37.925338: step 11226, loss 0.000472879, acc 1
2016-11-12T19:18:37.989672: step 11227, loss 5.6366e-05, acc 1
2016-11-12T19:18:38.046418: step 11228, loss 0.0104404, acc 1
2016-11-12T19:18:38.103865: step 11229, loss 0.00472492, acc 1
2016-11-12T19:18:38.162301: step 11230, loss 0.0166873, acc 0.984375
2016-11-12T19:18:38.220504: step 11231, loss 6.8732e-05, acc 1
2016-11-12T19:18:38.276923: step 11232, loss 0.0125676, acc 0.984375
2016-11-12T19:18:38.337757: step 11233, loss 0.000180326, acc 1
2016-11-12T19:18:38.394529: step 11234, loss 0.00649706, acc 1
2016-11-12T19:18:38.453236: step 11235, loss 0.0287449, acc 0.984375
2016-11-12T19:18:38.511500: step 11236, loss 0.000182477, acc 1
2016-11-12T19:18:38.571367: step 11237, loss 0.0116788, acc 1
2016-11-12T19:18:38.628946: step 11238, loss 0.00191766, acc 1
2016-11-12T19:18:38.686716: step 11239, loss 0.0108427, acc 1
2016-11-12T19:18:38.744844: step 11240, loss 0.00118871, acc 1
2016-11-12T19:18:38.805531: step 11241, loss 0.000712194, acc 1
2016-11-12T19:18:38.862860: step 11242, loss 0.000267874, acc 1
2016-11-12T19:18:38.920609: step 11243, loss 0.000223702, acc 1
2016-11-12T19:18:38.978852: step 11244, loss 0.0197375, acc 0.984375
2016-11-12T19:18:39.037188: step 11245, loss 7.02714e-06, acc 1
2016-11-12T19:18:39.092962: step 11246, loss 0.00187353, acc 1
2016-11-12T19:18:39.151972: step 11247, loss 0.00189574, acc 1
2016-11-12T19:18:39.212960: step 11248, loss 0.016218, acc 0.984375
2016-11-12T19:18:39.271600: step 11249, loss 0.00296153, acc 1
2016-11-12T19:18:39.329947: step 11250, loss 0.0316355, acc 0.984375
2016-11-12T19:18:39.386436: step 11251, loss 0.000342128, acc 1
2016-11-12T19:18:39.445569: step 11252, loss 0.0019049, acc 1
2016-11-12T19:18:39.502888: step 11253, loss 0.00216835, acc 1
2016-11-12T19:18:39.561791: step 11254, loss 7.81336e-05, acc 1
2016-11-12T19:18:39.619944: step 11255, loss 0.00642157, acc 1
2016-11-12T19:18:39.677215: step 11256, loss 0.000252499, acc 1
2016-11-12T19:18:39.737181: step 11257, loss 0.000495328, acc 1
2016-11-12T19:18:39.796329: step 11258, loss 0.000870685, acc 1
2016-11-12T19:18:39.853343: step 11259, loss 0.0431287, acc 0.984375
2016-11-12T19:18:39.911752: step 11260, loss 0.00179479, acc 1
2016-11-12T19:18:39.969418: step 11261, loss 0.00013177, acc 1
2016-11-12T19:18:40.027378: step 11262, loss 8.49003e-05, acc 1
2016-11-12T19:18:40.085098: step 11263, loss 0.000137808, acc 1
2016-11-12T19:18:40.147691: step 11264, loss 0.000261303, acc 1
2016-11-12T19:18:40.205028: step 11265, loss 0.0715945, acc 0.96875
2016-11-12T19:18:40.264800: step 11266, loss 0.00370852, acc 1
2016-11-12T19:18:40.322159: step 11267, loss 0.000118679, acc 1
2016-11-12T19:18:40.379145: step 11268, loss 0.000402913, acc 1
2016-11-12T19:18:40.437566: step 11269, loss 9.10019e-06, acc 1
2016-11-12T19:18:40.493784: step 11270, loss 0.000514637, acc 1
2016-11-12T19:18:40.552210: step 11271, loss 0.0101519, acc 1
2016-11-12T19:18:40.613654: step 11272, loss 0.00036255, acc 1
2016-11-12T19:18:40.673663: step 11273, loss 8.17984e-05, acc 1
2016-11-12T19:18:40.731002: step 11274, loss 0.0539362, acc 0.96875
2016-11-12T19:18:40.790388: step 11275, loss 0.000403304, acc 1
2016-11-12T19:18:40.847890: step 11276, loss 0.0105067, acc 1
2016-11-12T19:18:40.904835: step 11277, loss 7.88608e-05, acc 1
2016-11-12T19:18:40.961809: step 11278, loss 8.27418e-05, acc 1
2016-11-12T19:18:41.019060: step 11279, loss 0.0109431, acc 1
2016-11-12T19:18:41.078867: step 11280, loss 0.00739916, acc 1
2016-11-12T19:18:41.141379: step 11281, loss 0.000178733, acc 1
2016-11-12T19:18:41.198819: step 11282, loss 0.00531079, acc 1
2016-11-12T19:18:41.259254: step 11283, loss 0.242064, acc 0.96875
2016-11-12T19:18:41.318952: step 11284, loss 0.0171201, acc 0.984375
2016-11-12T19:18:41.376769: step 11285, loss 0.00180106, acc 1
2016-11-12T19:18:41.435874: step 11286, loss 0.00118537, acc 1
2016-11-12T19:18:41.493648: step 11287, loss 0.000734833, acc 1
2016-11-12T19:18:41.552778: step 11288, loss 0.000150197, acc 1
2016-11-12T19:18:41.592900: step 11289, loss 0.000284362, acc 1
2016-11-12T19:18:41.652214: step 11290, loss 0.00453278, acc 1
2016-11-12T19:18:41.712856: step 11291, loss 0.0108343, acc 1
2016-11-12T19:18:41.770229: step 11292, loss 0.0042225, acc 1
2016-11-12T19:18:41.829396: step 11293, loss 0.0319377, acc 0.984375
2016-11-12T19:18:41.890304: step 11294, loss 0.0120582, acc 0.984375
2016-11-12T19:18:41.947943: step 11295, loss 0.00178742, acc 1
2016-11-12T19:18:42.008493: step 11296, loss 0.000865274, acc 1
2016-11-12T19:18:42.067125: step 11297, loss 0.000679862, acc 1
2016-11-12T19:18:42.125801: step 11298, loss 0.000419825, acc 1
2016-11-12T19:18:42.183136: step 11299, loss 0.0117912, acc 0.984375
2016-11-12T19:18:42.240279: step 11300, loss 0.0135818, acc 0.984375

Evaluation:
2016-11-12T19:18:42.312935: step 11300, loss 4.67508, acc 0.544

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-11300

2016-11-12T19:18:42.801558: step 11301, loss 0.000419967, acc 1
2016-11-12T19:18:42.861349: step 11302, loss 0.00137048, acc 1
2016-11-12T19:18:42.919981: step 11303, loss 1.96399e-05, acc 1
2016-11-12T19:18:42.978117: step 11304, loss 0.00432812, acc 1
2016-11-12T19:18:43.038530: step 11305, loss 0.0108433, acc 1
2016-11-12T19:18:43.096761: step 11306, loss 0.0039802, acc 1
2016-11-12T19:18:43.156997: step 11307, loss 0.000682133, acc 1
2016-11-12T19:18:43.215788: step 11308, loss 0.00321371, acc 1
2016-11-12T19:18:43.271584: step 11309, loss 0.00118706, acc 1
2016-11-12T19:18:43.329634: step 11310, loss 0.00024856, acc 1
2016-11-12T19:18:43.387120: step 11311, loss 0.0103784, acc 1
2016-11-12T19:18:43.446097: step 11312, loss 0.000527049, acc 1
2016-11-12T19:18:43.505181: step 11313, loss 0.00139742, acc 1
2016-11-12T19:18:43.563201: step 11314, loss 0.000368496, acc 1
2016-11-12T19:18:43.621098: step 11315, loss 0.000156356, acc 1
2016-11-12T19:18:43.678817: step 11316, loss 0.000333241, acc 1
2016-11-12T19:18:43.736777: step 11317, loss 0.000125351, acc 1
2016-11-12T19:18:43.793719: step 11318, loss 0.00347346, acc 1
2016-11-12T19:18:43.853664: step 11319, loss 0.00161356, acc 1
2016-11-12T19:18:43.912513: step 11320, loss 0.000959285, acc 1
2016-11-12T19:18:43.972128: step 11321, loss 0.00693383, acc 1
2016-11-12T19:18:44.032837: step 11322, loss 0.00221247, acc 1
2016-11-12T19:18:44.094276: step 11323, loss 5.90796e-06, acc 1
2016-11-12T19:18:44.150635: step 11324, loss 0.00013765, acc 1
2016-11-12T19:18:44.206441: step 11325, loss 4.739e-05, acc 1
2016-11-12T19:18:44.264052: step 11326, loss 0.00332304, acc 1
2016-11-12T19:18:44.322384: step 11327, loss 0.000651501, acc 1
2016-11-12T19:18:44.381204: step 11328, loss 0.00984503, acc 1
2016-11-12T19:18:44.438199: step 11329, loss 0.00869038, acc 1
2016-11-12T19:18:44.497585: step 11330, loss 0.0267116, acc 0.984375
2016-11-12T19:18:44.554919: step 11331, loss 5.1582e-05, acc 1
2016-11-12T19:18:44.611571: step 11332, loss 0.00232564, acc 1
2016-11-12T19:18:44.668883: step 11333, loss 0.0223611, acc 0.984375
2016-11-12T19:18:44.731821: step 11334, loss 0.0130956, acc 0.984375
2016-11-12T19:18:44.791427: step 11335, loss 2.88988e-05, acc 1
2016-11-12T19:18:44.848004: step 11336, loss 0.0142046, acc 0.984375
2016-11-12T19:18:44.906040: step 11337, loss 0.000170531, acc 1
2016-11-12T19:18:44.964427: step 11338, loss 0.000595141, acc 1
2016-11-12T19:18:45.024660: step 11339, loss 0.0132165, acc 0.984375
2016-11-12T19:18:45.084745: step 11340, loss 0.0244575, acc 0.984375
2016-11-12T19:18:45.142177: step 11341, loss 0.00022622, acc 1
2016-11-12T19:18:45.201415: step 11342, loss 0.0226299, acc 0.96875
2016-11-12T19:18:45.261768: step 11343, loss 0.020993, acc 0.984375
2016-11-12T19:18:45.320814: step 11344, loss 0.00108449, acc 1
2016-11-12T19:18:45.378747: step 11345, loss 3.12654e-05, acc 1
2016-11-12T19:18:45.435327: step 11346, loss 0.000477057, acc 1
2016-11-12T19:18:45.493175: step 11347, loss 0.00926628, acc 1
2016-11-12T19:18:45.552914: step 11348, loss 0.00036126, acc 1
2016-11-12T19:18:45.612904: step 11349, loss 0.00153641, acc 1
2016-11-12T19:18:45.670966: step 11350, loss 0.120627, acc 0.984375
2016-11-12T19:18:45.731029: step 11351, loss 0.000242465, acc 1
2016-11-12T19:18:45.788080: step 11352, loss 0.00279149, acc 1
2016-11-12T19:18:45.847832: step 11353, loss 4.27523e-05, acc 1
2016-11-12T19:18:45.905046: step 11354, loss 0.00235807, acc 1
2016-11-12T19:18:45.964008: step 11355, loss 0.000236907, acc 1
2016-11-12T19:18:46.024350: step 11356, loss 0.000351, acc 1
2016-11-12T19:18:46.084315: step 11357, loss 0.0052016, acc 1
2016-11-12T19:18:46.144660: step 11358, loss 0.00693964, acc 1
2016-11-12T19:18:46.204414: step 11359, loss 0.0039317, acc 1
2016-11-12T19:18:46.244073: step 11360, loss 1.19208e-06, acc 1
2016-11-12T19:18:46.300664: step 11361, loss 0.00105142, acc 1
2016-11-12T19:18:46.357860: step 11362, loss 0.0245227, acc 1
2016-11-12T19:18:46.415523: step 11363, loss 0.0111555, acc 0.984375
2016-11-12T19:18:46.474762: step 11364, loss 0.00135129, acc 1
2016-11-12T19:18:46.536146: step 11365, loss 0.00142046, acc 1
2016-11-12T19:18:46.593397: step 11366, loss 1.60956e-05, acc 1
2016-11-12T19:18:46.652670: step 11367, loss 0.00203826, acc 1
2016-11-12T19:18:46.709837: step 11368, loss 0.000371988, acc 1
2016-11-12T19:18:46.768229: step 11369, loss 0.00181728, acc 1
2016-11-12T19:18:46.829086: step 11370, loss 0.00101135, acc 1
2016-11-12T19:18:46.889609: step 11371, loss 6.29463e-05, acc 1
2016-11-12T19:18:46.949328: step 11372, loss 0.0096619, acc 1
2016-11-12T19:18:47.009767: step 11373, loss 0.00112439, acc 1
2016-11-12T19:18:47.067837: step 11374, loss 0.0102834, acc 1
2016-11-12T19:18:47.123806: step 11375, loss 2.66226e-05, acc 1
2016-11-12T19:18:47.180845: step 11376, loss 0.00144221, acc 1
2016-11-12T19:18:47.240815: step 11377, loss 0.000291171, acc 1
2016-11-12T19:18:47.299543: step 11378, loss 0.00104293, acc 1
2016-11-12T19:18:47.358141: step 11379, loss 0.00714443, acc 1
2016-11-12T19:18:47.415839: step 11380, loss 0.000319585, acc 1
2016-11-12T19:18:47.473676: step 11381, loss 0.0302747, acc 0.96875
2016-11-12T19:18:47.531815: step 11382, loss 0.0110335, acc 1
2016-11-12T19:18:47.590755: step 11383, loss 8.68827e-05, acc 1
2016-11-12T19:18:47.649070: step 11384, loss 0.000211271, acc 1
2016-11-12T19:18:47.706737: step 11385, loss 6.85037e-05, acc 1
2016-11-12T19:18:47.764399: step 11386, loss 0.000728172, acc 1
2016-11-12T19:18:47.822464: step 11387, loss 0.00117878, acc 1
2016-11-12T19:18:47.881484: step 11388, loss 2.25814e-05, acc 1
2016-11-12T19:18:47.938068: step 11389, loss 0.000319602, acc 1
2016-11-12T19:18:47.997029: step 11390, loss 0.00194592, acc 1
2016-11-12T19:18:48.054242: step 11391, loss 0.0180227, acc 0.984375
2016-11-12T19:18:48.112670: step 11392, loss 0.000857092, acc 1
2016-11-12T19:18:48.172483: step 11393, loss 9.38221e-05, acc 1
2016-11-12T19:18:48.230015: step 11394, loss 4.5839e-05, acc 1
2016-11-12T19:18:48.287225: step 11395, loss 0.00113587, acc 1
2016-11-12T19:18:48.345110: step 11396, loss 0.00229945, acc 1
2016-11-12T19:18:48.402303: step 11397, loss 5.29189e-05, acc 1
2016-11-12T19:18:48.459437: step 11398, loss 7.98407e-05, acc 1
2016-11-12T19:18:48.516628: step 11399, loss 0.0204582, acc 0.984375
2016-11-12T19:18:48.576667: step 11400, loss 0.00822804, acc 1

Evaluation:
2016-11-12T19:18:48.648569: step 11400, loss 4.72548, acc 0.542

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-11400

2016-11-12T19:18:49.135292: step 11401, loss 0.000293766, acc 1
2016-11-12T19:18:49.195356: step 11402, loss 6.88466e-05, acc 1
2016-11-12T19:18:49.252777: step 11403, loss 0.00114005, acc 1
2016-11-12T19:18:49.312857: step 11404, loss 0.000266099, acc 1
2016-11-12T19:18:49.372661: step 11405, loss 0.000125692, acc 1
2016-11-12T19:18:49.432143: step 11406, loss 0.00925107, acc 1
2016-11-12T19:18:49.491998: step 11407, loss 4.32387e-05, acc 1
2016-11-12T19:18:49.549306: step 11408, loss 5.37295e-05, acc 1
2016-11-12T19:18:49.605747: step 11409, loss 0.0447654, acc 0.984375
2016-11-12T19:18:49.663965: step 11410, loss 0.0020308, acc 1
2016-11-12T19:18:49.720435: step 11411, loss 0.000210046, acc 1
2016-11-12T19:18:49.777842: step 11412, loss 0.0442641, acc 0.984375
2016-11-12T19:18:49.837168: step 11413, loss 8.82818e-05, acc 1
2016-11-12T19:18:49.895500: step 11414, loss 4.06058e-05, acc 1
2016-11-12T19:18:49.952205: step 11415, loss 0.0337287, acc 0.96875
2016-11-12T19:18:50.011085: step 11416, loss 0.0122671, acc 0.984375
2016-11-12T19:18:50.068941: step 11417, loss 0.0005969, acc 1
2016-11-12T19:18:50.129426: step 11418, loss 0.000100287, acc 1
2016-11-12T19:18:50.188057: step 11419, loss 0.00275439, acc 1
2016-11-12T19:18:50.245654: step 11420, loss 0.0174603, acc 0.984375
2016-11-12T19:18:50.303463: step 11421, loss 0.00077975, acc 1
2016-11-12T19:18:50.360871: step 11422, loss 0.0153875, acc 1
2016-11-12T19:18:50.420826: step 11423, loss 3.39145e-05, acc 1
2016-11-12T19:18:50.481136: step 11424, loss 3.04164e-05, acc 1
2016-11-12T19:18:50.537111: step 11425, loss 0.000363751, acc 1
2016-11-12T19:18:50.595291: step 11426, loss 5.61442e-05, acc 1
2016-11-12T19:18:50.653391: step 11427, loss 0.000345466, acc 1
2016-11-12T19:18:50.713460: step 11428, loss 0.00364608, acc 1
2016-11-12T19:18:50.770839: step 11429, loss 0.00110911, acc 1
2016-11-12T19:18:50.831591: step 11430, loss 0.0235646, acc 0.984375
2016-11-12T19:18:50.873200: step 11431, loss 0.000670336, acc 1
2016-11-12T19:18:50.933426: step 11432, loss 1.50594e-05, acc 1
2016-11-12T19:18:50.990704: step 11433, loss 0.0265801, acc 0.984375
2016-11-12T19:18:51.052615: step 11434, loss 0.000100732, acc 1
2016-11-12T19:18:51.109435: step 11435, loss 1.90479e-05, acc 1
2016-11-12T19:18:51.167342: step 11436, loss 0.0283689, acc 0.984375
2016-11-12T19:18:51.224537: step 11437, loss 0.0132119, acc 1
2016-11-12T19:18:51.282861: step 11438, loss 0.000266392, acc 1
2016-11-12T19:18:51.340788: step 11439, loss 0.0295418, acc 0.984375
2016-11-12T19:18:51.399033: step 11440, loss 0.00115507, acc 1
2016-11-12T19:18:51.457077: step 11441, loss 0.00057538, acc 1
2016-11-12T19:18:51.516016: step 11442, loss 0.0108961, acc 1
2016-11-12T19:18:51.574443: step 11443, loss 0.0124974, acc 0.984375
2016-11-12T19:18:51.631902: step 11444, loss 1.21659e-05, acc 1
2016-11-12T19:18:51.689296: step 11445, loss 0.000214445, acc 1
2016-11-12T19:18:51.749314: step 11446, loss 0.00470414, acc 1
2016-11-12T19:18:51.808799: step 11447, loss 0.000159763, acc 1
2016-11-12T19:18:51.868808: step 11448, loss 0.0026967, acc 1
2016-11-12T19:18:51.926630: step 11449, loss 8.08758e-05, acc 1
2016-11-12T19:18:51.985703: step 11450, loss 8.00817e-05, acc 1
2016-11-12T19:18:52.043112: step 11451, loss 0.00025234, acc 1
2016-11-12T19:18:52.100708: step 11452, loss 0.0108755, acc 1
2016-11-12T19:18:52.160416: step 11453, loss 7.50467e-05, acc 1
2016-11-12T19:18:52.217033: step 11454, loss 4.10698e-06, acc 1
2016-11-12T19:18:52.273028: step 11455, loss 0.0110425, acc 0.984375
2016-11-12T19:18:52.331712: step 11456, loss 0.0129721, acc 0.984375
2016-11-12T19:18:52.389753: step 11457, loss 0.000131921, acc 1
2016-11-12T19:18:52.447358: step 11458, loss 6.4166e-06, acc 1
2016-11-12T19:18:52.504840: step 11459, loss 4.63331e-05, acc 1
2016-11-12T19:18:52.561693: step 11460, loss 0.000132445, acc 1
2016-11-12T19:18:52.620146: step 11461, loss 0.0196722, acc 0.984375
2016-11-12T19:18:52.677940: step 11462, loss 0.00700586, acc 1
2016-11-12T19:18:52.735924: step 11463, loss 0.00159897, acc 1
2016-11-12T19:18:52.794507: step 11464, loss 0.0111837, acc 0.984375
2016-11-12T19:18:52.852124: step 11465, loss 2.96415e-05, acc 1
2016-11-12T19:18:52.908406: step 11466, loss 1.88202e-05, acc 1
2016-11-12T19:18:52.965411: step 11467, loss 0.000512406, acc 1
2016-11-12T19:18:53.023636: step 11468, loss 5.37406e-05, acc 1
2016-11-12T19:18:53.080473: step 11469, loss 0.0280163, acc 0.984375
2016-11-12T19:18:53.138617: step 11470, loss 5.17213e-06, acc 1
2016-11-12T19:18:53.194697: step 11471, loss 0.000823808, acc 1
2016-11-12T19:18:53.252178: step 11472, loss 0.00254745, acc 1
2016-11-12T19:18:53.315444: step 11473, loss 0.000169327, acc 1
2016-11-12T19:18:53.372022: step 11474, loss 0.0006976, acc 1
2016-11-12T19:18:53.431309: step 11475, loss 7.93811e-05, acc 1
2016-11-12T19:18:53.488089: step 11476, loss 0.00810792, acc 1
2016-11-12T19:18:53.545598: step 11477, loss 0.00883402, acc 1
2016-11-12T19:18:53.605087: step 11478, loss 0.00521043, acc 1
2016-11-12T19:18:53.666251: step 11479, loss 0.0195523, acc 0.984375
2016-11-12T19:18:53.724587: step 11480, loss 0.0239938, acc 1
2016-11-12T19:18:53.784787: step 11481, loss 0.0229642, acc 0.984375
2016-11-12T19:18:53.842695: step 11482, loss 7.74204e-05, acc 1
2016-11-12T19:18:53.899556: step 11483, loss 0.0312954, acc 0.984375
2016-11-12T19:18:53.958456: step 11484, loss 5.45174e-05, acc 1
2016-11-12T19:18:54.016894: step 11485, loss 3.66638e-05, acc 1
2016-11-12T19:18:54.076940: step 11486, loss 0.000154433, acc 1
2016-11-12T19:18:54.136157: step 11487, loss 3.89142e-05, acc 1
2016-11-12T19:18:54.192698: step 11488, loss 0.000201494, acc 1
2016-11-12T19:18:54.251068: step 11489, loss 0.000146014, acc 1
2016-11-12T19:18:54.307001: step 11490, loss 0.000547344, acc 1
2016-11-12T19:18:54.365267: step 11491, loss 6.27794e-05, acc 1
2016-11-12T19:18:54.423491: step 11492, loss 0.0109487, acc 0.984375
2016-11-12T19:18:54.480214: step 11493, loss 0.000295794, acc 1
2016-11-12T19:18:54.537713: step 11494, loss 0.00206757, acc 1
2016-11-12T19:18:54.596740: step 11495, loss 1.94983e-05, acc 1
2016-11-12T19:18:54.653406: step 11496, loss 0.0047962, acc 1
2016-11-12T19:18:54.717634: step 11497, loss 0.000402498, acc 1
2016-11-12T19:18:54.776978: step 11498, loss 0.00315157, acc 1
2016-11-12T19:18:54.837178: step 11499, loss 0.00238489, acc 1
2016-11-12T19:18:54.895253: step 11500, loss 4.77387e-05, acc 1

Evaluation:
2016-11-12T19:18:54.967026: step 11500, loss 4.76201, acc 0.546

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-11500

2016-11-12T19:18:55.453111: step 11501, loss 0.000152084, acc 1
2016-11-12T19:18:55.491936: step 11502, loss 8.04657e-07, acc 1
2016-11-12T19:18:55.549379: step 11503, loss 0.00372396, acc 1
2016-11-12T19:18:55.609591: step 11504, loss 0.0103771, acc 1
2016-11-12T19:18:55.668019: step 11505, loss 0.000523441, acc 1
2016-11-12T19:18:55.725250: step 11506, loss 0.00290223, acc 1
2016-11-12T19:18:55.784331: step 11507, loss 3.79772e-05, acc 1
2016-11-12T19:18:55.840883: step 11508, loss 0.0222686, acc 0.984375
2016-11-12T19:18:55.899825: step 11509, loss 4.79771e-06, acc 1
2016-11-12T19:18:55.956347: step 11510, loss 0.0116012, acc 0.984375
2016-11-12T19:18:56.013993: step 11511, loss 2.79816e-05, acc 1
2016-11-12T19:18:56.071498: step 11512, loss 0.00688197, acc 1
2016-11-12T19:18:56.130154: step 11513, loss 0.00145229, acc 1
2016-11-12T19:18:56.187593: step 11514, loss 0.0167557, acc 0.984375
2016-11-12T19:18:56.246664: step 11515, loss 0.00133091, acc 1
2016-11-12T19:18:56.304967: step 11516, loss 0.000811546, acc 1
2016-11-12T19:18:56.364334: step 11517, loss 0.00024686, acc 1
2016-11-12T19:18:56.422079: step 11518, loss 0.000100655, acc 1
2016-11-12T19:18:56.479607: step 11519, loss 0.0330714, acc 0.984375
2016-11-12T19:18:56.538953: step 11520, loss 0.0143002, acc 0.984375
2016-11-12T19:18:56.598428: step 11521, loss 0.00676644, acc 1
2016-11-12T19:18:56.656690: step 11522, loss 0.0236072, acc 0.984375
2016-11-12T19:18:56.715886: step 11523, loss 0.000911981, acc 1
2016-11-12T19:18:56.776810: step 11524, loss 2.60662e-05, acc 1
2016-11-12T19:18:56.835478: step 11525, loss 0.000120032, acc 1
2016-11-12T19:18:56.892059: step 11526, loss 0.000150483, acc 1
2016-11-12T19:18:56.952291: step 11527, loss 0.0109357, acc 0.984375
2016-11-12T19:18:57.009712: step 11528, loss 0.000925237, acc 1
2016-11-12T19:18:57.069305: step 11529, loss 5.13628e-05, acc 1
2016-11-12T19:18:57.127467: step 11530, loss 0.0167466, acc 0.984375
2016-11-12T19:18:57.187613: step 11531, loss 0.00289, acc 1
2016-11-12T19:18:57.245675: step 11532, loss 0.00277078, acc 1
2016-11-12T19:18:57.304334: step 11533, loss 0.00975629, acc 1
2016-11-12T19:18:57.362872: step 11534, loss 0.0013015, acc 1
2016-11-12T19:18:57.420466: step 11535, loss 0.0105881, acc 1
2016-11-12T19:18:57.477005: step 11536, loss 2.351e-05, acc 1
2016-11-12T19:18:57.533495: step 11537, loss 0.0037396, acc 1
2016-11-12T19:18:57.592839: step 11538, loss 0.025875, acc 0.984375
2016-11-12T19:18:57.652812: step 11539, loss 0.0393624, acc 0.984375
2016-11-12T19:18:57.712487: step 11540, loss 0.00211397, acc 1
2016-11-12T19:18:57.772111: step 11541, loss 0.000402906, acc 1
2016-11-12T19:18:57.832936: step 11542, loss 8.52117e-05, acc 1
2016-11-12T19:18:57.891191: step 11543, loss 5.37042e-05, acc 1
2016-11-12T19:18:57.948800: step 11544, loss 0.00031238, acc 1
2016-11-12T19:18:58.005753: step 11545, loss 5.31799e-05, acc 1
2016-11-12T19:18:58.063470: step 11546, loss 3.63272e-05, acc 1
2016-11-12T19:18:58.119657: step 11547, loss 0.000283413, acc 1
2016-11-12T19:18:58.177330: step 11548, loss 0.0018127, acc 1
2016-11-12T19:18:58.236620: step 11549, loss 0.0395765, acc 0.984375
2016-11-12T19:18:58.296727: step 11550, loss 0.0128521, acc 0.984375
2016-11-12T19:18:58.358071: step 11551, loss 0.000373824, acc 1
2016-11-12T19:18:58.416651: step 11552, loss 0.00037376, acc 1
2016-11-12T19:18:58.476505: step 11553, loss 0.000101932, acc 1
2016-11-12T19:18:58.533220: step 11554, loss 2.98514e-05, acc 1
2016-11-12T19:18:58.589859: step 11555, loss 0.0227154, acc 0.984375
2016-11-12T19:18:58.649189: step 11556, loss 0.0101439, acc 1
2016-11-12T19:18:58.708750: step 11557, loss 0.0202906, acc 0.984375
2016-11-12T19:18:58.766217: step 11558, loss 0.000271031, acc 1
2016-11-12T19:18:58.823024: step 11559, loss 9.3209e-05, acc 1
2016-11-12T19:18:58.880721: step 11560, loss 0.00312581, acc 1
2016-11-12T19:18:58.939411: step 11561, loss 0.000479056, acc 1
2016-11-12T19:18:58.998738: step 11562, loss 0.00141994, acc 1
2016-11-12T19:18:59.064164: step 11563, loss 0.00102237, acc 1
2016-11-12T19:18:59.121798: step 11564, loss 0.000618958, acc 1
2016-11-12T19:18:59.180613: step 11565, loss 0.000112125, acc 1
2016-11-12T19:18:59.238883: step 11566, loss 8.26397e-06, acc 1
2016-11-12T19:18:59.294915: step 11567, loss 0.0484403, acc 0.984375
2016-11-12T19:18:59.353979: step 11568, loss 0.00092781, acc 1
2016-11-12T19:18:59.413002: step 11569, loss 0.000207968, acc 1
2016-11-12T19:18:59.469726: step 11570, loss 0.0121102, acc 0.984375
2016-11-12T19:18:59.528659: step 11571, loss 0.00127743, acc 1
2016-11-12T19:18:59.587547: step 11572, loss 7.48842e-05, acc 1
2016-11-12T19:18:59.626146: step 11573, loss 0.00146267, acc 1
2016-11-12T19:18:59.684682: step 11574, loss 0.00894029, acc 1
2016-11-12T19:18:59.744050: step 11575, loss 2.4982e-05, acc 1
2016-11-12T19:18:59.800311: step 11576, loss 0.000677817, acc 1
2016-11-12T19:18:59.859501: step 11577, loss 0.000142215, acc 1
2016-11-12T19:18:59.916785: step 11578, loss 9.62361e-06, acc 1
2016-11-12T19:18:59.972757: step 11579, loss 4.28126e-05, acc 1
2016-11-12T19:19:00.029158: step 11580, loss 0.000543235, acc 1
2016-11-12T19:19:00.087951: step 11581, loss 0.0011856, acc 1
2016-11-12T19:19:00.149198: step 11582, loss 0.00554918, acc 1
2016-11-12T19:19:00.208840: step 11583, loss 0.301622, acc 0.984375
2016-11-12T19:19:00.269758: step 11584, loss 0.00019452, acc 1
2016-11-12T19:19:00.327753: step 11585, loss 0.0121412, acc 0.984375
2016-11-12T19:19:00.388778: step 11586, loss 0.00427464, acc 1
2016-11-12T19:19:00.446421: step 11587, loss 1.27992e-05, acc 1
2016-11-12T19:19:00.503145: step 11588, loss 0.00690485, acc 1
2016-11-12T19:19:00.561436: step 11589, loss 0.00621363, acc 1
2016-11-12T19:19:00.618976: step 11590, loss 0.00610396, acc 1
2016-11-12T19:19:00.676247: step 11591, loss 0.000152843, acc 1
2016-11-12T19:19:00.733246: step 11592, loss 0.0138234, acc 0.984375
2016-11-12T19:19:00.790111: step 11593, loss 1.09642e-05, acc 1
2016-11-12T19:19:00.846096: step 11594, loss 0.0415543, acc 0.984375
2016-11-12T19:19:00.904833: step 11595, loss 0.00707881, acc 1
2016-11-12T19:19:00.962359: step 11596, loss 0.000102781, acc 1
2016-11-12T19:19:01.021035: step 11597, loss 0.00720292, acc 1
2016-11-12T19:19:01.082521: step 11598, loss 0.000273559, acc 1
2016-11-12T19:19:01.141596: step 11599, loss 0.0148532, acc 0.984375
2016-11-12T19:19:01.201529: step 11600, loss 0.000902125, acc 1

Evaluation:
2016-11-12T19:19:01.275042: step 11600, loss 4.78022, acc 0.552

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-11600

2016-11-12T19:19:01.766543: step 11601, loss 0.000664882, acc 1
2016-11-12T19:19:01.828057: step 11602, loss 0.00313885, acc 1
2016-11-12T19:19:01.888615: step 11603, loss 0.00073383, acc 1
2016-11-12T19:19:01.946216: step 11604, loss 8.80388e-05, acc 1
2016-11-12T19:19:02.004278: step 11605, loss 0.0467021, acc 0.984375
2016-11-12T19:19:02.064023: step 11606, loss 0.0106675, acc 1
2016-11-12T19:19:02.121575: step 11607, loss 0.00343683, acc 1
2016-11-12T19:19:02.180719: step 11608, loss 0.0277047, acc 0.96875
2016-11-12T19:19:02.238793: step 11609, loss 0.0276271, acc 0.96875
2016-11-12T19:19:02.296935: step 11610, loss 0.0141061, acc 0.984375
2016-11-12T19:19:02.356066: step 11611, loss 0.000429851, acc 1
2016-11-12T19:19:02.416448: step 11612, loss 2.9918e-05, acc 1
2016-11-12T19:19:02.472839: step 11613, loss 0.0107906, acc 1
2016-11-12T19:19:02.530631: step 11614, loss 0.0130923, acc 0.984375
2016-11-12T19:19:02.588556: step 11615, loss 0.0207259, acc 0.984375
2016-11-12T19:19:02.648419: step 11616, loss 0.00569793, acc 1
2016-11-12T19:19:02.706453: step 11617, loss 1.25232e-05, acc 1
2016-11-12T19:19:02.767311: step 11618, loss 0.019998, acc 0.984375
2016-11-12T19:19:02.826856: step 11619, loss 0.00176421, acc 1
2016-11-12T19:19:02.885988: step 11620, loss 1.84953e-05, acc 1
2016-11-12T19:19:02.944334: step 11621, loss 0.000116878, acc 1
2016-11-12T19:19:03.004145: step 11622, loss 0.0269428, acc 0.984375
2016-11-12T19:19:03.061381: step 11623, loss 4.07571e-05, acc 1
2016-11-12T19:19:03.119929: step 11624, loss 2.71826e-05, acc 1
2016-11-12T19:19:03.175920: step 11625, loss 0.000441793, acc 1
2016-11-12T19:19:03.236661: step 11626, loss 0.0861249, acc 0.984375
2016-11-12T19:19:03.298144: step 11627, loss 0.0150642, acc 1
2016-11-12T19:19:03.356762: step 11628, loss 0.000457435, acc 1
2016-11-12T19:19:03.414668: step 11629, loss 0.000820125, acc 1
2016-11-12T19:19:03.471838: step 11630, loss 0.000146014, acc 1
2016-11-12T19:19:03.530267: step 11631, loss 0.00596916, acc 1
2016-11-12T19:19:03.587868: step 11632, loss 0.000989276, acc 1
2016-11-12T19:19:03.645431: step 11633, loss 4.8983e-05, acc 1
2016-11-12T19:19:03.706110: step 11634, loss 0.000213888, acc 1
2016-11-12T19:19:03.764146: step 11635, loss 0.00157452, acc 1
2016-11-12T19:19:03.822089: step 11636, loss 0.00010497, acc 1
2016-11-12T19:19:03.880397: step 11637, loss 3.94271e-05, acc 1
2016-11-12T19:19:03.937229: step 11638, loss 7.36343e-05, acc 1
2016-11-12T19:19:03.993988: step 11639, loss 0.00749837, acc 1
2016-11-12T19:19:04.052991: step 11640, loss 0.00411864, acc 1
2016-11-12T19:19:04.111751: step 11641, loss 0.00154505, acc 1
2016-11-12T19:19:04.172908: step 11642, loss 2.32342e-05, acc 1
2016-11-12T19:19:04.232052: step 11643, loss 0.000977416, acc 1
2016-11-12T19:19:04.270057: step 11644, loss 1.74911e-05, acc 1
2016-11-12T19:19:04.328774: step 11645, loss 0.000145159, acc 1
2016-11-12T19:19:04.387777: step 11646, loss 0.00181433, acc 1
2016-11-12T19:19:04.445880: step 11647, loss 0.0160099, acc 0.984375
2016-11-12T19:19:04.504879: step 11648, loss 0.000655896, acc 1
2016-11-12T19:19:04.561302: step 11649, loss 0.0251914, acc 0.984375
2016-11-12T19:19:04.619228: step 11650, loss 8.83389e-06, acc 1
2016-11-12T19:19:04.676833: step 11651, loss 0.00704781, acc 1
2016-11-12T19:19:04.737006: step 11652, loss 0.00573913, acc 1
2016-11-12T19:19:04.794139: step 11653, loss 0.00044431, acc 1
2016-11-12T19:19:04.851673: step 11654, loss 0.00636659, acc 1
2016-11-12T19:19:04.911456: step 11655, loss 0.000214344, acc 1
2016-11-12T19:19:04.968882: step 11656, loss 0.0008709, acc 1
2016-11-12T19:19:05.025553: step 11657, loss 5.93814e-05, acc 1
2016-11-12T19:19:05.082575: step 11658, loss 2.08446e-05, acc 1
2016-11-12T19:19:05.139113: step 11659, loss 0.00245203, acc 1
2016-11-12T19:19:05.197260: step 11660, loss 0.00222024, acc 1
2016-11-12T19:19:05.264826: step 11661, loss 0.00664565, acc 1
2016-11-12T19:19:05.321735: step 11662, loss 0.000183934, acc 1
2016-11-12T19:19:05.380766: step 11663, loss 0.00713793, acc 1
2016-11-12T19:19:05.441745: step 11664, loss 0.00993069, acc 1
2016-11-12T19:19:05.500977: step 11665, loss 0.00646726, acc 1
2016-11-12T19:19:05.560565: step 11666, loss 0.000216022, acc 1
2016-11-12T19:19:05.617776: step 11667, loss 0.000258894, acc 1
2016-11-12T19:19:05.676868: step 11668, loss 0.00136395, acc 1
2016-11-12T19:19:05.734556: step 11669, loss 6.90365e-05, acc 1
2016-11-12T19:19:05.796631: step 11670, loss 0.0273649, acc 0.984375
2016-11-12T19:19:05.856334: step 11671, loss 0.000110969, acc 1
2016-11-12T19:19:05.916567: step 11672, loss 0.000210372, acc 1
2016-11-12T19:19:05.974055: step 11673, loss 0.00187283, acc 1
2016-11-12T19:19:06.032341: step 11674, loss 0.000503717, acc 1
2016-11-12T19:19:06.089579: step 11675, loss 0.00430963, acc 1
2016-11-12T19:19:06.148151: step 11676, loss 6.78575e-05, acc 1
2016-11-12T19:19:06.206055: step 11677, loss 0.0136992, acc 0.984375
2016-11-12T19:19:06.266171: step 11678, loss 8.16609e-06, acc 1
2016-11-12T19:19:06.325300: step 11679, loss 0.0108813, acc 1
2016-11-12T19:19:06.385318: step 11680, loss 0.045762, acc 0.984375
2016-11-12T19:19:06.444943: step 11681, loss 0.000403419, acc 1
2016-11-12T19:19:06.502155: step 11682, loss 2.21073e-05, acc 1
2016-11-12T19:19:06.561181: step 11683, loss 5.91562e-05, acc 1
2016-11-12T19:19:06.620104: step 11684, loss 0.0142666, acc 0.984375
2016-11-12T19:19:06.679363: step 11685, loss 0.000179647, acc 1
2016-11-12T19:19:06.736619: step 11686, loss 0.0326289, acc 0.984375
2016-11-12T19:19:06.795073: step 11687, loss 0.000176532, acc 1
2016-11-12T19:19:06.854692: step 11688, loss 0.000170787, acc 1
2016-11-12T19:19:06.913177: step 11689, loss 0.00261564, acc 1
2016-11-12T19:19:06.981449: step 11690, loss 4.5615e-06, acc 1
2016-11-12T19:19:07.038780: step 11691, loss 0.0372055, acc 0.984375
2016-11-12T19:19:07.097053: step 11692, loss 0.00532433, acc 1
2016-11-12T19:19:07.156594: step 11693, loss 0.0103521, acc 1
2016-11-12T19:19:07.216467: step 11694, loss 0.00262345, acc 1
2016-11-12T19:19:07.277361: step 11695, loss 4.07336e-06, acc 1
2016-11-12T19:19:07.334560: step 11696, loss 0.000842238, acc 1
2016-11-12T19:19:07.393414: step 11697, loss 7.70569e-05, acc 1
2016-11-12T19:19:07.452220: step 11698, loss 0.000249525, acc 1
2016-11-12T19:19:07.510919: step 11699, loss 0.00132934, acc 1
2016-11-12T19:19:07.568976: step 11700, loss 0.00899485, acc 1

Evaluation:
2016-11-12T19:19:07.641330: step 11700, loss 4.78216, acc 0.548

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-11700

2016-11-12T19:19:08.128815: step 11701, loss 0.00533031, acc 1
2016-11-12T19:19:08.187428: step 11702, loss 4.95997e-06, acc 1
2016-11-12T19:19:08.242931: step 11703, loss 0.0032541, acc 1
2016-11-12T19:19:08.300833: step 11704, loss 0.0108119, acc 1
2016-11-12T19:19:08.357529: step 11705, loss 0.000465123, acc 1
2016-11-12T19:19:08.416664: step 11706, loss 1.4364e-05, acc 1
2016-11-12T19:19:08.473267: step 11707, loss 7.11148e-05, acc 1
2016-11-12T19:19:08.529777: step 11708, loss 8.07963e-06, acc 1
2016-11-12T19:19:08.586574: step 11709, loss 0.0125586, acc 0.984375
2016-11-12T19:19:08.646564: step 11710, loss 9.51089e-06, acc 1
2016-11-12T19:19:08.703722: step 11711, loss 0.000215387, acc 1
2016-11-12T19:19:08.764873: step 11712, loss 0.000261286, acc 1
2016-11-12T19:19:08.822658: step 11713, loss 0.00161618, acc 1
2016-11-12T19:19:08.880705: step 11714, loss 0.00044219, acc 1
2016-11-12T19:19:08.919981: step 11715, loss 7.15211e-06, acc 1
2016-11-12T19:19:08.976277: step 11716, loss 3.79871e-05, acc 1
2016-11-12T19:19:09.032608: step 11717, loss 0.00122539, acc 1
2016-11-12T19:19:09.090257: step 11718, loss 9.19898e-05, acc 1
2016-11-12T19:19:09.148262: step 11719, loss 0.0273508, acc 0.984375
2016-11-12T19:19:09.206760: step 11720, loss 4.78026e-05, acc 1
2016-11-12T19:19:09.265048: step 11721, loss 0.000511702, acc 1
2016-11-12T19:19:09.322475: step 11722, loss 4.24049e-05, acc 1
2016-11-12T19:19:09.378334: step 11723, loss 0.000177281, acc 1
2016-11-12T19:19:09.436655: step 11724, loss 0.00395042, acc 1
2016-11-12T19:19:09.496015: step 11725, loss 0.00180687, acc 1
2016-11-12T19:19:09.556794: step 11726, loss 0.00534367, acc 1
2016-11-12T19:19:09.616032: step 11727, loss 0.0126594, acc 0.984375
2016-11-12T19:19:09.673366: step 11728, loss 0.000608904, acc 1
2016-11-12T19:19:09.731983: step 11729, loss 1.12228e-05, acc 1
2016-11-12T19:19:09.788878: step 11730, loss 0.000374138, acc 1
2016-11-12T19:19:09.847900: step 11731, loss 0.0125348, acc 1
2016-11-12T19:19:09.907581: step 11732, loss 3.65732e-05, acc 1
2016-11-12T19:19:09.965682: step 11733, loss 7.85809e-05, acc 1
2016-11-12T19:19:10.022560: step 11734, loss 0.0115107, acc 0.984375
2016-11-12T19:19:10.078728: step 11735, loss 0.0341213, acc 0.984375
2016-11-12T19:19:10.136048: step 11736, loss 0.000306987, acc 1
2016-11-12T19:19:10.193672: step 11737, loss 2.93171e-05, acc 1
2016-11-12T19:19:10.249344: step 11738, loss 0.000637257, acc 1
2016-11-12T19:19:10.308057: step 11739, loss 1.59254e-06, acc 1
2016-11-12T19:19:10.364005: step 11740, loss 0.000169411, acc 1
2016-11-12T19:19:10.424800: step 11741, loss 0.00866675, acc 1
2016-11-12T19:19:10.484968: step 11742, loss 0.000577074, acc 1
2016-11-12T19:19:10.544104: step 11743, loss 0.0398932, acc 0.984375
2016-11-12T19:19:10.601528: step 11744, loss 0.0029969, acc 1
2016-11-12T19:19:10.658324: step 11745, loss 0.00100584, acc 1
2016-11-12T19:19:10.717745: step 11746, loss 0.0138534, acc 0.984375
2016-11-12T19:19:10.776594: step 11747, loss 0.000437616, acc 1
2016-11-12T19:19:10.834309: step 11748, loss 0.076415, acc 0.984375
2016-11-12T19:19:10.893826: step 11749, loss 0.00012621, acc 1
2016-11-12T19:19:10.953645: step 11750, loss 0.0130884, acc 0.984375
2016-11-12T19:19:11.012667: step 11751, loss 0.0129326, acc 0.984375
2016-11-12T19:19:11.072818: step 11752, loss 0.00368858, acc 1
2016-11-12T19:19:11.132283: step 11753, loss 0.00540025, acc 1
2016-11-12T19:19:11.192029: step 11754, loss 0.0202388, acc 1
2016-11-12T19:19:11.250111: step 11755, loss 0.000676971, acc 1
2016-11-12T19:19:11.308889: step 11756, loss 0.03443, acc 0.984375
2016-11-12T19:19:11.368367: step 11757, loss 0.00609526, acc 1
2016-11-12T19:19:11.429764: step 11758, loss 1.60815e-05, acc 1
2016-11-12T19:19:11.487384: step 11759, loss 0.018328, acc 0.984375
2016-11-12T19:19:11.544735: step 11760, loss 0.000599509, acc 1
2016-11-12T19:19:11.604915: step 11761, loss 5.57203e-05, acc 1
2016-11-12T19:19:11.663229: step 11762, loss 0.0169697, acc 0.984375
2016-11-12T19:19:11.720021: step 11763, loss 0.0530399, acc 0.984375
2016-11-12T19:19:11.779889: step 11764, loss 0.00782132, acc 1
2016-11-12T19:19:11.840655: step 11765, loss 0.0136392, acc 0.984375
2016-11-12T19:19:11.899054: step 11766, loss 3.05387e-05, acc 1
2016-11-12T19:19:11.956813: step 11767, loss 9.67267e-05, acc 1
2016-11-12T19:19:12.015979: step 11768, loss 1.89225e-05, acc 1
2016-11-12T19:19:12.072137: step 11769, loss 0.000211275, acc 1
2016-11-12T19:19:12.130098: step 11770, loss 5.11309e-05, acc 1
2016-11-12T19:19:12.188710: step 11771, loss 6.77451e-05, acc 1
2016-11-12T19:19:12.248901: step 11772, loss 0.142672, acc 0.984375
2016-11-12T19:19:12.309716: step 11773, loss 0.000133427, acc 1
2016-11-12T19:19:12.368755: step 11774, loss 0.000130866, acc 1
2016-11-12T19:19:12.426670: step 11775, loss 0.00134227, acc 1
2016-11-12T19:19:12.485015: step 11776, loss 0.00092826, acc 1
2016-11-12T19:19:12.541989: step 11777, loss 4.05258e-05, acc 1
2016-11-12T19:19:12.599252: step 11778, loss 0.000250432, acc 1
2016-11-12T19:19:12.656987: step 11779, loss 0.00923548, acc 1
2016-11-12T19:19:12.714916: step 11780, loss 1.25309e-05, acc 1
2016-11-12T19:19:12.772833: step 11781, loss 2.8929e-05, acc 1
2016-11-12T19:19:12.829973: step 11782, loss 0.00015094, acc 1
2016-11-12T19:19:12.889411: step 11783, loss 8.33456e-06, acc 1
2016-11-12T19:19:12.945498: step 11784, loss 0.00034617, acc 1
2016-11-12T19:19:13.003801: step 11785, loss 0.046891, acc 0.96875
2016-11-12T19:19:13.045448: step 11786, loss 0.0249959, acc 1
2016-11-12T19:19:13.104749: step 11787, loss 0.0155736, acc 0.984375
2016-11-12T19:19:13.163408: step 11788, loss 0.00072382, acc 1
2016-11-12T19:19:13.223823: step 11789, loss 2.10679e-05, acc 1
2016-11-12T19:19:13.281518: step 11790, loss 0.000249885, acc 1
2016-11-12T19:19:13.339218: step 11791, loss 0.00027634, acc 1
2016-11-12T19:19:13.398081: step 11792, loss 0.0134808, acc 0.984375
2016-11-12T19:19:13.456515: step 11793, loss 0.00672663, acc 1
2016-11-12T19:19:13.516575: step 11794, loss 0.000855742, acc 1
2016-11-12T19:19:13.576411: step 11795, loss 0.000954428, acc 1
2016-11-12T19:19:13.636411: step 11796, loss 0.00490565, acc 1
2016-11-12T19:19:13.696473: step 11797, loss 0.000479261, acc 1
2016-11-12T19:19:13.756000: step 11798, loss 0.000102602, acc 1
2016-11-12T19:19:13.814268: step 11799, loss 0.00193363, acc 1
2016-11-12T19:19:13.871841: step 11800, loss 5.66395e-05, acc 1

Evaluation:
2016-11-12T19:19:13.944334: step 11800, loss 4.76501, acc 0.552

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-11800

2016-11-12T19:19:14.427949: step 11801, loss 0.036455, acc 0.984375
2016-11-12T19:19:14.490114: step 11802, loss 0.0388248, acc 0.96875
2016-11-12T19:19:14.548676: step 11803, loss 0.000526332, acc 1
2016-11-12T19:19:14.608782: step 11804, loss 0.0043356, acc 1
2016-11-12T19:19:14.668577: step 11805, loss 0.0209937, acc 1
2016-11-12T19:19:14.727184: step 11806, loss 0.00549817, acc 1
2016-11-12T19:19:14.784466: step 11807, loss 0.00583045, acc 1
2016-11-12T19:19:14.843150: step 11808, loss 0.0002537, acc 1
2016-11-12T19:19:14.900564: step 11809, loss 0.000220158, acc 1
2016-11-12T19:19:14.959741: step 11810, loss 0.0164295, acc 0.984375
2016-11-12T19:19:15.020568: step 11811, loss 0.000179742, acc 1
2016-11-12T19:19:15.080364: step 11812, loss 4.59291e-06, acc 1
2016-11-12T19:19:15.137010: step 11813, loss 0.00438529, acc 1
2016-11-12T19:19:15.197364: step 11814, loss 6.54305e-05, acc 1
2016-11-12T19:19:15.253820: step 11815, loss 0.237843, acc 0.984375
2016-11-12T19:19:15.312773: step 11816, loss 2.46247e-05, acc 1
2016-11-12T19:19:15.372265: step 11817, loss 0.00876764, acc 1
2016-11-12T19:19:15.429816: step 11818, loss 0.00698747, acc 1
2016-11-12T19:19:15.487646: step 11819, loss 0.000445481, acc 1
2016-11-12T19:19:15.546190: step 11820, loss 0.000374775, acc 1
2016-11-12T19:19:15.603472: step 11821, loss 0.0108833, acc 1
2016-11-12T19:19:15.661420: step 11822, loss 0.00193199, acc 1
2016-11-12T19:19:15.718665: step 11823, loss 0.00754827, acc 1
2016-11-12T19:19:15.777684: step 11824, loss 5.9529e-05, acc 1
2016-11-12T19:19:15.835245: step 11825, loss 0.000394594, acc 1
2016-11-12T19:19:15.892560: step 11826, loss 0.0428874, acc 0.984375
2016-11-12T19:19:15.952198: step 11827, loss 0.0100944, acc 1
2016-11-12T19:19:16.012789: step 11828, loss 0.00636358, acc 1
2016-11-12T19:19:16.070653: step 11829, loss 0.0187121, acc 1
2016-11-12T19:19:16.128016: step 11830, loss 0.00897922, acc 1
2016-11-12T19:19:16.188804: step 11831, loss 0.000354899, acc 1
2016-11-12T19:19:16.246639: step 11832, loss 0.00049176, acc 1
2016-11-12T19:19:16.303473: step 11833, loss 0.0198551, acc 0.984375
2016-11-12T19:19:16.361269: step 11834, loss 0.00332024, acc 1
2016-11-12T19:19:16.418018: step 11835, loss 0.000633317, acc 1
2016-11-12T19:19:16.476762: step 11836, loss 3.6995e-05, acc 1
2016-11-12T19:19:16.532784: step 11837, loss 0.000515934, acc 1
2016-11-12T19:19:16.589293: step 11838, loss 0.00108744, acc 1
2016-11-12T19:19:16.645409: step 11839, loss 3.02661e-05, acc 1
2016-11-12T19:19:16.704955: step 11840, loss 0.0102576, acc 1
2016-11-12T19:19:16.766156: step 11841, loss 0.000312555, acc 1
2016-11-12T19:19:16.825024: step 11842, loss 0.00243639, acc 1
2016-11-12T19:19:16.885312: step 11843, loss 1.80668e-05, acc 1
2016-11-12T19:19:16.941449: step 11844, loss 0.000113595, acc 1
2016-11-12T19:19:16.997733: step 11845, loss 0.000304401, acc 1
2016-11-12T19:19:17.056140: step 11846, loss 0.00488003, acc 1
2016-11-12T19:19:17.115172: step 11847, loss 0.000430819, acc 1
2016-11-12T19:19:17.172645: step 11848, loss 0.0334514, acc 0.984375
2016-11-12T19:19:17.231095: step 11849, loss 0.000110666, acc 1
2016-11-12T19:19:17.287989: step 11850, loss 0.000371266, acc 1
2016-11-12T19:19:17.345420: step 11851, loss 0.00933512, acc 1
2016-11-12T19:19:17.404733: step 11852, loss 0.0118143, acc 0.984375
2016-11-12T19:19:17.464852: step 11853, loss 2.09516e-05, acc 1
2016-11-12T19:19:17.521389: step 11854, loss 0.000234989, acc 1
2016-11-12T19:19:17.581812: step 11855, loss 0.0073777, acc 1
2016-11-12T19:19:17.640761: step 11856, loss 1.33437e-05, acc 1
2016-11-12T19:19:17.679545: step 11857, loss 8.27425e-05, acc 1
2016-11-12T19:19:17.738299: step 11858, loss 0.0666615, acc 0.984375
2016-11-12T19:19:17.796422: step 11859, loss 0.000797356, acc 1
2016-11-12T19:19:17.853554: step 11860, loss 4.00133e-05, acc 1
2016-11-12T19:19:17.910299: step 11861, loss 0.00131355, acc 1
2016-11-12T19:19:17.969132: step 11862, loss 0.000994036, acc 1
2016-11-12T19:19:18.031667: step 11863, loss 0.0114695, acc 0.984375
2016-11-12T19:19:18.090354: step 11864, loss 7.85028e-05, acc 1
2016-11-12T19:19:18.148510: step 11865, loss 0.00174102, acc 1
2016-11-12T19:19:18.206897: step 11866, loss 0.144886, acc 0.984375
2016-11-12T19:19:18.265804: step 11867, loss 0.000471504, acc 1
2016-11-12T19:19:18.325432: step 11868, loss 0.0213634, acc 0.984375
2016-11-12T19:19:18.384527: step 11869, loss 0.00297411, acc 1
2016-11-12T19:19:18.445194: step 11870, loss 0.00640654, acc 1
2016-11-12T19:19:18.504386: step 11871, loss 0.000172168, acc 1
2016-11-12T19:19:18.562655: step 11872, loss 0.00336204, acc 1
2016-11-12T19:19:18.622951: step 11873, loss 5.3123e-05, acc 1
2016-11-12T19:19:18.680966: step 11874, loss 0.000136705, acc 1
2016-11-12T19:19:18.740195: step 11875, loss 2.34801e-05, acc 1
2016-11-12T19:19:18.797042: step 11876, loss 0.00384262, acc 1
2016-11-12T19:19:18.858241: step 11877, loss 0.000129784, acc 1
2016-11-12T19:19:18.916744: step 11878, loss 0.00337312, acc 1
2016-11-12T19:19:18.973569: step 11879, loss 0.000209053, acc 1
2016-11-12T19:19:19.031010: step 11880, loss 0.0184015, acc 1
2016-11-12T19:19:19.092317: step 11881, loss 0.00119303, acc 1
2016-11-12T19:19:19.149276: step 11882, loss 7.28118e-05, acc 1
2016-11-12T19:19:19.208417: step 11883, loss 9.88147e-05, acc 1
2016-11-12T19:19:19.264636: step 11884, loss 4.6136e-05, acc 1
2016-11-12T19:19:19.322617: step 11885, loss 0.0150331, acc 0.984375
2016-11-12T19:19:19.382778: step 11886, loss 0.00364411, acc 1
2016-11-12T19:19:19.440096: step 11887, loss 0.000138508, acc 1
2016-11-12T19:19:19.499721: step 11888, loss 0.0022812, acc 1
2016-11-12T19:19:19.558293: step 11889, loss 4.41702e-05, acc 1
2016-11-12T19:19:19.614963: step 11890, loss 1.15422e-05, acc 1
2016-11-12T19:19:19.673479: step 11891, loss 0.0147549, acc 0.984375
2016-11-12T19:19:19.731475: step 11892, loss 0.000262626, acc 1
2016-11-12T19:19:19.788716: step 11893, loss 5.76832e-05, acc 1
2016-11-12T19:19:19.846933: step 11894, loss 0.00320344, acc 1
2016-11-12T19:19:19.904364: step 11895, loss 0.0112717, acc 0.984375
2016-11-12T19:19:19.962290: step 11896, loss 0.000603109, acc 1
2016-11-12T19:19:20.020988: step 11897, loss 0.00782057, acc 1
2016-11-12T19:19:20.080702: step 11898, loss 0.000237371, acc 1
2016-11-12T19:19:20.138736: step 11899, loss 0.034609, acc 0.984375
2016-11-12T19:19:20.200530: step 11900, loss 0.0188251, acc 0.984375

Evaluation:
2016-11-12T19:19:20.273972: step 11900, loss 4.86835, acc 0.554

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-11900

2016-11-12T19:19:20.760343: step 11901, loss 0.00221953, acc 1
2016-11-12T19:19:20.819779: step 11902, loss 0.0170667, acc 0.984375
2016-11-12T19:19:20.880508: step 11903, loss 0.000286487, acc 1
2016-11-12T19:19:20.938797: step 11904, loss 0.000763546, acc 1
2016-11-12T19:19:20.996807: step 11905, loss 0.000467891, acc 1
2016-11-12T19:19:21.054981: step 11906, loss 0.0183806, acc 0.984375
2016-11-12T19:19:21.113789: step 11907, loss 0.0608665, acc 0.984375
2016-11-12T19:19:21.171210: step 11908, loss 0.000628733, acc 1
2016-11-12T19:19:21.228331: step 11909, loss 0.00592192, acc 1
2016-11-12T19:19:21.289339: step 11910, loss 0.0216162, acc 0.984375
2016-11-12T19:19:21.350103: step 11911, loss 0.000241453, acc 1
2016-11-12T19:19:21.408907: step 11912, loss 0.000186304, acc 1
2016-11-12T19:19:21.468142: step 11913, loss 0.0726594, acc 0.96875
2016-11-12T19:19:21.528363: step 11914, loss 0.00113907, acc 1
2016-11-12T19:19:21.587223: step 11915, loss 0.0092441, acc 1
2016-11-12T19:19:21.646988: step 11916, loss 0.000475852, acc 1
2016-11-12T19:19:21.705306: step 11917, loss 6.16655e-05, acc 1
2016-11-12T19:19:21.762193: step 11918, loss 0.000120615, acc 1
2016-11-12T19:19:21.820752: step 11919, loss 0.00014333, acc 1
2016-11-12T19:19:21.879097: step 11920, loss 0.000734061, acc 1
2016-11-12T19:19:21.936574: step 11921, loss 9.80488e-06, acc 1
2016-11-12T19:19:21.993444: step 11922, loss 0.000659051, acc 1
2016-11-12T19:19:22.050477: step 11923, loss 0.0363962, acc 0.984375
2016-11-12T19:19:22.108670: step 11924, loss 1.16423e-05, acc 1
2016-11-12T19:19:22.167762: step 11925, loss 6.40356e-05, acc 1
2016-11-12T19:19:22.225042: step 11926, loss 1.39141e-05, acc 1
2016-11-12T19:19:22.280684: step 11927, loss 0.000755453, acc 1
2016-11-12T19:19:22.320482: step 11928, loss 9.6772e-05, acc 1
2016-11-12T19:19:22.377917: step 11929, loss 0.000126686, acc 1
2016-11-12T19:19:22.436557: step 11930, loss 4.9775e-05, acc 1
2016-11-12T19:19:22.496859: step 11931, loss 0.000714427, acc 1
2016-11-12T19:19:22.557086: step 11932, loss 0.187834, acc 0.984375
2016-11-12T19:19:22.617127: step 11933, loss 9.86502e-05, acc 1
2016-11-12T19:19:22.674831: step 11934, loss 4.30706e-05, acc 1
2016-11-12T19:19:22.732879: step 11935, loss 0.000383425, acc 1
2016-11-12T19:19:22.790002: step 11936, loss 0.00549905, acc 1
2016-11-12T19:19:22.849553: step 11937, loss 0.00026393, acc 1
2016-11-12T19:19:22.905658: step 11938, loss 0.0170375, acc 0.984375
2016-11-12T19:19:22.963228: step 11939, loss 4.36394e-06, acc 1
2016-11-12T19:19:23.019556: step 11940, loss 0.0032251, acc 1
2016-11-12T19:19:23.078947: step 11941, loss 0.000120333, acc 1
2016-11-12T19:19:23.136384: step 11942, loss 0.0209494, acc 0.984375
2016-11-12T19:19:23.194643: step 11943, loss 0.0136624, acc 0.984375
2016-11-12T19:19:23.254148: step 11944, loss 2.73424e-06, acc 1
2016-11-12T19:19:23.310104: step 11945, loss 0.0113414, acc 1
2016-11-12T19:19:23.367128: step 11946, loss 1.67225e-05, acc 1
2016-11-12T19:19:23.424023: step 11947, loss 0.00987578, acc 1
2016-11-12T19:19:23.484816: step 11948, loss 0.00526648, acc 1
2016-11-12T19:19:23.542430: step 11949, loss 3.58955e-05, acc 1
2016-11-12T19:19:23.600943: step 11950, loss 0.00124607, acc 1
2016-11-12T19:19:23.660491: step 11951, loss 0.014168, acc 1
2016-11-12T19:19:23.719127: step 11952, loss 0.013122, acc 0.984375
2016-11-12T19:19:23.777566: step 11953, loss 2.38597e-05, acc 1
2016-11-12T19:19:23.834867: step 11954, loss 0.0982208, acc 0.984375
2016-11-12T19:19:23.894824: step 11955, loss 0.00728837, acc 1
2016-11-12T19:19:23.952212: step 11956, loss 0.000168913, acc 1
2016-11-12T19:19:24.010147: step 11957, loss 0.0164131, acc 1
2016-11-12T19:19:24.068985: step 11958, loss 0.000408325, acc 1
2016-11-12T19:19:24.127145: step 11959, loss 0.000428313, acc 1
2016-11-12T19:19:24.185124: step 11960, loss 0.000127278, acc 1
2016-11-12T19:19:24.242035: step 11961, loss 0.000109876, acc 1
2016-11-12T19:19:24.299732: step 11962, loss 0.0014607, acc 1
2016-11-12T19:19:24.360755: step 11963, loss 0.0079448, acc 1
2016-11-12T19:19:24.418728: step 11964, loss 4.84641e-05, acc 1
2016-11-12T19:19:24.477122: step 11965, loss 0.000165996, acc 1
2016-11-12T19:19:24.534823: step 11966, loss 0.0074021, acc 1
2016-11-12T19:19:24.596177: step 11967, loss 8.19472e-06, acc 1
2016-11-12T19:19:24.651875: step 11968, loss 0.000132787, acc 1
2016-11-12T19:19:24.711433: step 11969, loss 0.000362873, acc 1
2016-11-12T19:19:24.773127: step 11970, loss 0.0133172, acc 0.984375
2016-11-12T19:19:24.829656: step 11971, loss 0.0025571, acc 1
2016-11-12T19:19:24.888657: step 11972, loss 0.000213009, acc 1
2016-11-12T19:19:24.949393: step 11973, loss 0.000184542, acc 1
2016-11-12T19:19:25.007608: step 11974, loss 3.8943e-05, acc 1
2016-11-12T19:19:25.064495: step 11975, loss 0.00356133, acc 1
2016-11-12T19:19:25.122646: step 11976, loss 0.000467298, acc 1
2016-11-12T19:19:25.180780: step 11977, loss 0.00792004, acc 1
2016-11-12T19:19:25.239799: step 11978, loss 0.0505885, acc 0.96875
2016-11-12T19:19:25.299466: step 11979, loss 0.00256568, acc 1
2016-11-12T19:19:25.357888: step 11980, loss 0.000651978, acc 1
2016-11-12T19:19:25.416405: step 11981, loss 0.004435, acc 1
2016-11-12T19:19:25.476706: step 11982, loss 0.0297326, acc 0.96875
2016-11-12T19:19:25.534916: step 11983, loss 0.0169959, acc 1
2016-11-12T19:19:25.593036: step 11984, loss 0.0117871, acc 0.984375
2016-11-12T19:19:25.652838: step 11985, loss 0.00491035, acc 1
2016-11-12T19:19:25.711133: step 11986, loss 0.0316854, acc 0.96875
2016-11-12T19:19:25.771490: step 11987, loss 9.7166e-05, acc 1
2016-11-12T19:19:25.828134: step 11988, loss 3.05276e-06, acc 1
2016-11-12T19:19:25.884969: step 11989, loss 2.98895e-05, acc 1
2016-11-12T19:19:25.942993: step 11990, loss 9.26568e-06, acc 1
2016-11-12T19:19:25.999380: step 11991, loss 0.00541649, acc 1
2016-11-12T19:19:26.056931: step 11992, loss 0.000279434, acc 1
2016-11-12T19:19:26.115733: step 11993, loss 0.0100681, acc 1
2016-11-12T19:19:26.174104: step 11994, loss 0.0124506, acc 0.984375
2016-11-12T19:19:26.236637: step 11995, loss 0.000423043, acc 1
2016-11-12T19:19:26.294009: step 11996, loss 0.000331635, acc 1
2016-11-12T19:19:26.352124: step 11997, loss 0.0003322, acc 1
2016-11-12T19:19:26.412663: step 11998, loss 0.000949499, acc 1
2016-11-12T19:19:26.455930: step 11999, loss 4.41055e-06, acc 1
2016-11-12T19:19:26.516884: step 12000, loss 6.02979e-05, acc 1

Evaluation:
2016-11-12T19:19:26.588809: step 12000, loss 4.77816, acc 0.552

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-12000

2016-11-12T19:19:27.071799: step 12001, loss 3.02568e-05, acc 1
2016-11-12T19:19:27.128796: step 12002, loss 0.00396417, acc 1
2016-11-12T19:19:27.187921: step 12003, loss 0.00779983, acc 1
2016-11-12T19:19:27.247338: step 12004, loss 0.000270745, acc 1
2016-11-12T19:19:27.305375: step 12005, loss 6.60218e-06, acc 1
2016-11-12T19:19:27.363642: step 12006, loss 0.000258198, acc 1
2016-11-12T19:19:27.424755: step 12007, loss 0.00357514, acc 1
2016-11-12T19:19:27.487358: step 12008, loss 3.10982e-05, acc 1
2016-11-12T19:19:27.544791: step 12009, loss 0.000955042, acc 1
2016-11-12T19:19:27.605685: step 12010, loss 0.0107412, acc 1
2016-11-12T19:19:27.664733: step 12011, loss 0.000277921, acc 1
2016-11-12T19:19:27.722222: step 12012, loss 3.55053e-05, acc 1
2016-11-12T19:19:27.778949: step 12013, loss 0.000227783, acc 1
2016-11-12T19:19:27.835812: step 12014, loss 0.0463725, acc 0.984375
2016-11-12T19:19:27.896275: step 12015, loss 0.0118514, acc 0.984375
2016-11-12T19:19:27.957118: step 12016, loss 0.000147705, acc 1
2016-11-12T19:19:28.015019: step 12017, loss 0.0108922, acc 0.984375
2016-11-12T19:19:28.071546: step 12018, loss 9.38996e-06, acc 1
2016-11-12T19:19:28.128229: step 12019, loss 0.00175823, acc 1
2016-11-12T19:19:28.188082: step 12020, loss 0.0148402, acc 0.984375
2016-11-12T19:19:28.246643: step 12021, loss 0.00233468, acc 1
2016-11-12T19:19:28.305042: step 12022, loss 0.00101156, acc 1
2016-11-12T19:19:28.363516: step 12023, loss 0.000120599, acc 1
2016-11-12T19:19:28.420195: step 12024, loss 0.00270777, acc 1
2016-11-12T19:19:28.481044: step 12025, loss 0.0194514, acc 0.984375
2016-11-12T19:19:28.540478: step 12026, loss 0.000132266, acc 1
2016-11-12T19:19:28.600585: step 12027, loss 1.40072e-05, acc 1
2016-11-12T19:19:28.656458: step 12028, loss 6.68023e-05, acc 1
2016-11-12T19:19:28.714428: step 12029, loss 0.000143619, acc 1
2016-11-12T19:19:28.771866: step 12030, loss 0.0156263, acc 0.984375
2016-11-12T19:19:28.830189: step 12031, loss 0.000767886, acc 1
2016-11-12T19:19:28.888803: step 12032, loss 0.296312, acc 0.96875
2016-11-12T19:19:28.948994: step 12033, loss 0.000445236, acc 1
2016-11-12T19:19:29.008789: step 12034, loss 0.00229899, acc 1
2016-11-12T19:19:29.068321: step 12035, loss 6.70437e-05, acc 1
2016-11-12T19:19:29.128599: step 12036, loss 2.19039e-05, acc 1
2016-11-12T19:19:29.185768: step 12037, loss 0.0373772, acc 0.984375
2016-11-12T19:19:29.244176: step 12038, loss 3.00208e-05, acc 1
2016-11-12T19:19:29.304668: step 12039, loss 0.000614164, acc 1
2016-11-12T19:19:29.361304: step 12040, loss 0.00153188, acc 1
2016-11-12T19:19:29.421929: step 12041, loss 0.0117384, acc 0.984375
2016-11-12T19:19:29.480308: step 12042, loss 0.0180571, acc 0.984375
2016-11-12T19:19:29.539331: step 12043, loss 0.058154, acc 0.984375
2016-11-12T19:19:29.597175: step 12044, loss 0.0251079, acc 0.984375
2016-11-12T19:19:29.656808: step 12045, loss 0.0175036, acc 0.984375
2016-11-12T19:19:29.716569: step 12046, loss 6.44489e-05, acc 1
2016-11-12T19:19:29.773080: step 12047, loss 0.00945006, acc 1
2016-11-12T19:19:29.829643: step 12048, loss 5.4412e-05, acc 1
2016-11-12T19:19:29.885983: step 12049, loss 0.00399642, acc 1
2016-11-12T19:19:29.944927: step 12050, loss 5.35649e-06, acc 1
2016-11-12T19:19:30.004499: step 12051, loss 0.000116932, acc 1
2016-11-12T19:19:30.063959: step 12052, loss 0.000107583, acc 1
2016-11-12T19:19:30.122008: step 12053, loss 5.53622e-05, acc 1
2016-11-12T19:19:30.178468: step 12054, loss 0.000262457, acc 1
2016-11-12T19:19:30.235236: step 12055, loss 0.00551993, acc 1
2016-11-12T19:19:30.295476: step 12056, loss 0.0255605, acc 0.96875
2016-11-12T19:19:30.355688: step 12057, loss 0.0100078, acc 1
2016-11-12T19:19:30.413093: step 12058, loss 0.0150929, acc 0.984375
2016-11-12T19:19:30.474620: step 12059, loss 3.17759e-05, acc 1
2016-11-12T19:19:30.531470: step 12060, loss 0.0129353, acc 0.984375
2016-11-12T19:19:30.590121: step 12061, loss 0.0067047, acc 1
2016-11-12T19:19:30.648888: step 12062, loss 8.90271e-05, acc 1
2016-11-12T19:19:30.709025: step 12063, loss 0.000368722, acc 1
2016-11-12T19:19:30.769299: step 12064, loss 0.000655981, acc 1
2016-11-12T19:19:30.826828: step 12065, loss 0.000446738, acc 1
2016-11-12T19:19:30.883654: step 12066, loss 0.00190383, acc 1
2016-11-12T19:19:30.943066: step 12067, loss 0.000316206, acc 1
2016-11-12T19:19:31.000608: step 12068, loss 0.0042421, acc 1
2016-11-12T19:19:31.061494: step 12069, loss 0.000657316, acc 1
2016-11-12T19:19:31.101407: step 12070, loss 7.46585e-05, acc 1
2016-11-12T19:19:31.161004: step 12071, loss 0.0019029, acc 1
2016-11-12T19:19:31.221289: step 12072, loss 3.76282e-05, acc 1
2016-11-12T19:19:31.278867: step 12073, loss 3.73635e-06, acc 1
2016-11-12T19:19:31.334433: step 12074, loss 0.0132921, acc 0.984375
2016-11-12T19:19:31.393233: step 12075, loss 4.73046e-05, acc 1
2016-11-12T19:19:31.452569: step 12076, loss 0.000408271, acc 1
2016-11-12T19:19:31.510442: step 12077, loss 7.71328e-05, acc 1
2016-11-12T19:19:31.568137: step 12078, loss 0.000444652, acc 1
2016-11-12T19:19:31.627879: step 12079, loss 9.97366e-05, acc 1
2016-11-12T19:19:31.684146: step 12080, loss 0.000876397, acc 1
2016-11-12T19:19:31.742163: step 12081, loss 0.00806432, acc 1
2016-11-12T19:19:31.801392: step 12082, loss 3.70826e-06, acc 1
2016-11-12T19:19:31.859432: step 12083, loss 0.0115553, acc 1
2016-11-12T19:19:31.917754: step 12084, loss 0.000888652, acc 1
2016-11-12T19:19:31.976407: step 12085, loss 0.00261789, acc 1
2016-11-12T19:19:32.035183: step 12086, loss 0.00012934, acc 1
2016-11-12T19:19:32.093465: step 12087, loss 0.017675, acc 1
2016-11-12T19:19:32.154276: step 12088, loss 0.0220463, acc 0.984375
2016-11-12T19:19:32.211897: step 12089, loss 0.000308157, acc 1
2016-11-12T19:19:32.272840: step 12090, loss 0.00077892, acc 1
2016-11-12T19:19:32.333501: step 12091, loss 0.000396562, acc 1
2016-11-12T19:19:32.391967: step 12092, loss 0.0139898, acc 0.984375
2016-11-12T19:19:32.453530: step 12093, loss 2.26335e-05, acc 1
2016-11-12T19:19:32.510045: step 12094, loss 0.00176398, acc 1
2016-11-12T19:19:32.568283: step 12095, loss 0.00166809, acc 1
2016-11-12T19:19:32.628813: step 12096, loss 0.0546758, acc 0.96875
2016-11-12T19:19:32.690111: step 12097, loss 9.5167e-05, acc 1
2016-11-12T19:19:32.745662: step 12098, loss 7.48187e-05, acc 1
2016-11-12T19:19:32.801855: step 12099, loss 1.71548e-05, acc 1
2016-11-12T19:19:32.859663: step 12100, loss 0.000135777, acc 1

Evaluation:
2016-11-12T19:19:32.932268: step 12100, loss 4.84502, acc 0.532

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-12100

2016-11-12T19:19:33.414695: step 12101, loss 0.000114603, acc 1
2016-11-12T19:19:33.473590: step 12102, loss 0.202905, acc 0.96875
2016-11-12T19:19:33.532403: step 12103, loss 0.000130473, acc 1
2016-11-12T19:19:33.592666: step 12104, loss 0.00024708, acc 1
2016-11-12T19:19:33.650126: step 12105, loss 0.000548339, acc 1
2016-11-12T19:19:33.708376: step 12106, loss 0.000926793, acc 1
2016-11-12T19:19:33.768224: step 12107, loss 0.0277111, acc 0.984375
2016-11-12T19:19:33.828788: step 12108, loss 0.0103003, acc 1
2016-11-12T19:19:33.885784: step 12109, loss 0.0155903, acc 0.984375
2016-11-12T19:19:33.949558: step 12110, loss 0.0102223, acc 1
2016-11-12T19:19:34.008713: step 12111, loss 0.00656036, acc 1
2016-11-12T19:19:34.068597: step 12112, loss 0.0041551, acc 1
2016-11-12T19:19:34.125890: step 12113, loss 0.000272214, acc 1
2016-11-12T19:19:34.185414: step 12114, loss 0.00410199, acc 1
2016-11-12T19:19:34.245527: step 12115, loss 0.0292061, acc 0.96875
2016-11-12T19:19:34.305583: step 12116, loss 0.0252772, acc 0.984375
2016-11-12T19:19:34.362990: step 12117, loss 0.0037112, acc 1
2016-11-12T19:19:34.420623: step 12118, loss 0.0113044, acc 1
2016-11-12T19:19:34.481863: step 12119, loss 1.92455e-05, acc 1
2016-11-12T19:19:34.537499: step 12120, loss 0.00151354, acc 1
2016-11-12T19:19:34.596901: step 12121, loss 0.00680893, acc 1
2016-11-12T19:19:34.655197: step 12122, loss 0.000276437, acc 1
2016-11-12T19:19:34.716044: step 12123, loss 5.48504e-06, acc 1
2016-11-12T19:19:34.775640: step 12124, loss 0.0215852, acc 0.984375
2016-11-12T19:19:34.835844: step 12125, loss 0.000787786, acc 1
2016-11-12T19:19:34.894627: step 12126, loss 2.28165e-05, acc 1
2016-11-12T19:19:34.951727: step 12127, loss 0.0120338, acc 0.984375
2016-11-12T19:19:35.010025: step 12128, loss 0.000135321, acc 1
2016-11-12T19:19:35.067489: step 12129, loss 2.55302e-05, acc 1
2016-11-12T19:19:35.124184: step 12130, loss 8.30609e-06, acc 1
2016-11-12T19:19:35.180102: step 12131, loss 0.0107451, acc 1
2016-11-12T19:19:35.239287: step 12132, loss 0.000653305, acc 1
2016-11-12T19:19:35.296241: step 12133, loss 0.0113842, acc 0.984375
2016-11-12T19:19:35.355941: step 12134, loss 0.0166729, acc 1
2016-11-12T19:19:35.414234: step 12135, loss 0.00010694, acc 1
2016-11-12T19:19:35.470694: step 12136, loss 0.000648224, acc 1
2016-11-12T19:19:35.528791: step 12137, loss 0.00249665, acc 1
2016-11-12T19:19:35.588725: step 12138, loss 0.000209191, acc 1
2016-11-12T19:19:35.648985: step 12139, loss 0.000329819, acc 1
2016-11-12T19:19:35.705629: step 12140, loss 0.00181914, acc 1
2016-11-12T19:19:35.748660: step 12141, loss 0.000114558, acc 1
2016-11-12T19:19:35.807326: step 12142, loss 0.0101136, acc 1
2016-11-12T19:19:35.864837: step 12143, loss 0.000884347, acc 1
2016-11-12T19:19:35.925636: step 12144, loss 0.0110077, acc 1
2016-11-12T19:19:35.984436: step 12145, loss 0.0124693, acc 0.984375
2016-11-12T19:19:36.042284: step 12146, loss 0.0100988, acc 1
2016-11-12T19:19:36.101263: step 12147, loss 0.00851175, acc 1
2016-11-12T19:19:36.158447: step 12148, loss 0.000219043, acc 1
2016-11-12T19:19:36.215621: step 12149, loss 5.85204e-05, acc 1
2016-11-12T19:19:36.272222: step 12150, loss 0.000319428, acc 1
2016-11-12T19:19:36.328587: step 12151, loss 4.95394e-05, acc 1
2016-11-12T19:19:36.384982: step 12152, loss 0.00450375, acc 1
2016-11-12T19:19:36.443909: step 12153, loss 0.000388614, acc 1
2016-11-12T19:19:36.504134: step 12154, loss 5.16904e-05, acc 1
2016-11-12T19:19:36.564706: step 12155, loss 3.16593e-05, acc 1
2016-11-12T19:19:36.622136: step 12156, loss 3.97203e-05, acc 1
2016-11-12T19:19:36.680997: step 12157, loss 0.000132514, acc 1
2016-11-12T19:19:36.738974: step 12158, loss 0.0162888, acc 0.984375
2016-11-12T19:19:36.797569: step 12159, loss 0.000598952, acc 1
2016-11-12T19:19:36.854982: step 12160, loss 0.000579247, acc 1
2016-11-12T19:19:36.912294: step 12161, loss 0.0187799, acc 0.984375
2016-11-12T19:19:36.972855: step 12162, loss 0.000894083, acc 1
2016-11-12T19:19:37.032770: step 12163, loss 0.0184433, acc 0.984375
2016-11-12T19:19:37.091325: step 12164, loss 0.0307454, acc 0.984375
2016-11-12T19:19:37.152666: step 12165, loss 0.00193145, acc 1
2016-11-12T19:19:37.211739: step 12166, loss 0.0146612, acc 0.984375
2016-11-12T19:19:37.270466: step 12167, loss 0.0100352, acc 1
2016-11-12T19:19:37.328594: step 12168, loss 0.0060119, acc 1
2016-11-12T19:19:37.388341: step 12169, loss 0.00012312, acc 1
2016-11-12T19:19:37.446828: step 12170, loss 0.000131474, acc 1
2016-11-12T19:19:37.504589: step 12171, loss 0.000316016, acc 1
2016-11-12T19:19:37.563529: step 12172, loss 4.08458e-06, acc 1
2016-11-12T19:19:37.619193: step 12173, loss 0.00376579, acc 1
2016-11-12T19:19:37.678844: step 12174, loss 0.0183871, acc 0.984375
2016-11-12T19:19:37.737178: step 12175, loss 0.2528, acc 0.984375
2016-11-12T19:19:37.796713: step 12176, loss 2.12782e-05, acc 1
2016-11-12T19:19:37.854872: step 12177, loss 0.00453554, acc 1
2016-11-12T19:19:37.913436: step 12178, loss 0.000405473, acc 1
2016-11-12T19:19:37.971332: step 12179, loss 0.0895863, acc 0.984375
2016-11-12T19:19:38.028472: step 12180, loss 0.000300484, acc 1
2016-11-12T19:19:38.088639: step 12181, loss 0.000453845, acc 1
2016-11-12T19:19:38.148323: step 12182, loss 6.44566e-05, acc 1
2016-11-12T19:19:38.206371: step 12183, loss 0.00315739, acc 1
2016-11-12T19:19:38.264161: step 12184, loss 8.15253e-05, acc 1
2016-11-12T19:19:38.320910: step 12185, loss 0.0386193, acc 0.984375
2016-11-12T19:19:38.380623: step 12186, loss 0.000350305, acc 1
2016-11-12T19:19:38.438561: step 12187, loss 6.60955e-05, acc 1
2016-11-12T19:19:38.495228: step 12188, loss 0.0106477, acc 1
2016-11-12T19:19:38.555262: step 12189, loss 0.000636583, acc 1
2016-11-12T19:19:38.613089: step 12190, loss 0.0286729, acc 0.984375
2016-11-12T19:19:38.672327: step 12191, loss 0.018815, acc 0.984375
2016-11-12T19:19:38.732680: step 12192, loss 3.06069e-05, acc 1
2016-11-12T19:19:38.789608: step 12193, loss 0.000100442, acc 1
2016-11-12T19:19:38.847608: step 12194, loss 0.0188899, acc 0.984375
2016-11-12T19:19:38.908489: step 12195, loss 0.00148342, acc 1
2016-11-12T19:19:38.966424: step 12196, loss 0.0121762, acc 1
2016-11-12T19:19:39.024983: step 12197, loss 3.9673e-05, acc 1
2016-11-12T19:19:39.083672: step 12198, loss 0.00762006, acc 1
2016-11-12T19:19:39.144943: step 12199, loss 3.62798e-05, acc 1
2016-11-12T19:19:39.204054: step 12200, loss 0.0106972, acc 1

Evaluation:
2016-11-12T19:19:39.277597: step 12200, loss 4.88028, acc 0.542

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-12200

2016-11-12T19:19:39.763039: step 12201, loss 0.000346196, acc 1
2016-11-12T19:19:39.820634: step 12202, loss 0.0579499, acc 0.984375
2016-11-12T19:19:39.881083: step 12203, loss 0.000167929, acc 1
2016-11-12T19:19:39.939312: step 12204, loss 0.000195699, acc 1
2016-11-12T19:19:39.996391: step 12205, loss 0.000163032, acc 1
2016-11-12T19:19:40.056072: step 12206, loss 0.0144471, acc 0.984375
2016-11-12T19:19:40.114287: step 12207, loss 0.00846911, acc 1
2016-11-12T19:19:40.172127: step 12208, loss 0.0208487, acc 1
2016-11-12T19:19:40.231277: step 12209, loss 0.00471416, acc 1
2016-11-12T19:19:40.288713: step 12210, loss 8.36166e-05, acc 1
2016-11-12T19:19:40.346168: step 12211, loss 0.000311047, acc 1
2016-11-12T19:19:40.386642: step 12212, loss 0.00042873, acc 1
2016-11-12T19:19:40.445710: step 12213, loss 1.28935e-05, acc 1
2016-11-12T19:19:40.504119: step 12214, loss 0.000405344, acc 1
2016-11-12T19:19:40.562000: step 12215, loss 0.0423611, acc 0.984375
2016-11-12T19:19:40.624285: step 12216, loss 0.00326466, acc 1
2016-11-12T19:19:40.680851: step 12217, loss 0.00689019, acc 1
2016-11-12T19:19:40.740242: step 12218, loss 0.0189537, acc 0.984375
2016-11-12T19:19:40.799521: step 12219, loss 0.000722854, acc 1
2016-11-12T19:19:40.860068: step 12220, loss 0.000202774, acc 1
2016-11-12T19:19:40.916335: step 12221, loss 0.00583686, acc 1
2016-11-12T19:19:40.976377: step 12222, loss 0.0022115, acc 1
2016-11-12T19:19:41.033557: step 12223, loss 7.70252e-06, acc 1
2016-11-12T19:19:41.091101: step 12224, loss 6.39639e-05, acc 1
2016-11-12T19:19:41.148676: step 12225, loss 1.15439e-05, acc 1
2016-11-12T19:19:41.206022: step 12226, loss 0.000110626, acc 1
2016-11-12T19:19:41.264356: step 12227, loss 0.0086562, acc 1
2016-11-12T19:19:41.325179: step 12228, loss 0.0203402, acc 0.984375
2016-11-12T19:19:41.384325: step 12229, loss 0.0247282, acc 0.984375
2016-11-12T19:19:41.444825: step 12230, loss 0.0138765, acc 0.984375
2016-11-12T19:19:41.505001: step 12231, loss 1.82901e-05, acc 1
2016-11-12T19:19:41.562608: step 12232, loss 2.87958e-06, acc 1
2016-11-12T19:19:41.618118: step 12233, loss 1.51273e-05, acc 1
2016-11-12T19:19:41.674813: step 12234, loss 0.0132264, acc 1
2016-11-12T19:19:41.736213: step 12235, loss 0.00428368, acc 1
2016-11-12T19:19:41.792926: step 12236, loss 0.0137302, acc 0.984375
2016-11-12T19:19:41.852482: step 12237, loss 4.85725e-06, acc 1
2016-11-12T19:19:41.910040: step 12238, loss 3.33226e-05, acc 1
2016-11-12T19:19:41.967167: step 12239, loss 0.00266726, acc 1
2016-11-12T19:19:42.028065: step 12240, loss 7.66506e-05, acc 1
2016-11-12T19:19:42.086308: step 12241, loss 1.5384e-05, acc 1
2016-11-12T19:19:42.142592: step 12242, loss 0.0535802, acc 0.96875
2016-11-12T19:19:42.200282: step 12243, loss 0.000198424, acc 1
2016-11-12T19:19:42.261773: step 12244, loss 0.000991473, acc 1
2016-11-12T19:19:42.322204: step 12245, loss 0.000272795, acc 1
2016-11-12T19:19:42.381230: step 12246, loss 3.58082e-05, acc 1
2016-11-12T19:19:42.440748: step 12247, loss 2.39724e-05, acc 1
2016-11-12T19:19:42.496934: step 12248, loss 0.000811143, acc 1
2016-11-12T19:19:42.555652: step 12249, loss 0.000425404, acc 1
2016-11-12T19:19:42.613630: step 12250, loss 0.000146971, acc 1
2016-11-12T19:19:42.673063: step 12251, loss 0.00033044, acc 1
2016-11-12T19:19:42.732457: step 12252, loss 0.0506321, acc 0.984375
2016-11-12T19:19:42.790517: step 12253, loss 0.025955, acc 0.984375
2016-11-12T19:19:42.851652: step 12254, loss 0.000123762, acc 1
2016-11-12T19:19:42.908168: step 12255, loss 0.00282335, acc 1
2016-11-12T19:19:42.965915: step 12256, loss 0.0131537, acc 1
2016-11-12T19:19:43.023311: step 12257, loss 6.8597e-05, acc 1
2016-11-12T19:19:43.081178: step 12258, loss 0.0124033, acc 0.984375
2016-11-12T19:19:43.140257: step 12259, loss 4.95778e-05, acc 1
2016-11-12T19:19:43.197927: step 12260, loss 0.00684222, acc 1
2016-11-12T19:19:43.256348: step 12261, loss 0.00214322, acc 1
2016-11-12T19:19:43.316335: step 12262, loss 0.0115294, acc 0.984375
2016-11-12T19:19:43.373919: step 12263, loss 0.0185365, acc 0.984375
2016-11-12T19:19:43.433743: step 12264, loss 2.27181e-05, acc 1
2016-11-12T19:19:43.492876: step 12265, loss 0.278958, acc 0.984375
2016-11-12T19:19:43.552881: step 12266, loss 0.0012587, acc 1
2016-11-12T19:19:43.612860: step 12267, loss 0.00066981, acc 1
2016-11-12T19:19:43.673990: step 12268, loss 0.00691331, acc 1
2016-11-12T19:19:43.735213: step 12269, loss 0.0119529, acc 0.984375
2016-11-12T19:19:43.794750: step 12270, loss 0.0223378, acc 0.984375
2016-11-12T19:19:43.854147: step 12271, loss 0.000556494, acc 1
2016-11-12T19:19:43.913075: step 12272, loss 0.0122841, acc 1
2016-11-12T19:19:43.970413: step 12273, loss 0.00018387, acc 1
2016-11-12T19:19:44.028998: step 12274, loss 0.0274083, acc 0.984375
2016-11-12T19:19:44.088790: step 12275, loss 3.6995e-05, acc 1
2016-11-12T19:19:44.146358: step 12276, loss 0.00480788, acc 1
2016-11-12T19:19:44.203391: step 12277, loss 0.00121707, acc 1
2016-11-12T19:19:44.264018: step 12278, loss 8.52944e-05, acc 1
2016-11-12T19:19:44.321250: step 12279, loss 0.000570883, acc 1
2016-11-12T19:19:44.381021: step 12280, loss 9.37121e-05, acc 1
2016-11-12T19:19:44.440546: step 12281, loss 0.00331365, acc 1
2016-11-12T19:19:44.499974: step 12282, loss 4.13537e-05, acc 1
2016-11-12T19:19:44.537952: step 12283, loss 1.99076e-06, acc 1
2016-11-12T19:19:44.596925: step 12284, loss 0.000589013, acc 1
2016-11-12T19:19:44.654697: step 12285, loss 0.029608, acc 0.984375
2016-11-12T19:19:44.713316: step 12286, loss 1.55397e-05, acc 1
2016-11-12T19:19:44.770643: step 12287, loss 0.00558282, acc 1
2016-11-12T19:19:44.828160: step 12288, loss 0.00357897, acc 1
2016-11-12T19:19:44.888432: step 12289, loss 0.0125313, acc 1
2016-11-12T19:19:44.948441: step 12290, loss 0.000234165, acc 1
2016-11-12T19:19:45.006115: step 12291, loss 0.0304561, acc 0.984375
2016-11-12T19:19:45.064441: step 12292, loss 0.000456067, acc 1
2016-11-12T19:19:45.122918: step 12293, loss 1.45903e-05, acc 1
2016-11-12T19:19:45.180856: step 12294, loss 6.63289e-05, acc 1
2016-11-12T19:19:45.237252: step 12295, loss 2.67403e-05, acc 1
2016-11-12T19:19:45.296619: step 12296, loss 0.000786879, acc 1
2016-11-12T19:19:45.355254: step 12297, loss 9.63704e-05, acc 1
2016-11-12T19:19:45.412521: step 12298, loss 5.89553e-05, acc 1
2016-11-12T19:19:45.470003: step 12299, loss 2.56063e-05, acc 1
2016-11-12T19:19:45.527780: step 12300, loss 0.000220746, acc 1

Evaluation:
2016-11-12T19:19:45.600334: step 12300, loss 4.83564, acc 0.534

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-12300

2016-11-12T19:19:46.080745: step 12301, loss 0.00932013, acc 1
2016-11-12T19:19:46.143957: step 12302, loss 9.35522e-05, acc 1
2016-11-12T19:19:46.199856: step 12303, loss 1.91419e-05, acc 1
2016-11-12T19:19:46.267116: step 12304, loss 0.000100833, acc 1
2016-11-12T19:19:46.326750: step 12305, loss 5.26697e-05, acc 1
2016-11-12T19:19:46.384861: step 12306, loss 8.70068e-05, acc 1
2016-11-12T19:19:46.444833: step 12307, loss 0.000248568, acc 1
2016-11-12T19:19:46.503772: step 12308, loss 0.000848185, acc 1
2016-11-12T19:19:46.562223: step 12309, loss 0.0108241, acc 1
2016-11-12T19:19:46.620179: step 12310, loss 0.0180486, acc 1
2016-11-12T19:19:46.678042: step 12311, loss 0.00108909, acc 1
2016-11-12T19:19:46.737105: step 12312, loss 0.00172836, acc 1
2016-11-12T19:19:46.797104: step 12313, loss 2.14945e-06, acc 1
2016-11-12T19:19:46.852715: step 12314, loss 0.00129454, acc 1
2016-11-12T19:19:46.911209: step 12315, loss 0.00773432, acc 1
2016-11-12T19:19:46.968424: step 12316, loss 0.0398924, acc 0.984375
2016-11-12T19:19:47.028462: step 12317, loss 0.0465005, acc 0.984375
2016-11-12T19:19:47.088144: step 12318, loss 0.00982719, acc 1
2016-11-12T19:19:47.148653: step 12319, loss 0.000625075, acc 1
2016-11-12T19:19:47.206565: step 12320, loss 0.0153735, acc 1
2016-11-12T19:19:47.265625: step 12321, loss 0.00031264, acc 1
2016-11-12T19:19:47.324707: step 12322, loss 0.00366702, acc 1
2016-11-12T19:19:47.382026: step 12323, loss 0.00485184, acc 1
2016-11-12T19:19:47.439687: step 12324, loss 6.10369e-05, acc 1
2016-11-12T19:19:47.496115: step 12325, loss 5.88628e-05, acc 1
2016-11-12T19:19:47.552918: step 12326, loss 0.0102879, acc 1
2016-11-12T19:19:47.610848: step 12327, loss 0.000203322, acc 1
2016-11-12T19:19:47.669385: step 12328, loss 3.22308e-05, acc 1
2016-11-12T19:19:47.726427: step 12329, loss 0.00258221, acc 1
2016-11-12T19:19:47.788342: step 12330, loss 0.0154327, acc 0.984375
2016-11-12T19:19:47.849008: step 12331, loss 3.77276e-05, acc 1
2016-11-12T19:19:47.908116: step 12332, loss 0.0213649, acc 0.984375
2016-11-12T19:19:47.967622: step 12333, loss 0.0039242, acc 1
2016-11-12T19:19:48.026030: step 12334, loss 0.00175667, acc 1
2016-11-12T19:19:48.084214: step 12335, loss 0.000220188, acc 1
2016-11-12T19:19:48.142424: step 12336, loss 2.43745e-05, acc 1
2016-11-12T19:19:48.200293: step 12337, loss 0.000667927, acc 1
2016-11-12T19:19:48.260105: step 12338, loss 9.69916e-05, acc 1
2016-11-12T19:19:48.318099: step 12339, loss 0.0329476, acc 0.984375
2016-11-12T19:19:48.377148: step 12340, loss 6.22445e-05, acc 1
2016-11-12T19:19:48.436863: step 12341, loss 0.000485511, acc 1
2016-11-12T19:19:48.494619: step 12342, loss 0.10499, acc 0.984375
2016-11-12T19:19:48.553301: step 12343, loss 0.00207947, acc 1
2016-11-12T19:19:48.612342: step 12344, loss 0.000220513, acc 1
2016-11-12T19:19:48.667877: step 12345, loss 4.3964e-05, acc 1
2016-11-12T19:19:48.723868: step 12346, loss 0.00869099, acc 1
2016-11-12T19:19:48.783558: step 12347, loss 7.80456e-05, acc 1
2016-11-12T19:19:48.841140: step 12348, loss 0.0113343, acc 0.984375
2016-11-12T19:19:48.900106: step 12349, loss 7.79207e-05, acc 1
2016-11-12T19:19:48.959099: step 12350, loss 0.0123055, acc 0.984375
2016-11-12T19:19:49.017259: step 12351, loss 0.0116823, acc 0.984375
2016-11-12T19:19:49.077060: step 12352, loss 0.000181259, acc 1
2016-11-12T19:19:49.136243: step 12353, loss 0.008031, acc 1
2016-11-12T19:19:49.176604: step 12354, loss 8.4039e-06, acc 1
2016-11-12T19:19:49.236323: step 12355, loss 0.0116489, acc 0.984375
2016-11-12T19:19:49.295223: step 12356, loss 3.61137e-05, acc 1
2016-11-12T19:19:49.351925: step 12357, loss 2.33567e-05, acc 1
2016-11-12T19:19:49.408862: step 12358, loss 5.57555e-05, acc 1
2016-11-12T19:19:49.468207: step 12359, loss 5.77142e-05, acc 1
2016-11-12T19:19:49.525413: step 12360, loss 0.00334, acc 1
2016-11-12T19:19:49.584235: step 12361, loss 0.0233364, acc 0.984375
2016-11-12T19:19:49.643129: step 12362, loss 6.24912e-05, acc 1
2016-11-12T19:19:49.700782: step 12363, loss 1.2843e-05, acc 1
2016-11-12T19:19:49.756923: step 12364, loss 0.0115, acc 0.984375
2016-11-12T19:19:49.815769: step 12365, loss 0.00026182, acc 1
2016-11-12T19:19:49.876703: step 12366, loss 4.35474e-06, acc 1
2016-11-12T19:19:49.933200: step 12367, loss 0.000150163, acc 1
2016-11-12T19:19:49.990860: step 12368, loss 0.000286471, acc 1
2016-11-12T19:19:50.048149: step 12369, loss 0.000395992, acc 1
2016-11-12T19:19:50.106104: step 12370, loss 0.0343391, acc 0.984375
2016-11-12T19:19:50.165608: step 12371, loss 0.000310921, acc 1
2016-11-12T19:19:50.222638: step 12372, loss 9.23024e-05, acc 1
2016-11-12T19:19:50.280646: step 12373, loss 0.0227811, acc 0.96875
2016-11-12T19:19:50.338179: step 12374, loss 0.0145566, acc 0.984375
2016-11-12T19:19:50.396908: step 12375, loss 0.0134671, acc 0.984375
2016-11-12T19:19:50.456587: step 12376, loss 0.000185104, acc 1
2016-11-12T19:19:50.513996: step 12377, loss 2.84791e-06, acc 1
2016-11-12T19:19:50.573165: step 12378, loss 5.38338e-05, acc 1
2016-11-12T19:19:50.630082: step 12379, loss 0.00436803, acc 1
2016-11-12T19:19:50.688140: step 12380, loss 0.00118828, acc 1
2016-11-12T19:19:50.747822: step 12381, loss 6.03225e-05, acc 1
2016-11-12T19:19:50.806335: step 12382, loss 1.3522e-05, acc 1
2016-11-12T19:19:50.864534: step 12383, loss 0.000100749, acc 1
2016-11-12T19:19:50.922981: step 12384, loss 0.0311429, acc 0.96875
2016-11-12T19:19:50.980579: step 12385, loss 0.000207457, acc 1
2016-11-12T19:19:51.040738: step 12386, loss 0.00110573, acc 1
2016-11-12T19:19:51.102118: step 12387, loss 0.00475736, acc 1
2016-11-12T19:19:51.161025: step 12388, loss 0.0167125, acc 0.984375
2016-11-12T19:19:51.219364: step 12389, loss 0.0177239, acc 0.984375
2016-11-12T19:19:51.278153: step 12390, loss 0.00249797, acc 1
2016-11-12T19:19:51.336631: step 12391, loss 0.00661067, acc 1
2016-11-12T19:19:51.396210: step 12392, loss 0.0127588, acc 1
2016-11-12T19:19:51.456994: step 12393, loss 0.000446438, acc 1
2016-11-12T19:19:51.514931: step 12394, loss 0.0105886, acc 1
2016-11-12T19:19:51.572798: step 12395, loss 0.000134426, acc 1
2016-11-12T19:19:51.629598: step 12396, loss 0.0080451, acc 1
2016-11-12T19:19:51.687936: step 12397, loss 0.0173591, acc 0.984375
2016-11-12T19:19:51.745960: step 12398, loss 0.000160922, acc 1
2016-11-12T19:19:51.804209: step 12399, loss 0.00013465, acc 1
2016-11-12T19:19:51.860893: step 12400, loss 0.0175557, acc 0.984375

Evaluation:
2016-11-12T19:19:51.932368: step 12400, loss 4.86077, acc 0.536

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-12400

2016-11-12T19:19:52.413615: step 12401, loss 0.00100418, acc 1
2016-11-12T19:19:52.471654: step 12402, loss 0.000574703, acc 1
2016-11-12T19:19:52.530694: step 12403, loss 0.0053739, acc 1
2016-11-12T19:19:52.588820: step 12404, loss 0.000752114, acc 1
2016-11-12T19:19:52.649592: step 12405, loss 0.00994971, acc 1
2016-11-12T19:19:52.709672: step 12406, loss 0.0349719, acc 0.984375
2016-11-12T19:19:52.770407: step 12407, loss 0.000629627, acc 1
2016-11-12T19:19:52.827446: step 12408, loss 0.00610519, acc 1
2016-11-12T19:19:52.886841: step 12409, loss 0.000109137, acc 1
2016-11-12T19:19:52.944838: step 12410, loss 0.0103403, acc 1
2016-11-12T19:19:53.005418: step 12411, loss 0.035468, acc 0.984375
2016-11-12T19:19:53.063507: step 12412, loss 0.000181355, acc 1
2016-11-12T19:19:53.122430: step 12413, loss 0.0117168, acc 0.984375
2016-11-12T19:19:53.180693: step 12414, loss 4.21401e-05, acc 1
2016-11-12T19:19:53.239176: step 12415, loss 0.000269794, acc 1
2016-11-12T19:19:53.298138: step 12416, loss 0.00745624, acc 1
2016-11-12T19:19:53.355670: step 12417, loss 0.000226548, acc 1
2016-11-12T19:19:53.414990: step 12418, loss 0.000728264, acc 1
2016-11-12T19:19:53.471404: step 12419, loss 1.2711e-05, acc 1
2016-11-12T19:19:53.527300: step 12420, loss 1.83107e-05, acc 1
2016-11-12T19:19:53.585045: step 12421, loss 0.00189907, acc 1
2016-11-12T19:19:53.644308: step 12422, loss 0.163065, acc 0.984375
2016-11-12T19:19:53.703161: step 12423, loss 0.0150946, acc 1
2016-11-12T19:19:53.761146: step 12424, loss 0.0105894, acc 1
2016-11-12T19:19:53.802271: step 12425, loss 1.20097e-05, acc 1
2016-11-12T19:19:53.859306: step 12426, loss 0.000649125, acc 1
2016-11-12T19:19:53.916577: step 12427, loss 0.0106466, acc 1
2016-11-12T19:19:53.977551: step 12428, loss 0.00248324, acc 1
2016-11-12T19:19:54.038090: step 12429, loss 0.0146065, acc 0.984375
2016-11-12T19:19:54.096583: step 12430, loss 5.55579e-05, acc 1
2016-11-12T19:19:54.153792: step 12431, loss 3.14732e-05, acc 1
2016-11-12T19:19:54.211090: step 12432, loss 0.0011416, acc 1
2016-11-12T19:19:54.271087: step 12433, loss 0.000204929, acc 1
2016-11-12T19:19:54.329157: step 12434, loss 0.00474305, acc 1
2016-11-12T19:19:54.388801: step 12435, loss 0.000650934, acc 1
2016-11-12T19:19:54.448537: step 12436, loss 0.0105453, acc 1
2016-11-12T19:19:54.508875: step 12437, loss 2.01663e-05, acc 1
2016-11-12T19:19:54.572375: step 12438, loss 0.00926234, acc 1
2016-11-12T19:19:54.634733: step 12439, loss 6.42458e-05, acc 1
2016-11-12T19:19:54.692891: step 12440, loss 0.0318089, acc 0.96875
2016-11-12T19:19:54.752717: step 12441, loss 0.0351599, acc 0.96875
2016-11-12T19:19:54.811103: step 12442, loss 0.0115463, acc 1
2016-11-12T19:19:54.872473: step 12443, loss 0.0104543, acc 1
2016-11-12T19:19:54.930105: step 12444, loss 1.46206e-05, acc 1
2016-11-12T19:19:54.988687: step 12445, loss 2.55438e-05, acc 1
2016-11-12T19:19:55.044614: step 12446, loss 0.0198186, acc 0.984375
2016-11-12T19:19:55.103112: step 12447, loss 0.0123562, acc 0.984375
2016-11-12T19:19:55.159883: step 12448, loss 0.0140733, acc 0.984375
2016-11-12T19:19:55.217533: step 12449, loss 0.000187766, acc 1
2016-11-12T19:19:55.276363: step 12450, loss 0.00564166, acc 1
2016-11-12T19:19:55.337042: step 12451, loss 0.000154458, acc 1
2016-11-12T19:19:55.395188: step 12452, loss 0.00533177, acc 1
2016-11-12T19:19:55.453102: step 12453, loss 0.0167248, acc 0.984375
2016-11-12T19:19:55.511292: step 12454, loss 0.000186613, acc 1
2016-11-12T19:19:55.568318: step 12455, loss 7.49623e-05, acc 1
2016-11-12T19:19:55.624586: step 12456, loss 0.0183736, acc 0.984375
2016-11-12T19:19:55.683185: step 12457, loss 0.000832207, acc 1
2016-11-12T19:19:55.744764: step 12458, loss 2.50709e-05, acc 1
2016-11-12T19:19:55.800966: step 12459, loss 8.85939e-05, acc 1
2016-11-12T19:19:55.860805: step 12460, loss 9.32241e-05, acc 1
2016-11-12T19:19:55.917654: step 12461, loss 1.88992e-05, acc 1
2016-11-12T19:19:55.975919: step 12462, loss 0.00717039, acc 1
2016-11-12T19:19:56.033827: step 12463, loss 0.0497574, acc 0.984375
2016-11-12T19:19:56.092544: step 12464, loss 5.29329e-06, acc 1
2016-11-12T19:19:56.149363: step 12465, loss 0.00761516, acc 1
2016-11-12T19:19:56.208030: step 12466, loss 0.00097634, acc 1
2016-11-12T19:19:56.267227: step 12467, loss 0.00151029, acc 1
2016-11-12T19:19:56.324891: step 12468, loss 0.00227647, acc 1
2016-11-12T19:19:56.382000: step 12469, loss 0.0142705, acc 1
2016-11-12T19:19:56.440505: step 12470, loss 2.24967e-05, acc 1
2016-11-12T19:19:56.500457: step 12471, loss 0.0140744, acc 0.984375
2016-11-12T19:19:56.558996: step 12472, loss 0.00014111, acc 1
2016-11-12T19:19:56.616272: step 12473, loss 0.044597, acc 0.984375
2016-11-12T19:19:56.675315: step 12474, loss 0.0137553, acc 0.984375
2016-11-12T19:19:56.736238: step 12475, loss 0.000216723, acc 1
2016-11-12T19:19:56.792514: step 12476, loss 0.00727654, acc 1
2016-11-12T19:19:56.852853: step 12477, loss 0.000585432, acc 1
2016-11-12T19:19:56.910127: step 12478, loss 0.000179656, acc 1
2016-11-12T19:19:56.967827: step 12479, loss 0.00145076, acc 1
2016-11-12T19:19:57.028782: step 12480, loss 0.0206486, acc 0.984375
2016-11-12T19:19:57.088796: step 12481, loss 0.0441729, acc 0.984375
2016-11-12T19:19:57.149201: step 12482, loss 0.00474984, acc 1
2016-11-12T19:19:57.209234: step 12483, loss 1.25523e-05, acc 1
2016-11-12T19:19:57.265711: step 12484, loss 0.00594701, acc 1
2016-11-12T19:19:57.324815: step 12485, loss 0.0253959, acc 0.984375
2016-11-12T19:19:57.383218: step 12486, loss 0.000117178, acc 1
2016-11-12T19:19:57.441020: step 12487, loss 0.0385079, acc 0.984375
2016-11-12T19:19:57.499677: step 12488, loss 0.000219985, acc 1
2016-11-12T19:19:57.557175: step 12489, loss 0.000174429, acc 1
2016-11-12T19:19:57.614464: step 12490, loss 0.00446188, acc 1
2016-11-12T19:19:57.673313: step 12491, loss 0.000657047, acc 1
2016-11-12T19:19:57.731790: step 12492, loss 0.000779334, acc 1
2016-11-12T19:19:57.789639: step 12493, loss 0.000116603, acc 1
2016-11-12T19:19:57.848837: step 12494, loss 0.00102785, acc 1
2016-11-12T19:19:57.908758: step 12495, loss 0.000202068, acc 1
2016-11-12T19:19:57.947973: step 12496, loss 0.000312708, acc 1
2016-11-12T19:19:58.007367: step 12497, loss 0.0346041, acc 0.984375
2016-11-12T19:19:58.066396: step 12498, loss 0.00041272, acc 1
2016-11-12T19:19:58.124326: step 12499, loss 6.41108e-05, acc 1
2016-11-12T19:19:58.180958: step 12500, loss 0.000519009, acc 1

Evaluation:
2016-11-12T19:19:58.253902: step 12500, loss 5.05533, acc 0.536

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-12500

2016-11-12T19:19:58.737958: step 12501, loss 0.0020842, acc 1
2016-11-12T19:19:58.795947: step 12502, loss 0.000348645, acc 1
2016-11-12T19:19:58.855092: step 12503, loss 0.000115086, acc 1
2016-11-12T19:19:58.912137: step 12504, loss 0.00113697, acc 1
2016-11-12T19:19:58.972462: step 12505, loss 0.000569344, acc 1
2016-11-12T19:19:59.032878: step 12506, loss 0.0153097, acc 1
2016-11-12T19:19:59.090208: step 12507, loss 0.000293292, acc 1
2016-11-12T19:19:59.147601: step 12508, loss 0.0342055, acc 0.96875
2016-11-12T19:19:59.205952: step 12509, loss 0.000164262, acc 1
2016-11-12T19:19:59.263836: step 12510, loss 0.000345831, acc 1
2016-11-12T19:19:59.322728: step 12511, loss 0.000574466, acc 1
2016-11-12T19:19:59.381242: step 12512, loss 0.00936245, acc 1
2016-11-12T19:19:59.440108: step 12513, loss 8.72999e-06, acc 1
2016-11-12T19:19:59.496022: step 12514, loss 0.0254644, acc 0.984375
2016-11-12T19:19:59.557112: step 12515, loss 0.0088428, acc 1
2016-11-12T19:19:59.627785: step 12516, loss 3.25957e-05, acc 1
2016-11-12T19:19:59.685660: step 12517, loss 0.00508388, acc 1
2016-11-12T19:19:59.742547: step 12518, loss 0.012305, acc 0.984375
2016-11-12T19:19:59.801089: step 12519, loss 0.00125789, acc 1
2016-11-12T19:19:59.857898: step 12520, loss 0.000356958, acc 1
2016-11-12T19:19:59.916828: step 12521, loss 0.000220196, acc 1
2016-11-12T19:19:59.975850: step 12522, loss 0.000616051, acc 1
2016-11-12T19:20:00.035915: step 12523, loss 0.00145984, acc 1
2016-11-12T19:20:00.093637: step 12524, loss 0.0195632, acc 0.984375
2016-11-12T19:20:00.152347: step 12525, loss 0.0150707, acc 0.984375
2016-11-12T19:20:00.211011: step 12526, loss 7.93842e-05, acc 1
2016-11-12T19:20:00.268951: step 12527, loss 0.00469561, acc 1
2016-11-12T19:20:00.326408: step 12528, loss 4.18224e-05, acc 1
2016-11-12T19:20:00.383855: step 12529, loss 0.0150528, acc 0.984375
2016-11-12T19:20:00.444602: step 12530, loss 0.105432, acc 0.984375
2016-11-12T19:20:00.506292: step 12531, loss 0.0164472, acc 0.984375
2016-11-12T19:20:00.565563: step 12532, loss 7.90684e-05, acc 1
2016-11-12T19:20:00.623533: step 12533, loss 5.98488e-05, acc 1
2016-11-12T19:20:00.678771: step 12534, loss 0.0019935, acc 1
2016-11-12T19:20:00.736575: step 12535, loss 0.0245365, acc 0.984375
2016-11-12T19:20:00.795256: step 12536, loss 3.1515e-06, acc 1
2016-11-12T19:20:00.851299: step 12537, loss 0.0166827, acc 0.984375
2016-11-12T19:20:00.908700: step 12538, loss 3.41868e-05, acc 1
2016-11-12T19:20:00.967430: step 12539, loss 0.00180582, acc 1
2016-11-12T19:20:01.026883: step 12540, loss 0.009951, acc 1
2016-11-12T19:20:01.085690: step 12541, loss 0.000462077, acc 1
2016-11-12T19:20:01.143304: step 12542, loss 0.00809257, acc 1
2016-11-12T19:20:01.201753: step 12543, loss 0.00307428, acc 1
2016-11-12T19:20:01.261541: step 12544, loss 2.86252e-05, acc 1
2016-11-12T19:20:01.318067: step 12545, loss 3.45494e-06, acc 1
2016-11-12T19:20:01.374216: step 12546, loss 0.0102554, acc 1
2016-11-12T19:20:01.433160: step 12547, loss 0.16137, acc 0.984375
2016-11-12T19:20:01.493658: step 12548, loss 0.000458643, acc 1
2016-11-12T19:20:01.552749: step 12549, loss 0.0145954, acc 0.984375
2016-11-12T19:20:01.612846: step 12550, loss 0.000463908, acc 1
2016-11-12T19:20:01.670716: step 12551, loss 1.07227e-05, acc 1
2016-11-12T19:20:01.727482: step 12552, loss 0.000318934, acc 1
2016-11-12T19:20:01.784934: step 12553, loss 0.0360692, acc 0.984375
2016-11-12T19:20:01.843196: step 12554, loss 0.00866111, acc 1
2016-11-12T19:20:01.900971: step 12555, loss 0.00121382, acc 1
2016-11-12T19:20:01.960139: step 12556, loss 4.41032e-05, acc 1
2016-11-12T19:20:02.017742: step 12557, loss 0.000591696, acc 1
2016-11-12T19:20:02.079345: step 12558, loss 0.0220897, acc 0.984375
2016-11-12T19:20:02.140354: step 12559, loss 0.0032194, acc 1
2016-11-12T19:20:02.199495: step 12560, loss 0.0273916, acc 0.984375
2016-11-12T19:20:02.261262: step 12561, loss 0.000787349, acc 1
2016-11-12T19:20:02.319453: step 12562, loss 0.00026775, acc 1
2016-11-12T19:20:02.378010: step 12563, loss 0.000878861, acc 1
2016-11-12T19:20:02.435046: step 12564, loss 0.000290697, acc 1
2016-11-12T19:20:02.492810: step 12565, loss 0.0273725, acc 0.984375
2016-11-12T19:20:02.554347: step 12566, loss 0.000910761, acc 1
2016-11-12T19:20:02.594019: step 12567, loss 0.0037169, acc 1
2016-11-12T19:20:02.655030: step 12568, loss 9.86713e-06, acc 1
2016-11-12T19:20:02.713396: step 12569, loss 8.57866e-05, acc 1
2016-11-12T19:20:02.772498: step 12570, loss 6.75222e-05, acc 1
2016-11-12T19:20:02.830399: step 12571, loss 3.39296e-05, acc 1
2016-11-12T19:20:02.887529: step 12572, loss 0.0449442, acc 0.96875
2016-11-12T19:20:02.946502: step 12573, loss 0.000162306, acc 1
2016-11-12T19:20:03.005216: step 12574, loss 0.000319127, acc 1
2016-11-12T19:20:03.062481: step 12575, loss 0.0161129, acc 0.984375
2016-11-12T19:20:03.119426: step 12576, loss 3.24644e-06, acc 1
2016-11-12T19:20:03.176039: step 12577, loss 0.0110153, acc 0.984375
2016-11-12T19:20:03.236322: step 12578, loss 5.1796e-05, acc 1
2016-11-12T19:20:03.295147: step 12579, loss 0.000395607, acc 1
2016-11-12T19:20:03.356632: step 12580, loss 0.00819847, acc 1
2016-11-12T19:20:03.415197: step 12581, loss 4.41703e-05, acc 1
2016-11-12T19:20:03.473826: step 12582, loss 0.000204653, acc 1
2016-11-12T19:20:03.533248: step 12583, loss 0.0001245, acc 1
2016-11-12T19:20:03.591035: step 12584, loss 0.00964864, acc 1
2016-11-12T19:20:03.648591: step 12585, loss 3.96729e-06, acc 1
2016-11-12T19:20:03.706219: step 12586, loss 0.00367625, acc 1
2016-11-12T19:20:03.765018: step 12587, loss 0.000261677, acc 1
2016-11-12T19:20:03.824686: step 12588, loss 0.00036217, acc 1
2016-11-12T19:20:03.883698: step 12589, loss 0.000839971, acc 1
2016-11-12T19:20:03.941721: step 12590, loss 0.000136912, acc 1
2016-11-12T19:20:04.000590: step 12591, loss 0.00528505, acc 1
2016-11-12T19:20:04.061252: step 12592, loss 0.00293676, acc 1
2016-11-12T19:20:04.119223: step 12593, loss 4.47401e-05, acc 1
2016-11-12T19:20:04.176823: step 12594, loss 5.828e-06, acc 1
2016-11-12T19:20:04.235160: step 12595, loss 0.00473094, acc 1
2016-11-12T19:20:04.296226: step 12596, loss 8.43444e-05, acc 1
2016-11-12T19:20:04.353655: step 12597, loss 0.00191334, acc 1
2016-11-12T19:20:04.412534: step 12598, loss 0.00798204, acc 1
2016-11-12T19:20:04.473397: step 12599, loss 0.398598, acc 0.96875
2016-11-12T19:20:04.535254: step 12600, loss 0.00545257, acc 1

Evaluation:
2016-11-12T19:20:04.608281: step 12600, loss 4.96966, acc 0.54

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-12600

2016-11-12T19:20:05.092266: step 12601, loss 0.00134392, acc 1
2016-11-12T19:20:05.152269: step 12602, loss 0.0253912, acc 0.984375
2016-11-12T19:20:05.212191: step 12603, loss 0.0138114, acc 0.984375
2016-11-12T19:20:05.271365: step 12604, loss 0.000286935, acc 1
2016-11-12T19:20:05.329160: step 12605, loss 0.000540189, acc 1
2016-11-12T19:20:05.387522: step 12606, loss 3.52134e-05, acc 1
2016-11-12T19:20:05.446935: step 12607, loss 0.0060282, acc 1
2016-11-12T19:20:05.506606: step 12608, loss 0.000247601, acc 1
2016-11-12T19:20:05.563948: step 12609, loss 0.0459387, acc 0.96875
2016-11-12T19:20:05.623767: step 12610, loss 0.000196632, acc 1
2016-11-12T19:20:05.682623: step 12611, loss 0.000541616, acc 1
2016-11-12T19:20:05.740382: step 12612, loss 0.00992185, acc 1
2016-11-12T19:20:05.799868: step 12613, loss 0.0179613, acc 0.984375
2016-11-12T19:20:05.858207: step 12614, loss 9.97552e-05, acc 1
2016-11-12T19:20:05.915939: step 12615, loss 0.0223646, acc 0.984375
2016-11-12T19:20:05.973998: step 12616, loss 0.0162913, acc 1
2016-11-12T19:20:06.032290: step 12617, loss 0.0122507, acc 0.984375
2016-11-12T19:20:06.092671: step 12618, loss 0.000132272, acc 1
2016-11-12T19:20:06.149784: step 12619, loss 0.0177736, acc 0.984375
2016-11-12T19:20:06.209253: step 12620, loss 0.000241378, acc 1
2016-11-12T19:20:06.266802: step 12621, loss 0.000922896, acc 1
2016-11-12T19:20:06.325003: step 12622, loss 0.0106564, acc 1
2016-11-12T19:20:06.382938: step 12623, loss 0.00241729, acc 1
2016-11-12T19:20:06.443342: step 12624, loss 0.0409997, acc 0.984375
2016-11-12T19:20:06.501979: step 12625, loss 0.000237674, acc 1
2016-11-12T19:20:06.560636: step 12626, loss 0.00622809, acc 1
2016-11-12T19:20:06.617845: step 12627, loss 0.000293824, acc 1
2016-11-12T19:20:06.675756: step 12628, loss 1.28641e-05, acc 1
2016-11-12T19:20:06.734543: step 12629, loss 0.0149932, acc 0.984375
2016-11-12T19:20:06.792790: step 12630, loss 2.63544e-05, acc 1
2016-11-12T19:20:06.851040: step 12631, loss 8.77386e-06, acc 1
2016-11-12T19:20:06.908351: step 12632, loss 0.000429717, acc 1
2016-11-12T19:20:06.968170: step 12633, loss 0.000127135, acc 1
2016-11-12T19:20:07.027125: step 12634, loss 8.47236e-05, acc 1
2016-11-12T19:20:07.085210: step 12635, loss 0.00061459, acc 1
2016-11-12T19:20:07.144658: step 12636, loss 0.0212462, acc 0.984375
2016-11-12T19:20:07.204061: step 12637, loss 0.015812, acc 0.984375
2016-11-12T19:20:07.244846: step 12638, loss 5.4341e-05, acc 1
2016-11-12T19:20:07.304425: step 12639, loss 3.90086e-05, acc 1
2016-11-12T19:20:07.362415: step 12640, loss 6.148e-05, acc 1
2016-11-12T19:20:07.419684: step 12641, loss 0.000712964, acc 1
2016-11-12T19:20:07.477018: step 12642, loss 0.000476666, acc 1
2016-11-12T19:20:07.534900: step 12643, loss 3.51993e-05, acc 1
2016-11-12T19:20:07.593346: step 12644, loss 0.000920649, acc 1
2016-11-12T19:20:07.652047: step 12645, loss 0.00940403, acc 1
2016-11-12T19:20:07.710741: step 12646, loss 9.86375e-05, acc 1
2016-11-12T19:20:07.769567: step 12647, loss 0.00990274, acc 1
2016-11-12T19:20:07.832127: step 12648, loss 0.00045727, acc 1
2016-11-12T19:20:07.889954: step 12649, loss 0.0178696, acc 0.984375
2016-11-12T19:20:07.948358: step 12650, loss 2.4219e-05, acc 1
2016-11-12T19:20:08.004841: step 12651, loss 0.00881794, acc 1
2016-11-12T19:20:08.062517: step 12652, loss 6.9943e-05, acc 1
2016-11-12T19:20:08.119966: step 12653, loss 0.000598041, acc 1
2016-11-12T19:20:08.177789: step 12654, loss 0.0102457, acc 1
2016-11-12T19:20:08.235650: step 12655, loss 0.0010324, acc 1
2016-11-12T19:20:08.294184: step 12656, loss 0.0127131, acc 0.984375
2016-11-12T19:20:08.351047: step 12657, loss 0.034453, acc 0.984375
2016-11-12T19:20:08.408711: step 12658, loss 0.0110071, acc 1
2016-11-12T19:20:08.466927: step 12659, loss 0.00857123, acc 1
2016-11-12T19:20:08.524000: step 12660, loss 0.000655836, acc 1
2016-11-12T19:20:08.581532: step 12661, loss 0.000687547, acc 1
2016-11-12T19:20:08.639429: step 12662, loss 0.0214021, acc 0.984375
2016-11-12T19:20:08.698829: step 12663, loss 0.0494557, acc 0.984375
2016-11-12T19:20:08.756890: step 12664, loss 0.000340698, acc 1
2016-11-12T19:20:08.816039: step 12665, loss 0.0196502, acc 0.984375
2016-11-12T19:20:08.874354: step 12666, loss 0.016151, acc 0.984375
2016-11-12T19:20:08.931870: step 12667, loss 0.00251733, acc 1
2016-11-12T19:20:08.993806: step 12668, loss 0.0108218, acc 1
2016-11-12T19:20:09.051696: step 12669, loss 0.0126928, acc 1
2016-11-12T19:20:09.109577: step 12670, loss 0.131148, acc 0.984375
2016-11-12T19:20:09.169969: step 12671, loss 0.000162272, acc 1
2016-11-12T19:20:09.226756: step 12672, loss 1.06021e-05, acc 1
2016-11-12T19:20:09.287227: step 12673, loss 0.00410573, acc 1
2016-11-12T19:20:09.345196: step 12674, loss 0.00582282, acc 1
2016-11-12T19:20:09.403643: step 12675, loss 0.00170298, acc 1
2016-11-12T19:20:09.465617: step 12676, loss 8.01517e-05, acc 1
2016-11-12T19:20:09.522961: step 12677, loss 0.000110582, acc 1
2016-11-12T19:20:09.580725: step 12678, loss 0.0054652, acc 1
2016-11-12T19:20:09.638119: step 12679, loss 0.0153776, acc 0.984375
2016-11-12T19:20:09.697699: step 12680, loss 0.00862224, acc 1
2016-11-12T19:20:09.757465: step 12681, loss 5.93568e-06, acc 1
2016-11-12T19:20:09.814125: step 12682, loss 1.75978e-05, acc 1
2016-11-12T19:20:09.872123: step 12683, loss 0.00882908, acc 1
2016-11-12T19:20:09.928630: step 12684, loss 0.000622584, acc 1
2016-11-12T19:20:09.988651: step 12685, loss 0.0391751, acc 0.96875
2016-11-12T19:20:10.047851: step 12686, loss 0.00236553, acc 1
2016-11-12T19:20:10.108121: step 12687, loss 2.95563e-05, acc 1
2016-11-12T19:20:10.167942: step 12688, loss 0.0139178, acc 0.984375
2016-11-12T19:20:10.227060: step 12689, loss 0.00965281, acc 1
2016-11-12T19:20:10.283086: step 12690, loss 0.000309184, acc 1
2016-11-12T19:20:10.344041: step 12691, loss 0.00076305, acc 1
2016-11-12T19:20:10.400934: step 12692, loss 0.00239398, acc 1
2016-11-12T19:20:10.458156: step 12693, loss 0.000248589, acc 1
2016-11-12T19:20:10.516776: step 12694, loss 0.00336378, acc 1
2016-11-12T19:20:10.573911: step 12695, loss 8.0836e-05, acc 1
2016-11-12T19:20:10.630983: step 12696, loss 0.000617985, acc 1
2016-11-12T19:20:10.693896: step 12697, loss 0.000290246, acc 1
2016-11-12T19:20:10.750989: step 12698, loss 0.01173, acc 0.984375
2016-11-12T19:20:10.810616: step 12699, loss 1.77246e-05, acc 1
2016-11-12T19:20:10.866779: step 12700, loss 9.00273e-05, acc 1

Evaluation:
2016-11-12T19:20:10.938649: step 12700, loss 5.17307, acc 0.564

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-12700

2016-11-12T19:20:11.421486: step 12701, loss 3.0359e-06, acc 1
2016-11-12T19:20:11.480146: step 12702, loss 0.00763188, acc 1
2016-11-12T19:20:11.538013: step 12703, loss 0.000335398, acc 1
2016-11-12T19:20:11.597083: step 12704, loss 0.00472894, acc 1
2016-11-12T19:20:11.655382: step 12705, loss 3.09104e-05, acc 1
2016-11-12T19:20:11.711756: step 12706, loss 0.000664788, acc 1
2016-11-12T19:20:11.769533: step 12707, loss 9.96259e-05, acc 1
2016-11-12T19:20:11.826533: step 12708, loss 0.00223659, acc 1
2016-11-12T19:20:11.868183: step 12709, loss 2.51529e-06, acc 1
2016-11-12T19:20:11.924727: step 12710, loss 0.00422564, acc 1
2016-11-12T19:20:11.982574: step 12711, loss 3.038e-05, acc 1
2016-11-12T19:20:12.039673: step 12712, loss 2.11356e-05, acc 1
2016-11-12T19:20:12.096941: step 12713, loss 0.000211959, acc 1
2016-11-12T19:20:12.153445: step 12714, loss 0.00764351, acc 1
2016-11-12T19:20:12.212846: step 12715, loss 0.00628687, acc 1
2016-11-12T19:20:12.273148: step 12716, loss 0.000323051, acc 1
2016-11-12T19:20:12.330217: step 12717, loss 8.93487e-06, acc 1
2016-11-12T19:20:12.389855: step 12718, loss 0.000572179, acc 1
2016-11-12T19:20:12.446251: step 12719, loss 0.00692844, acc 1
2016-11-12T19:20:12.508030: step 12720, loss 8.40539e-05, acc 1
2016-11-12T19:20:12.564377: step 12721, loss 0.0084843, acc 1
2016-11-12T19:20:12.624278: step 12722, loss 0.00211493, acc 1
2016-11-12T19:20:12.682448: step 12723, loss 0.0170439, acc 0.984375
2016-11-12T19:20:12.744057: step 12724, loss 0.000402841, acc 1
2016-11-12T19:20:12.804863: step 12725, loss 0.0610514, acc 0.984375
2016-11-12T19:20:12.869092: step 12726, loss 2.52399e-05, acc 1
2016-11-12T19:20:12.926498: step 12727, loss 0.000127615, acc 1
2016-11-12T19:20:12.983747: step 12728, loss 0.000264876, acc 1
2016-11-12T19:20:13.044674: step 12729, loss 0.00525502, acc 1
2016-11-12T19:20:13.104799: step 12730, loss 0.00150463, acc 1
2016-11-12T19:20:13.165231: step 12731, loss 0.0465734, acc 0.984375
2016-11-12T19:20:13.226891: step 12732, loss 0.0092586, acc 1
2016-11-12T19:20:13.285644: step 12733, loss 0.000529075, acc 1
2016-11-12T19:20:13.345189: step 12734, loss 0.028097, acc 0.984375
2016-11-12T19:20:13.404672: step 12735, loss 1.15947e-05, acc 1
2016-11-12T19:20:13.460768: step 12736, loss 0.0339324, acc 0.984375
2016-11-12T19:20:13.519540: step 12737, loss 0.000246621, acc 1
2016-11-12T19:20:13.579200: step 12738, loss 0.000824029, acc 1
2016-11-12T19:20:13.636334: step 12739, loss 0.00674824, acc 1
2016-11-12T19:20:13.692219: step 12740, loss 0.00369018, acc 1
2016-11-12T19:20:13.751720: step 12741, loss 1.48935e-05, acc 1
2016-11-12T19:20:13.808509: step 12742, loss 8.25602e-05, acc 1
2016-11-12T19:20:13.868645: step 12743, loss 0.0196887, acc 0.984375
2016-11-12T19:20:13.925673: step 12744, loss 0.0033045, acc 1
2016-11-12T19:20:13.985765: step 12745, loss 2.8652e-05, acc 1
2016-11-12T19:20:14.044695: step 12746, loss 0.000188895, acc 1
2016-11-12T19:20:14.103937: step 12747, loss 0.00103887, acc 1
2016-11-12T19:20:14.162434: step 12748, loss 0.000116144, acc 1
2016-11-12T19:20:14.222040: step 12749, loss 0.000653372, acc 1
2016-11-12T19:20:14.281347: step 12750, loss 0.000507124, acc 1
2016-11-12T19:20:14.340910: step 12751, loss 0.000107344, acc 1
2016-11-12T19:20:14.401748: step 12752, loss 0.00128721, acc 1
2016-11-12T19:20:14.460866: step 12753, loss 2.15464e-05, acc 1
2016-11-12T19:20:14.516481: step 12754, loss 0.0151318, acc 0.984375
2016-11-12T19:20:14.575945: step 12755, loss 0.000130974, acc 1
2016-11-12T19:20:14.634362: step 12756, loss 0.000740448, acc 1
2016-11-12T19:20:14.694511: step 12757, loss 3.21554e-05, acc 1
2016-11-12T19:20:14.753548: step 12758, loss 0.014236, acc 0.984375
2016-11-12T19:20:14.811781: step 12759, loss 0.000102601, acc 1
2016-11-12T19:20:14.868635: step 12760, loss 0.0194035, acc 0.984375
2016-11-12T19:20:14.927933: step 12761, loss 0.0207847, acc 0.984375
2016-11-12T19:20:14.985830: step 12762, loss 0.00159761, acc 1
2016-11-12T19:20:15.045174: step 12763, loss 0.00038559, acc 1
2016-11-12T19:20:15.104819: step 12764, loss 8.34926e-05, acc 1
2016-11-12T19:20:15.163452: step 12765, loss 0.0173011, acc 0.984375
2016-11-12T19:20:15.221769: step 12766, loss 0.0181756, acc 0.984375
2016-11-12T19:20:15.280511: step 12767, loss 0.0128174, acc 1
2016-11-12T19:20:15.340865: step 12768, loss 0.0004856, acc 1
2016-11-12T19:20:15.399814: step 12769, loss 0.0141172, acc 0.984375
2016-11-12T19:20:15.460574: step 12770, loss 0.00764021, acc 1
2016-11-12T19:20:15.517208: step 12771, loss 0.000270412, acc 1
2016-11-12T19:20:15.575337: step 12772, loss 3.01164e-05, acc 1
2016-11-12T19:20:15.632203: step 12773, loss 0.00989589, acc 1
2016-11-12T19:20:15.691620: step 12774, loss 0.000434846, acc 1
2016-11-12T19:20:15.751713: step 12775, loss 0.0144755, acc 1
2016-11-12T19:20:15.810998: step 12776, loss 0.00384066, acc 1
2016-11-12T19:20:15.871722: step 12777, loss 6.86573e-05, acc 1
2016-11-12T19:20:15.928310: step 12778, loss 0.0957972, acc 0.96875
2016-11-12T19:20:15.987474: step 12779, loss 4.00355e-05, acc 1
2016-11-12T19:20:16.027940: step 12780, loss 9.29757e-05, acc 1
2016-11-12T19:20:16.088931: step 12781, loss 0.000687494, acc 1
2016-11-12T19:20:16.150340: step 12782, loss 0.0553978, acc 0.984375
2016-11-12T19:20:16.212405: step 12783, loss 0.0180313, acc 0.984375
2016-11-12T19:20:16.270852: step 12784, loss 0.000647415, acc 1
2016-11-12T19:20:16.329179: step 12785, loss 3.23514e-05, acc 1
2016-11-12T19:20:16.387959: step 12786, loss 5.65552e-05, acc 1
2016-11-12T19:20:16.445124: step 12787, loss 7.62895e-06, acc 1
2016-11-12T19:20:16.502544: step 12788, loss 0.00178376, acc 1
2016-11-12T19:20:16.559687: step 12789, loss 0.00145319, acc 1
2016-11-12T19:20:16.620885: step 12790, loss 0.00011548, acc 1
2016-11-12T19:20:16.684000: step 12791, loss 0.00443714, acc 1
2016-11-12T19:20:16.741595: step 12792, loss 0.000893302, acc 1
2016-11-12T19:20:16.799523: step 12793, loss 0.00330279, acc 1
2016-11-12T19:20:16.858020: step 12794, loss 0.000489208, acc 1
2016-11-12T19:20:16.917795: step 12795, loss 0.00698109, acc 1
2016-11-12T19:20:16.976246: step 12796, loss 0.0233969, acc 0.984375
2016-11-12T19:20:17.034191: step 12797, loss 0.165234, acc 0.984375
2016-11-12T19:20:17.093416: step 12798, loss 3.01316e-05, acc 1
2016-11-12T19:20:17.150821: step 12799, loss 0.00718063, acc 1
2016-11-12T19:20:17.208389: step 12800, loss 1.01913e-05, acc 1

Evaluation:
2016-11-12T19:20:17.278599: step 12800, loss 5.14377, acc 0.536

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-12800

2016-11-12T19:20:17.761324: step 12801, loss 0.0362184, acc 0.984375
2016-11-12T19:20:17.820148: step 12802, loss 0.0133435, acc 0.984375
2016-11-12T19:20:17.884126: step 12803, loss 0.000141249, acc 1
2016-11-12T19:20:17.941743: step 12804, loss 0.0125052, acc 0.984375
2016-11-12T19:20:18.001458: step 12805, loss 0.000338301, acc 1
2016-11-12T19:20:18.057805: step 12806, loss 0.00224036, acc 1
2016-11-12T19:20:18.117213: step 12807, loss 0.00251611, acc 1
2016-11-12T19:20:18.180315: step 12808, loss 0.00643471, acc 1
2016-11-12T19:20:18.239997: step 12809, loss 0.0106464, acc 1
2016-11-12T19:20:18.297772: step 12810, loss 8.61466e-05, acc 1
2016-11-12T19:20:18.354766: step 12811, loss 0.000260586, acc 1
2016-11-12T19:20:18.413674: step 12812, loss 0.00697384, acc 1
2016-11-12T19:20:18.473580: step 12813, loss 0.00179781, acc 1
2016-11-12T19:20:18.533075: step 12814, loss 5.99635e-05, acc 1
2016-11-12T19:20:18.590277: step 12815, loss 0.0313282, acc 0.984375
2016-11-12T19:20:18.650424: step 12816, loss 0.00245415, acc 1
2016-11-12T19:20:18.708832: step 12817, loss 0.029264, acc 0.96875
2016-11-12T19:20:18.768576: step 12818, loss 0.000114528, acc 1
2016-11-12T19:20:18.828236: step 12819, loss 7.88546e-05, acc 1
2016-11-12T19:20:18.886031: step 12820, loss 0.015132, acc 0.984375
2016-11-12T19:20:18.948633: step 12821, loss 6.47027e-05, acc 1
2016-11-12T19:20:19.006499: step 12822, loss 0.0132163, acc 0.984375
2016-11-12T19:20:19.064915: step 12823, loss 4.51391e-05, acc 1
2016-11-12T19:20:19.124227: step 12824, loss 0.00825483, acc 1
2016-11-12T19:20:19.185829: step 12825, loss 0.0437723, acc 0.984375
2016-11-12T19:20:19.246497: step 12826, loss 7.62016e-05, acc 1
2016-11-12T19:20:19.306131: step 12827, loss 0.00178049, acc 1
2016-11-12T19:20:19.364740: step 12828, loss 0.00789982, acc 1
2016-11-12T19:20:19.423051: step 12829, loss 0.00586477, acc 1
2016-11-12T19:20:19.484549: step 12830, loss 0.000610216, acc 1
2016-11-12T19:20:19.545592: step 12831, loss 0.0107627, acc 1
2016-11-12T19:20:19.603668: step 12832, loss 4.50847e-05, acc 1
2016-11-12T19:20:19.660479: step 12833, loss 0.00670575, acc 1
2016-11-12T19:20:19.719718: step 12834, loss 0.00542177, acc 1
2016-11-12T19:20:19.780876: step 12835, loss 0.0114051, acc 0.984375
2016-11-12T19:20:19.840757: step 12836, loss 0.000127446, acc 1
2016-11-12T19:20:19.897842: step 12837, loss 0.0100557, acc 1
2016-11-12T19:20:19.956627: step 12838, loss 0.000331995, acc 1
2016-11-12T19:20:20.013970: step 12839, loss 0.0142546, acc 0.984375
2016-11-12T19:20:20.072953: step 12840, loss 0.0028409, acc 1
2016-11-12T19:20:20.130236: step 12841, loss 2.80736e-05, acc 1
2016-11-12T19:20:20.187360: step 12842, loss 0.000321471, acc 1
2016-11-12T19:20:20.245776: step 12843, loss 1.42247e-05, acc 1
2016-11-12T19:20:20.304948: step 12844, loss 1.47731e-05, acc 1
2016-11-12T19:20:20.361765: step 12845, loss 0.000284451, acc 1
2016-11-12T19:20:20.419651: step 12846, loss 5.34446e-05, acc 1
2016-11-12T19:20:20.476751: step 12847, loss 7.49653e-06, acc 1
2016-11-12T19:20:20.532784: step 12848, loss 0.0155297, acc 0.984375
2016-11-12T19:20:20.591334: step 12849, loss 0.00473267, acc 1
2016-11-12T19:20:20.648460: step 12850, loss 0.0121752, acc 1
2016-11-12T19:20:20.689310: step 12851, loss 0.000154303, acc 1
2016-11-12T19:20:20.748340: step 12852, loss 0.0104712, acc 1
2016-11-12T19:20:20.805903: step 12853, loss 0.00761013, acc 1
2016-11-12T19:20:20.864707: step 12854, loss 3.3069e-05, acc 1
2016-11-12T19:20:20.921609: step 12855, loss 0.0103134, acc 1
2016-11-12T19:20:20.980102: step 12856, loss 0.000405667, acc 1
2016-11-12T19:20:21.040831: step 12857, loss 0.00837746, acc 1
2016-11-12T19:20:21.100744: step 12858, loss 0.00821058, acc 1
2016-11-12T19:20:21.159808: step 12859, loss 0.00178311, acc 1
2016-11-12T19:20:21.221174: step 12860, loss 0.000529733, acc 1
2016-11-12T19:20:21.280916: step 12861, loss 0.00159245, acc 1
2016-11-12T19:20:21.340086: step 12862, loss 2.93033e-05, acc 1
2016-11-12T19:20:21.397053: step 12863, loss 0.0163138, acc 0.984375
2016-11-12T19:20:21.457396: step 12864, loss 0.000137293, acc 1
2016-11-12T19:20:21.517441: step 12865, loss 1.35784e-06, acc 1
2016-11-12T19:20:21.577796: step 12866, loss 0.010775, acc 1
2016-11-12T19:20:21.636368: step 12867, loss 0.000404731, acc 1
2016-11-12T19:20:21.693551: step 12868, loss 0.000866482, acc 1
2016-11-12T19:20:21.753486: step 12869, loss 0.00536342, acc 1
2016-11-12T19:20:21.812494: step 12870, loss 0.00213386, acc 1
2016-11-12T19:20:21.870027: step 12871, loss 0.0167569, acc 0.984375
2016-11-12T19:20:21.928134: step 12872, loss 0.0459044, acc 0.96875
2016-11-12T19:20:21.986472: step 12873, loss 2.87372e-05, acc 1
2016-11-12T19:20:22.044207: step 12874, loss 0.000110542, acc 1
2016-11-12T19:20:22.102037: step 12875, loss 0.000248924, acc 1
2016-11-12T19:20:22.159007: step 12876, loss 0.000206077, acc 1
2016-11-12T19:20:22.220381: step 12877, loss 0.0213658, acc 0.984375
2016-11-12T19:20:22.278172: step 12878, loss 0.000758565, acc 1
2016-11-12T19:20:22.336949: step 12879, loss 0.0239301, acc 0.984375
2016-11-12T19:20:22.395143: step 12880, loss 0.00020539, acc 1
2016-11-12T19:20:22.453294: step 12881, loss 9.18428e-06, acc 1
2016-11-12T19:20:22.509498: step 12882, loss 0.0022724, acc 1
2016-11-12T19:20:22.568065: step 12883, loss 0.00872185, acc 1
2016-11-12T19:20:22.627673: step 12884, loss 0.00160558, acc 1
2016-11-12T19:20:22.684631: step 12885, loss 0.0491153, acc 0.984375
2016-11-12T19:20:22.743387: step 12886, loss 0.0156868, acc 0.984375
2016-11-12T19:20:22.802222: step 12887, loss 0.00730427, acc 1
2016-11-12T19:20:22.861751: step 12888, loss 0.0374612, acc 0.96875
2016-11-12T19:20:22.921277: step 12889, loss 5.24437e-05, acc 1
2016-11-12T19:20:22.980093: step 12890, loss 9.72763e-05, acc 1
2016-11-12T19:20:23.037109: step 12891, loss 0.000177972, acc 1
2016-11-12T19:20:23.094162: step 12892, loss 0.000614407, acc 1
2016-11-12T19:20:23.152462: step 12893, loss 0.0104894, acc 1
2016-11-12T19:20:23.212984: step 12894, loss 0.010642, acc 1
2016-11-12T19:20:23.272745: step 12895, loss 0.00039425, acc 1
2016-11-12T19:20:23.330564: step 12896, loss 0.00202877, acc 1
2016-11-12T19:20:23.390654: step 12897, loss 0.035253, acc 0.984375
2016-11-12T19:20:23.448725: step 12898, loss 0.0147941, acc 0.984375
2016-11-12T19:20:23.510704: step 12899, loss 0.00958175, acc 1
2016-11-12T19:20:23.571239: step 12900, loss 2.60849e-05, acc 1

Evaluation:
2016-11-12T19:20:23.643690: step 12900, loss 5.35107, acc 0.536

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-12900

2016-11-12T19:20:24.126429: step 12901, loss 0.00407968, acc 1
2016-11-12T19:20:24.186047: step 12902, loss 0.000186643, acc 1
2016-11-12T19:20:24.243310: step 12903, loss 0.000112539, acc 1
2016-11-12T19:20:24.301403: step 12904, loss 0.0115787, acc 0.984375
2016-11-12T19:20:24.360889: step 12905, loss 2.85781e-05, acc 1
2016-11-12T19:20:24.420079: step 12906, loss 0.0178626, acc 0.984375
2016-11-12T19:20:24.478506: step 12907, loss 0.00138138, acc 1
2016-11-12T19:20:24.536719: step 12908, loss 0.0215405, acc 0.984375
2016-11-12T19:20:24.596675: step 12909, loss 0.00295544, acc 1
2016-11-12T19:20:24.656121: step 12910, loss 0.000687714, acc 1
2016-11-12T19:20:24.714991: step 12911, loss 0.000129212, acc 1
2016-11-12T19:20:24.772464: step 12912, loss 0.0135183, acc 0.984375
2016-11-12T19:20:24.832838: step 12913, loss 9.83541e-05, acc 1
2016-11-12T19:20:24.889875: step 12914, loss 0.00101737, acc 1
2016-11-12T19:20:24.948854: step 12915, loss 0.0119661, acc 0.984375
2016-11-12T19:20:25.006558: step 12916, loss 1.02493e-05, acc 1
2016-11-12T19:20:25.064481: step 12917, loss 3.09945e-05, acc 1
2016-11-12T19:20:25.122191: step 12918, loss 0.00857931, acc 1
2016-11-12T19:20:25.181749: step 12919, loss 0.00887926, acc 1
2016-11-12T19:20:25.239670: step 12920, loss 9.27454e-05, acc 1
2016-11-12T19:20:25.300145: step 12921, loss 0.00627497, acc 1
2016-11-12T19:20:25.341573: step 12922, loss 0.000108679, acc 1
2016-11-12T19:20:25.400697: step 12923, loss 0.00329932, acc 1
2016-11-12T19:20:25.460654: step 12924, loss 0.00131877, acc 1
2016-11-12T19:20:25.517472: step 12925, loss 0.0214795, acc 0.984375
2016-11-12T19:20:25.577443: step 12926, loss 0.00528744, acc 1
2016-11-12T19:20:25.636346: step 12927, loss 0.000284392, acc 1
2016-11-12T19:20:25.693718: step 12928, loss 0.000313107, acc 1
2016-11-12T19:20:25.752602: step 12929, loss 0.00980359, acc 1
2016-11-12T19:20:25.809388: step 12930, loss 0.0140835, acc 0.984375
2016-11-12T19:20:25.867765: step 12931, loss 0.0044707, acc 1
2016-11-12T19:20:25.926879: step 12932, loss 1.46886e-05, acc 1
2016-11-12T19:20:25.983441: step 12933, loss 0.00458014, acc 1
2016-11-12T19:20:26.043289: step 12934, loss 0.0310533, acc 0.984375
2016-11-12T19:20:26.102512: step 12935, loss 0.00421986, acc 1
2016-11-12T19:20:26.160710: step 12936, loss 0.0158074, acc 1
2016-11-12T19:20:26.218974: step 12937, loss 0.0209183, acc 0.984375
2016-11-12T19:20:26.279167: step 12938, loss 0.000301759, acc 1
2016-11-12T19:20:26.338910: step 12939, loss 0.0122217, acc 0.984375
2016-11-12T19:20:26.397264: step 12940, loss 0.00530022, acc 1
2016-11-12T19:20:26.454739: step 12941, loss 1.12368e-05, acc 1
2016-11-12T19:20:26.510866: step 12942, loss 0.000516096, acc 1
2016-11-12T19:20:26.569380: step 12943, loss 0.0154018, acc 0.984375
2016-11-12T19:20:26.627772: step 12944, loss 9.94674e-06, acc 1
2016-11-12T19:20:26.684616: step 12945, loss 0.0133722, acc 1
2016-11-12T19:20:26.742280: step 12946, loss 6.37979e-05, acc 1
2016-11-12T19:20:26.799092: step 12947, loss 9.76679e-06, acc 1
2016-11-12T19:20:26.856746: step 12948, loss 2.23931e-05, acc 1
2016-11-12T19:20:26.914102: step 12949, loss 0.0412558, acc 0.984375
2016-11-12T19:20:26.971198: step 12950, loss 0.00216817, acc 1
2016-11-12T19:20:27.030134: step 12951, loss 0.001236, acc 1
2016-11-12T19:20:27.089529: step 12952, loss 0.0191561, acc 0.984375
2016-11-12T19:20:27.147959: step 12953, loss 4.43461e-06, acc 1
2016-11-12T19:20:27.205847: step 12954, loss 0.00846002, acc 1
2016-11-12T19:20:27.264915: step 12955, loss 2.68257e-05, acc 1
2016-11-12T19:20:27.321418: step 12956, loss 0.000154204, acc 1
2016-11-12T19:20:27.379747: step 12957, loss 2.57118e-05, acc 1
2016-11-12T19:20:27.436803: step 12958, loss 0.00110284, acc 1
2016-11-12T19:20:27.494552: step 12959, loss 0.0004657, acc 1
2016-11-12T19:20:27.552174: step 12960, loss 1.79183e-06, acc 1
2016-11-12T19:20:27.608191: step 12961, loss 1.22466e-05, acc 1
2016-11-12T19:20:27.664380: step 12962, loss 6.26407e-05, acc 1
2016-11-12T19:20:27.723674: step 12963, loss 2.3302e-05, acc 1
2016-11-12T19:20:27.783189: step 12964, loss 0.0149455, acc 1
2016-11-12T19:20:27.841575: step 12965, loss 0.000140324, acc 1
2016-11-12T19:20:27.900458: step 12966, loss 0.000122916, acc 1
2016-11-12T19:20:27.962028: step 12967, loss 0.000290845, acc 1
2016-11-12T19:20:28.023401: step 12968, loss 0.000280603, acc 1
2016-11-12T19:20:28.082822: step 12969, loss 0.000404762, acc 1
2016-11-12T19:20:28.140750: step 12970, loss 2.16089e-05, acc 1
2016-11-12T19:20:28.197450: step 12971, loss 0.000150509, acc 1
2016-11-12T19:20:28.256416: step 12972, loss 0.000236438, acc 1
2016-11-12T19:20:28.316771: step 12973, loss 0.00855818, acc 1
2016-11-12T19:20:28.374171: step 12974, loss 7.31048e-05, acc 1
2016-11-12T19:20:28.432941: step 12975, loss 0.0413444, acc 0.984375
2016-11-12T19:20:28.492058: step 12976, loss 0.00657723, acc 1
2016-11-12T19:20:28.552035: step 12977, loss 2.81252e-06, acc 1
2016-11-12T19:20:28.608849: step 12978, loss 3.29479e-06, acc 1
2016-11-12T19:20:28.665642: step 12979, loss 0.000310126, acc 1
2016-11-12T19:20:28.721948: step 12980, loss 0.000106721, acc 1
2016-11-12T19:20:28.778440: step 12981, loss 0.0065098, acc 1
2016-11-12T19:20:28.836908: step 12982, loss 0.000899805, acc 1
2016-11-12T19:20:28.896507: step 12983, loss 0.000274653, acc 1
2016-11-12T19:20:28.954233: step 12984, loss 0.0117006, acc 0.984375
2016-11-12T19:20:29.011453: step 12985, loss 0.0254006, acc 0.984375
2016-11-12T19:20:29.073498: step 12986, loss 0.027353, acc 0.984375
2016-11-12T19:20:29.133800: step 12987, loss 0.000563896, acc 1
2016-11-12T19:20:29.191724: step 12988, loss 3.70514e-05, acc 1
2016-11-12T19:20:29.251078: step 12989, loss 0.000942754, acc 1
2016-11-12T19:20:29.308666: step 12990, loss 0.000180103, acc 1
2016-11-12T19:20:29.365292: step 12991, loss 0.0227014, acc 0.984375
2016-11-12T19:20:29.424083: step 12992, loss 0.0496657, acc 0.96875
2016-11-12T19:20:29.464037: step 12993, loss 4.05311e-07, acc 1
2016-11-12T19:20:29.521996: step 12994, loss 0.00219038, acc 1
2016-11-12T19:20:29.581856: step 12995, loss 0.000144225, acc 1
2016-11-12T19:20:29.639932: step 12996, loss 0.000321762, acc 1
2016-11-12T19:20:29.697435: step 12997, loss 0.000113466, acc 1
2016-11-12T19:20:29.756522: step 12998, loss 6.80623e-05, acc 1
2016-11-12T19:20:29.815930: step 12999, loss 0.0100074, acc 1
2016-11-12T19:20:29.876895: step 13000, loss 0.0108936, acc 1

Evaluation:
2016-11-12T19:20:29.952619: step 13000, loss 5.30979, acc 0.534

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-13000

2016-11-12T19:20:30.434585: step 13001, loss 0.00168735, acc 1
2016-11-12T19:20:30.491773: step 13002, loss 0.00137256, acc 1
2016-11-12T19:20:30.552557: step 13003, loss 1.13013e-05, acc 1
2016-11-12T19:20:30.613239: step 13004, loss 0.00380638, acc 1
2016-11-12T19:20:30.672635: step 13005, loss 0.000708709, acc 1
2016-11-12T19:20:30.730591: step 13006, loss 0.0021452, acc 1
2016-11-12T19:20:30.788082: step 13007, loss 0.0586106, acc 0.984375
2016-11-12T19:20:30.849204: step 13008, loss 8.78745e-05, acc 1
2016-11-12T19:20:30.909032: step 13009, loss 2.93019e-05, acc 1
2016-11-12T19:20:30.966017: step 13010, loss 7.15885e-06, acc 1
2016-11-12T19:20:31.021796: step 13011, loss 0.000993444, acc 1
2016-11-12T19:20:31.079111: step 13012, loss 0.000602533, acc 1
2016-11-12T19:20:31.136400: step 13013, loss 0.00239337, acc 1
2016-11-12T19:20:31.196483: step 13014, loss 0.0166401, acc 0.984375
2016-11-12T19:20:31.254149: step 13015, loss 0.000680026, acc 1
2016-11-12T19:20:31.313610: step 13016, loss 0.0345838, acc 0.984375
2016-11-12T19:20:31.372270: step 13017, loss 0.00925738, acc 1
2016-11-12T19:20:31.429671: step 13018, loss 0.000124227, acc 1
2016-11-12T19:20:31.488886: step 13019, loss 6.13556e-05, acc 1
2016-11-12T19:20:31.546727: step 13020, loss 0.0147273, acc 1
2016-11-12T19:20:31.605474: step 13021, loss 2.1372e-05, acc 1
2016-11-12T19:20:31.662936: step 13022, loss 0.000200588, acc 1
2016-11-12T19:20:31.721213: step 13023, loss 0.000529718, acc 1
2016-11-12T19:20:31.778495: step 13024, loss 0.0516129, acc 0.984375
2016-11-12T19:20:31.836847: step 13025, loss 9.35031e-05, acc 1
2016-11-12T19:20:31.894534: step 13026, loss 0.0193088, acc 0.984375
2016-11-12T19:20:31.951556: step 13027, loss 7.80787e-05, acc 1
2016-11-12T19:20:32.008125: step 13028, loss 0.000130065, acc 1
2016-11-12T19:20:32.065242: step 13029, loss 0.00111864, acc 1
2016-11-12T19:20:32.124326: step 13030, loss 6.53003e-05, acc 1
2016-11-12T19:20:32.184355: step 13031, loss 0.00468004, acc 1
2016-11-12T19:20:32.244918: step 13032, loss 0.00066225, acc 1
2016-11-12T19:20:32.304913: step 13033, loss 0.00468447, acc 1
2016-11-12T19:20:32.362866: step 13034, loss 2.00044e-06, acc 1
2016-11-12T19:20:32.420843: step 13035, loss 0.0236152, acc 0.984375
2016-11-12T19:20:32.480273: step 13036, loss 0.000515368, acc 1
2016-11-12T19:20:32.539666: step 13037, loss 0.149023, acc 0.984375
2016-11-12T19:20:32.599840: step 13038, loss 0.0052296, acc 1
2016-11-12T19:20:32.660463: step 13039, loss 0.0102133, acc 1
2016-11-12T19:20:32.720202: step 13040, loss 6.63704e-05, acc 1
2016-11-12T19:20:32.776352: step 13041, loss 0.0306903, acc 0.96875
2016-11-12T19:20:32.836890: step 13042, loss 0.000822846, acc 1
2016-11-12T19:20:32.896810: step 13043, loss 0.00815129, acc 1
2016-11-12T19:20:32.955929: step 13044, loss 0.000121701, acc 1
2016-11-12T19:20:33.013539: step 13045, loss 1.07659e-06, acc 1
2016-11-12T19:20:33.068727: step 13046, loss 0.00397945, acc 1
2016-11-12T19:20:33.128470: step 13047, loss 0.01017, acc 1
2016-11-12T19:20:33.186064: step 13048, loss 0.0160395, acc 0.984375
2016-11-12T19:20:33.244224: step 13049, loss 0.015063, acc 0.984375
2016-11-12T19:20:33.303470: step 13050, loss 0.0136777, acc 0.984375
2016-11-12T19:20:33.361520: step 13051, loss 0.0125282, acc 0.984375
2016-11-12T19:20:33.420880: step 13052, loss 0.0318805, acc 0.984375
2016-11-12T19:20:33.483879: step 13053, loss 0.0180039, acc 0.984375
2016-11-12T19:20:33.541716: step 13054, loss 4.02845e-05, acc 1
2016-11-12T19:20:33.598647: step 13055, loss 3.9412e-06, acc 1
2016-11-12T19:20:33.655621: step 13056, loss 0.00250395, acc 1
2016-11-12T19:20:33.714748: step 13057, loss 0.000970617, acc 1
2016-11-12T19:20:33.775055: step 13058, loss 0.00153996, acc 1
2016-11-12T19:20:33.837197: step 13059, loss 0.0317256, acc 0.984375
2016-11-12T19:20:33.901733: step 13060, loss 4.73186e-05, acc 1
2016-11-12T19:20:33.961768: step 13061, loss 0.0243148, acc 0.984375
2016-11-12T19:20:34.019660: step 13062, loss 0.0136089, acc 0.984375
2016-11-12T19:20:34.079121: step 13063, loss 0.000473938, acc 1
2016-11-12T19:20:34.118083: step 13064, loss 3.25436e-06, acc 1
2016-11-12T19:20:34.176330: step 13065, loss 0.000131109, acc 1
2016-11-12T19:20:34.234050: step 13066, loss 0.0068468, acc 1
2016-11-12T19:20:34.291732: step 13067, loss 0.0122767, acc 0.984375
2016-11-12T19:20:34.349718: step 13068, loss 0.0146341, acc 0.984375
2016-11-12T19:20:34.407266: step 13069, loss 0.0281786, acc 0.984375
2016-11-12T19:20:34.465024: step 13070, loss 0.0010446, acc 1
2016-11-12T19:20:34.524396: step 13071, loss 0.000136847, acc 1
2016-11-12T19:20:34.582534: step 13072, loss 0.000175984, acc 1
2016-11-12T19:20:34.640643: step 13073, loss 0.0187751, acc 0.984375
2016-11-12T19:20:34.700732: step 13074, loss 8.61772e-06, acc 1
2016-11-12T19:20:34.758863: step 13075, loss 0.0159204, acc 0.984375
2016-11-12T19:20:34.817747: step 13076, loss 0.010961, acc 1
2016-11-12T19:20:34.877039: step 13077, loss 0.000103259, acc 1
2016-11-12T19:20:34.934453: step 13078, loss 0.00367686, acc 1
2016-11-12T19:20:34.992787: step 13079, loss 7.68115e-05, acc 1
2016-11-12T19:20:35.050448: step 13080, loss 9.17676e-05, acc 1
2016-11-12T19:20:35.107420: step 13081, loss 0.0120796, acc 0.984375
2016-11-12T19:20:35.167032: step 13082, loss 0.000187094, acc 1
2016-11-12T19:20:35.224321: step 13083, loss 6.88663e-05, acc 1
2016-11-12T19:20:35.282406: step 13084, loss 7.40393e-05, acc 1
2016-11-12T19:20:35.339375: step 13085, loss 0.000350664, acc 1
2016-11-12T19:20:35.396489: step 13086, loss 0.000199922, acc 1
2016-11-12T19:20:35.456666: step 13087, loss 0.00361343, acc 1
2016-11-12T19:20:35.516099: step 13088, loss 0.0570414, acc 0.984375
2016-11-12T19:20:35.576353: step 13089, loss 0.0139803, acc 0.984375
2016-11-12T19:20:35.635471: step 13090, loss 0.000925468, acc 1
2016-11-12T19:20:35.695578: step 13091, loss 2.16718e-05, acc 1
2016-11-12T19:20:35.753822: step 13092, loss 0.0158538, acc 0.984375
2016-11-12T19:20:35.813697: step 13093, loss 0.00232157, acc 1
2016-11-12T19:20:35.872284: step 13094, loss 7.8042e-05, acc 1
2016-11-12T19:20:35.930244: step 13095, loss 0.0080559, acc 1
2016-11-12T19:20:35.989432: step 13096, loss 0.0190972, acc 0.984375
2016-11-12T19:20:36.047981: step 13097, loss 0.00994569, acc 1
2016-11-12T19:20:36.106016: step 13098, loss 0.0447653, acc 0.96875
2016-11-12T19:20:36.164768: step 13099, loss 0.0703476, acc 0.96875
2016-11-12T19:20:36.224298: step 13100, loss 0.0310109, acc 0.984375

Evaluation:
2016-11-12T19:20:36.298176: step 13100, loss 5.28717, acc 0.544

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-13100

2016-11-12T19:20:36.781752: step 13101, loss 0.023498, acc 0.984375
2016-11-12T19:20:36.840636: step 13102, loss 0.000380908, acc 1
2016-11-12T19:20:36.899516: step 13103, loss 0.0147165, acc 0.984375
2016-11-12T19:20:36.959328: step 13104, loss 0.014393, acc 0.984375
2016-11-12T19:20:37.019158: step 13105, loss 0.000281765, acc 1
2016-11-12T19:20:37.076537: step 13106, loss 0.008335, acc 1
2016-11-12T19:20:37.136041: step 13107, loss 1.21748e-05, acc 1
2016-11-12T19:20:37.192435: step 13108, loss 0.00134311, acc 1
2016-11-12T19:20:37.251667: step 13109, loss 0.00064789, acc 1
2016-11-12T19:20:37.311073: step 13110, loss 8.11046e-05, acc 1
2016-11-12T19:20:37.367466: step 13111, loss 0.000190091, acc 1
2016-11-12T19:20:37.425660: step 13112, loss 0.005072, acc 1
2016-11-12T19:20:37.482026: step 13113, loss 0.00227041, acc 1
2016-11-12T19:20:37.539661: step 13114, loss 0.000150166, acc 1
2016-11-12T19:20:37.596651: step 13115, loss 1.08838e-05, acc 1
2016-11-12T19:20:37.656721: step 13116, loss 2.45767e-05, acc 1
2016-11-12T19:20:37.714546: step 13117, loss 0.00849279, acc 1
2016-11-12T19:20:37.773203: step 13118, loss 0.000150273, acc 1
2016-11-12T19:20:37.831575: step 13119, loss 0.0523843, acc 0.96875
2016-11-12T19:20:37.888344: step 13120, loss 0.00031457, acc 1
2016-11-12T19:20:37.945981: step 13121, loss 0.00298883, acc 1
2016-11-12T19:20:38.004830: step 13122, loss 0.00019869, acc 1
2016-11-12T19:20:38.063286: step 13123, loss 0.000722929, acc 1
2016-11-12T19:20:38.121519: step 13124, loss 0.00185661, acc 1
2016-11-12T19:20:38.179426: step 13125, loss 0.00380688, acc 1
2016-11-12T19:20:38.239437: step 13126, loss 0.0206628, acc 0.984375
2016-11-12T19:20:38.300328: step 13127, loss 0.00508387, acc 1
2016-11-12T19:20:38.359795: step 13128, loss 0.0175491, acc 1
2016-11-12T19:20:38.420118: step 13129, loss 0.042698, acc 0.984375
2016-11-12T19:20:38.479865: step 13130, loss 0.000150751, acc 1
2016-11-12T19:20:38.538856: step 13131, loss 0.00364768, acc 1
2016-11-12T19:20:38.596294: step 13132, loss 0.0154161, acc 0.984375
2016-11-12T19:20:38.659999: step 13133, loss 0.000436917, acc 1
2016-11-12T19:20:38.720341: step 13134, loss 0.00701492, acc 1
2016-11-12T19:20:38.762090: step 13135, loss 0.00308444, acc 1
2016-11-12T19:20:38.821711: step 13136, loss 0.167903, acc 0.984375
2016-11-12T19:20:38.885856: step 13137, loss 0.00206581, acc 1
2016-11-12T19:20:38.946203: step 13138, loss 0.042316, acc 0.96875
2016-11-12T19:20:39.004783: step 13139, loss 0.00577958, acc 1
2016-11-12T19:20:39.061645: step 13140, loss 0.00310563, acc 1
2016-11-12T19:20:39.123152: step 13141, loss 2.40273e-06, acc 1
2016-11-12T19:20:39.179145: step 13142, loss 0.00019216, acc 1
2016-11-12T19:20:39.236171: step 13143, loss 7.0607e-06, acc 1
2016-11-12T19:20:39.293599: step 13144, loss 0.441309, acc 0.96875
2016-11-12T19:20:39.357712: step 13145, loss 0.115331, acc 0.984375
2016-11-12T19:20:39.416918: step 13146, loss 0.00143441, acc 1
2016-11-12T19:20:39.477543: step 13147, loss 4.30239e-05, acc 1
2016-11-12T19:20:39.536790: step 13148, loss 0.000657766, acc 1
2016-11-12T19:20:39.595766: step 13149, loss 8.72529e-05, acc 1
2016-11-12T19:20:39.656577: step 13150, loss 0.000129818, acc 1
2016-11-12T19:20:39.714438: step 13151, loss 0.00541399, acc 1
2016-11-12T19:20:39.773516: step 13152, loss 0.0140261, acc 0.984375
2016-11-12T19:20:39.832895: step 13153, loss 0.0279175, acc 0.984375
2016-11-12T19:20:39.891696: step 13154, loss 7.39541e-05, acc 1
2016-11-12T19:20:39.950644: step 13155, loss 0.0375942, acc 0.984375
2016-11-12T19:20:40.009324: step 13156, loss 0.000211493, acc 1
2016-11-12T19:20:40.065706: step 13157, loss 0.0137661, acc 1
2016-11-12T19:20:40.125357: step 13158, loss 0.000467982, acc 1
2016-11-12T19:20:40.183400: step 13159, loss 0.0360724, acc 0.984375
2016-11-12T19:20:40.244346: step 13160, loss 0.000643459, acc 1
2016-11-12T19:20:40.303866: step 13161, loss 0.000150683, acc 1
2016-11-12T19:20:40.363017: step 13162, loss 0.000292461, acc 1
2016-11-12T19:20:40.420642: step 13163, loss 0.00228466, acc 1
2016-11-12T19:20:40.478689: step 13164, loss 0.00018938, acc 1
2016-11-12T19:20:40.537143: step 13165, loss 0.00437241, acc 1
2016-11-12T19:20:40.597111: step 13166, loss 0.000135696, acc 1
2016-11-12T19:20:40.653871: step 13167, loss 0.00563759, acc 1
2016-11-12T19:20:40.711816: step 13168, loss 0.035743, acc 0.96875
2016-11-12T19:20:40.773227: step 13169, loss 4.67314e-06, acc 1
2016-11-12T19:20:40.830887: step 13170, loss 0.00479756, acc 1
2016-11-12T19:20:40.892752: step 13171, loss 0.00514482, acc 1
2016-11-12T19:20:40.950785: step 13172, loss 0.000656103, acc 1
2016-11-12T19:20:41.010565: step 13173, loss 0.0294075, acc 0.96875
2016-11-12T19:20:41.071034: step 13174, loss 3.54106e-05, acc 1
2016-11-12T19:20:41.129577: step 13175, loss 0.00171582, acc 1
2016-11-12T19:20:41.187579: step 13176, loss 0.0155921, acc 0.984375
2016-11-12T19:20:41.246580: step 13177, loss 9.21488e-05, acc 1
2016-11-12T19:20:41.304607: step 13178, loss 0.00369838, acc 1
2016-11-12T19:20:41.362677: step 13179, loss 0.0348293, acc 1
2016-11-12T19:20:41.421661: step 13180, loss 0.00423111, acc 1
2016-11-12T19:20:41.480479: step 13181, loss 0.000605392, acc 1
2016-11-12T19:20:41.541267: step 13182, loss 0.000220672, acc 1
2016-11-12T19:20:41.601453: step 13183, loss 0.000111149, acc 1
2016-11-12T19:20:41.658943: step 13184, loss 1.91239e-05, acc 1
2016-11-12T19:20:41.714807: step 13185, loss 2.60227e-05, acc 1
2016-11-12T19:20:41.772031: step 13186, loss 1.24581e-05, acc 1
2016-11-12T19:20:41.828336: step 13187, loss 0.101543, acc 0.984375
2016-11-12T19:20:41.887217: step 13188, loss 0.000584852, acc 1
2016-11-12T19:20:41.944759: step 13189, loss 0.000626126, acc 1
2016-11-12T19:20:42.005000: step 13190, loss 0.0191768, acc 1
2016-11-12T19:20:42.065348: step 13191, loss 0.0419279, acc 0.96875
2016-11-12T19:20:42.124940: step 13192, loss 0.0954436, acc 0.96875
2016-11-12T19:20:42.184842: step 13193, loss 0.00590543, acc 1
2016-11-12T19:20:42.244203: step 13194, loss 0.00401396, acc 1
2016-11-12T19:20:42.301569: step 13195, loss 4.00819e-06, acc 1
2016-11-12T19:20:42.360797: step 13196, loss 0.00127132, acc 1
2016-11-12T19:20:42.418520: step 13197, loss 0.00133965, acc 1
2016-11-12T19:20:42.476468: step 13198, loss 0.0303739, acc 0.984375
2016-11-12T19:20:42.536733: step 13199, loss 0.0229806, acc 0.984375
2016-11-12T19:20:42.594975: step 13200, loss 0.00104943, acc 1

Evaluation:
2016-11-12T19:20:42.668153: step 13200, loss 5.5099, acc 0.542

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-13200

2016-11-12T19:20:43.151511: step 13201, loss 0.00134589, acc 1
2016-11-12T19:20:43.213097: step 13202, loss 0.0172045, acc 0.984375
2016-11-12T19:20:43.273156: step 13203, loss 0.00803232, acc 1
2016-11-12T19:20:43.331584: step 13204, loss 0.00131229, acc 1
2016-11-12T19:20:43.390288: step 13205, loss 0.00011689, acc 1
2016-11-12T19:20:43.427797: step 13206, loss 0.00092819, acc 1
2016-11-12T19:20:43.487396: step 13207, loss 0.0133399, acc 0.984375
2016-11-12T19:20:43.545188: step 13208, loss 0.0302028, acc 0.984375
2016-11-12T19:20:43.604735: step 13209, loss 2.8883e-05, acc 1
2016-11-12T19:20:43.664545: step 13210, loss 1.22897e-05, acc 1
2016-11-12T19:20:43.721974: step 13211, loss 0.00228268, acc 1
2016-11-12T19:20:43.780221: step 13212, loss 0.0460639, acc 0.96875
2016-11-12T19:20:43.839314: step 13213, loss 0.000101352, acc 1
2016-11-12T19:20:43.900679: step 13214, loss 0.000133425, acc 1
2016-11-12T19:20:43.960634: step 13215, loss 0.00119496, acc 1
2016-11-12T19:20:44.021585: step 13216, loss 6.81701e-06, acc 1
2016-11-12T19:20:44.077721: step 13217, loss 0.0110587, acc 0.984375
2016-11-12T19:20:44.136925: step 13218, loss 1.78735e-05, acc 1
2016-11-12T19:20:44.193292: step 13219, loss 1.49906e-05, acc 1
2016-11-12T19:20:44.250657: step 13220, loss 0.0103765, acc 1
2016-11-12T19:20:44.312084: step 13221, loss 0.00023527, acc 1
2016-11-12T19:20:44.370557: step 13222, loss 0.299845, acc 0.984375
2016-11-12T19:20:44.429791: step 13223, loss 0.00229251, acc 1
2016-11-12T19:20:44.489913: step 13224, loss 0.0226989, acc 0.984375
2016-11-12T19:20:44.549025: step 13225, loss 0.00158461, acc 1
2016-11-12T19:20:44.608213: step 13226, loss 2.5211e-05, acc 1
2016-11-12T19:20:44.668103: step 13227, loss 0.0107855, acc 1
2016-11-12T19:20:44.729968: step 13228, loss 0.000123073, acc 1
2016-11-12T19:20:44.789407: step 13229, loss 0.0441307, acc 0.984375
2016-11-12T19:20:44.850768: step 13230, loss 0.00174274, acc 1
2016-11-12T19:20:44.910510: step 13231, loss 0.0174764, acc 0.984375
2016-11-12T19:20:44.968965: step 13232, loss 0.00114547, acc 1
2016-11-12T19:20:45.027862: step 13233, loss 0.138263, acc 0.953125
2016-11-12T19:20:45.088078: step 13234, loss 0.118421, acc 0.984375
2016-11-12T19:20:45.146056: step 13235, loss 0.0370776, acc 0.984375
2016-11-12T19:20:45.205873: step 13236, loss 0.00601444, acc 1
2016-11-12T19:20:45.264123: step 13237, loss 6.70242e-05, acc 1
2016-11-12T19:20:45.321389: step 13238, loss 0.0489878, acc 0.984375
2016-11-12T19:20:45.380653: step 13239, loss 0.000170597, acc 1
2016-11-12T19:20:45.437906: step 13240, loss 0.024638, acc 0.984375
2016-11-12T19:20:45.499950: step 13241, loss 0.0115387, acc 0.984375
2016-11-12T19:20:45.556833: step 13242, loss 0.009724, acc 1
2016-11-12T19:20:45.616844: step 13243, loss 2.17287e-05, acc 1
2016-11-12T19:20:45.673553: step 13244, loss 0.000110081, acc 1
2016-11-12T19:20:45.732991: step 13245, loss 0.00165748, acc 1
2016-11-12T19:20:45.791744: step 13246, loss 0.0127294, acc 0.984375
2016-11-12T19:20:45.850173: step 13247, loss 0.000196269, acc 1
2016-11-12T19:20:45.907527: step 13248, loss 0.0047208, acc 1
2016-11-12T19:20:45.966145: step 13249, loss 0.0321374, acc 0.984375
2016-11-12T19:20:46.024775: step 13250, loss 0.0193896, acc 0.984375
2016-11-12T19:20:46.084680: step 13251, loss 0.000189619, acc 1
2016-11-12T19:20:46.141706: step 13252, loss 0.000137924, acc 1
2016-11-12T19:20:46.201452: step 13253, loss 0.000182052, acc 1
2016-11-12T19:20:46.261083: step 13254, loss 0.000261473, acc 1
2016-11-12T19:20:46.320717: step 13255, loss 0.00512786, acc 1
2016-11-12T19:20:46.383655: step 13256, loss 0.034729, acc 0.96875
2016-11-12T19:20:46.445242: step 13257, loss 0.0125794, acc 0.984375
2016-11-12T19:20:46.503984: step 13258, loss 0.000125581, acc 1
2016-11-12T19:20:46.560703: step 13259, loss 0.0155148, acc 1
2016-11-12T19:20:46.618231: step 13260, loss 0.0146416, acc 1
2016-11-12T19:20:46.678382: step 13261, loss 7.08065e-05, acc 1
2016-11-12T19:20:46.735834: step 13262, loss 0.0183045, acc 0.984375
2016-11-12T19:20:46.793334: step 13263, loss 0.00316206, acc 1
2016-11-12T19:20:46.852768: step 13264, loss 0.000418943, acc 1
2016-11-12T19:20:46.910159: step 13265, loss 0.0076679, acc 1
2016-11-12T19:20:46.972519: step 13266, loss 0.00727771, acc 1
2016-11-12T19:20:47.032628: step 13267, loss 0.0038041, acc 1
2016-11-12T19:20:47.090592: step 13268, loss 0.0184455, acc 1
2016-11-12T19:20:47.151245: step 13269, loss 0.00525809, acc 1
2016-11-12T19:20:47.211600: step 13270, loss 0.000662111, acc 1
2016-11-12T19:20:47.270278: step 13271, loss 0.00421899, acc 1
2016-11-12T19:20:47.329573: step 13272, loss 0.000912768, acc 1
2016-11-12T19:20:47.387617: step 13273, loss 0.01639, acc 0.984375
2016-11-12T19:20:47.446161: step 13274, loss 0.00257197, acc 1
2016-11-12T19:20:47.506469: step 13275, loss 0.00183449, acc 1
2016-11-12T19:20:47.565470: step 13276, loss 0.00517048, acc 1
2016-11-12T19:20:47.606996: step 13277, loss 0.0107657, acc 1
2016-11-12T19:20:47.667959: step 13278, loss 0.000320454, acc 1
2016-11-12T19:20:47.725429: step 13279, loss 0.00129672, acc 1
2016-11-12T19:20:47.783728: step 13280, loss 6.16195e-05, acc 1
2016-11-12T19:20:47.842289: step 13281, loss 0.0073178, acc 1
2016-11-12T19:20:47.900418: step 13282, loss 0.0318061, acc 0.96875
2016-11-12T19:20:47.964443: step 13283, loss 0.000577155, acc 1
2016-11-12T19:20:48.021939: step 13284, loss 0.00590717, acc 1
2016-11-12T19:20:48.079878: step 13285, loss 0.000612146, acc 1
2016-11-12T19:20:48.137684: step 13286, loss 0.000620886, acc 1
2016-11-12T19:20:48.196203: step 13287, loss 6.13591e-05, acc 1
2016-11-12T19:20:48.253351: step 13288, loss 0.00189165, acc 1
2016-11-12T19:20:48.313079: step 13289, loss 0.104712, acc 0.96875
2016-11-12T19:20:48.371324: step 13290, loss 0.0349182, acc 0.96875
2016-11-12T19:20:48.430132: step 13291, loss 0.168148, acc 0.96875
2016-11-12T19:20:48.489667: step 13292, loss 0.0215872, acc 0.984375
2016-11-12T19:20:48.550239: step 13293, loss 0.000714034, acc 1
2016-11-12T19:20:48.612448: step 13294, loss 0.000213128, acc 1
2016-11-12T19:20:48.671036: step 13295, loss 0.000809338, acc 1
2016-11-12T19:20:48.733433: step 13296, loss 0.0123269, acc 0.984375
2016-11-12T19:20:48.792781: step 13297, loss 0.0155268, acc 0.984375
2016-11-12T19:20:48.853861: step 13298, loss 0.000346037, acc 1
2016-11-12T19:20:48.911398: step 13299, loss 2.86366e-05, acc 1
2016-11-12T19:20:48.969484: step 13300, loss 0.00638836, acc 1

Evaluation:
2016-11-12T19:20:49.042847: step 13300, loss 5.45173, acc 0.556

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-13300

2016-11-12T19:20:49.525037: step 13301, loss 0.000326364, acc 1
2016-11-12T19:20:49.582462: step 13302, loss 0.00023605, acc 1
2016-11-12T19:20:49.639065: step 13303, loss 0.00767361, acc 1
2016-11-12T19:20:49.697933: step 13304, loss 0.0196552, acc 0.984375
2016-11-12T19:20:49.757882: step 13305, loss 0.00620303, acc 1
2016-11-12T19:20:49.815900: step 13306, loss 0.00425563, acc 1
2016-11-12T19:20:49.876740: step 13307, loss 0.000550118, acc 1
2016-11-12T19:20:49.937497: step 13308, loss 0.00123812, acc 1
2016-11-12T19:20:49.996131: step 13309, loss 1.78626e-05, acc 1
2016-11-12T19:20:50.062297: step 13310, loss 2.47955e-05, acc 1
2016-11-12T19:20:50.120761: step 13311, loss 0.000515539, acc 1
2016-11-12T19:20:50.180687: step 13312, loss 0.00421244, acc 1
2016-11-12T19:20:50.240400: step 13313, loss 0.00125981, acc 1
2016-11-12T19:20:50.298705: step 13314, loss 0.000439539, acc 1
2016-11-12T19:20:50.357231: step 13315, loss 0.00938955, acc 1
2016-11-12T19:20:50.415368: step 13316, loss 0.00676934, acc 1
2016-11-12T19:20:50.472553: step 13317, loss 0.0019283, acc 1
2016-11-12T19:20:50.530523: step 13318, loss 0.018286, acc 1
2016-11-12T19:20:50.588782: step 13319, loss 0.0173692, acc 0.984375
2016-11-12T19:20:50.647385: step 13320, loss 0.000893177, acc 1
2016-11-12T19:20:50.708449: step 13321, loss 0.000645885, acc 1
2016-11-12T19:20:50.767818: step 13322, loss 0.000623756, acc 1
2016-11-12T19:20:50.827665: step 13323, loss 0.0245801, acc 0.984375
2016-11-12T19:20:50.888447: step 13324, loss 0.00815017, acc 1
2016-11-12T19:20:50.944850: step 13325, loss 0.0100069, acc 1
2016-11-12T19:20:51.003142: step 13326, loss 0.000375397, acc 1
2016-11-12T19:20:51.061109: step 13327, loss 0.00018683, acc 1
2016-11-12T19:20:51.119048: step 13328, loss 0.0185599, acc 0.984375
2016-11-12T19:20:51.178041: step 13329, loss 0.00104155, acc 1
2016-11-12T19:20:51.237655: step 13330, loss 0.00511548, acc 1
2016-11-12T19:20:51.296358: step 13331, loss 0.00134562, acc 1
2016-11-12T19:20:51.355992: step 13332, loss 0.00223656, acc 1
2016-11-12T19:20:51.415968: step 13333, loss 0.00162214, acc 1
2016-11-12T19:20:51.474215: step 13334, loss 0.00165536, acc 1
2016-11-12T19:20:51.532649: step 13335, loss 0.00431571, acc 1
2016-11-12T19:20:51.592886: step 13336, loss 0.0436558, acc 0.96875
2016-11-12T19:20:51.652800: step 13337, loss 0.00100207, acc 1
2016-11-12T19:20:51.712378: step 13338, loss 0.000346459, acc 1
2016-11-12T19:20:51.772077: step 13339, loss 0.000316864, acc 1
2016-11-12T19:20:51.830118: step 13340, loss 0.00019895, acc 1
2016-11-12T19:20:51.887215: step 13341, loss 0.000488406, acc 1
2016-11-12T19:20:51.945056: step 13342, loss 0.0110297, acc 0.984375
2016-11-12T19:20:52.006528: step 13343, loss 0.00260677, acc 1
2016-11-12T19:20:52.064933: step 13344, loss 0.0185682, acc 1
2016-11-12T19:20:52.126154: step 13345, loss 2.65124e-05, acc 1
2016-11-12T19:20:52.184724: step 13346, loss 0.011849, acc 1
2016-11-12T19:20:52.243713: step 13347, loss 0.00353099, acc 1
2016-11-12T19:20:52.285164: step 13348, loss 0.000102225, acc 1
2016-11-12T19:20:52.343527: step 13349, loss 0.0019901, acc 1
2016-11-12T19:20:52.401989: step 13350, loss 0.00184004, acc 1
2016-11-12T19:20:52.461374: step 13351, loss 0.00100699, acc 1
2016-11-12T19:20:52.520848: step 13352, loss 5.39787e-05, acc 1
2016-11-12T19:20:52.580602: step 13353, loss 0.00307056, acc 1
2016-11-12T19:20:52.640852: step 13354, loss 1.28799e-05, acc 1
2016-11-12T19:20:52.697054: step 13355, loss 0.0433946, acc 0.96875
2016-11-12T19:20:52.755093: step 13356, loss 0.00407412, acc 1
2016-11-12T19:20:52.812505: step 13357, loss 0.00183643, acc 1
2016-11-12T19:20:52.872518: step 13358, loss 0.115044, acc 0.984375
2016-11-12T19:20:52.932479: step 13359, loss 0.0100325, acc 1
2016-11-12T19:20:52.992839: step 13360, loss 0.00209972, acc 1
2016-11-12T19:20:53.052653: step 13361, loss 0.00235279, acc 1
2016-11-12T19:20:53.111939: step 13362, loss 0.00935039, acc 1
2016-11-12T19:20:53.170567: step 13363, loss 0.00840968, acc 1
2016-11-12T19:20:53.232322: step 13364, loss 0.0185732, acc 0.984375
2016-11-12T19:20:53.290973: step 13365, loss 0.000708276, acc 1
2016-11-12T19:20:53.349332: step 13366, loss 2.84518e-05, acc 1
2016-11-12T19:20:53.407571: step 13367, loss 0.00190954, acc 1
2016-11-12T19:20:53.471717: step 13368, loss 0.0136651, acc 1
2016-11-12T19:20:53.532095: step 13369, loss 0.000858816, acc 1
2016-11-12T19:20:53.592867: step 13370, loss 5.85633e-05, acc 1
2016-11-12T19:20:53.655644: step 13371, loss 0.00993557, acc 1
2016-11-12T19:20:53.717580: step 13372, loss 0.000241958, acc 1
2016-11-12T19:20:53.774786: step 13373, loss 2.06033e-05, acc 1
2016-11-12T19:20:53.835091: step 13374, loss 0.000101625, acc 1
2016-11-12T19:20:53.892264: step 13375, loss 7.3161e-05, acc 1
2016-11-12T19:20:53.951102: step 13376, loss 0.0271823, acc 0.984375
2016-11-12T19:20:54.008111: step 13377, loss 0.023669, acc 0.984375
2016-11-12T19:20:54.067502: step 13378, loss 0.00216873, acc 1
2016-11-12T19:20:54.124534: step 13379, loss 0.0379262, acc 0.984375
2016-11-12T19:20:54.182780: step 13380, loss 6.87147e-05, acc 1
2016-11-12T19:20:54.241702: step 13381, loss 0.000653405, acc 1
2016-11-12T19:20:54.301410: step 13382, loss 0.00796437, acc 1
2016-11-12T19:20:54.364116: step 13383, loss 0.000134092, acc 1
2016-11-12T19:20:54.427072: step 13384, loss 0.00418658, acc 1
2016-11-12T19:20:54.485135: step 13385, loss 0.000774028, acc 1
2016-11-12T19:20:54.544599: step 13386, loss 1.59171e-05, acc 1
2016-11-12T19:20:54.601023: step 13387, loss 0.00646864, acc 1
2016-11-12T19:20:54.660138: step 13388, loss 0.019766, acc 0.984375
2016-11-12T19:20:54.720892: step 13389, loss 0.0374697, acc 0.984375
2016-11-12T19:20:54.780830: step 13390, loss 0.0360175, acc 0.984375
2016-11-12T19:20:54.840176: step 13391, loss 0.00066951, acc 1
2016-11-12T19:20:54.898659: step 13392, loss 0.000964755, acc 1
2016-11-12T19:20:54.956915: step 13393, loss 0.221837, acc 0.96875
2016-11-12T19:20:55.015868: step 13394, loss 0.00306081, acc 1
2016-11-12T19:20:55.074438: step 13395, loss 0.00794983, acc 1
2016-11-12T19:20:55.132796: step 13396, loss 0.00148923, acc 1
2016-11-12T19:20:55.192982: step 13397, loss 0.00717159, acc 1
2016-11-12T19:20:55.251353: step 13398, loss 9.10783e-05, acc 1
2016-11-12T19:20:55.308484: step 13399, loss 0.000546338, acc 1
2016-11-12T19:20:55.368640: step 13400, loss 0.000589277, acc 1

Evaluation:
2016-11-12T19:20:55.441914: step 13400, loss 5.62832, acc 0.556

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-13400

2016-11-12T19:20:55.920786: step 13401, loss 0.00303333, acc 1
2016-11-12T19:20:55.980929: step 13402, loss 0.000832953, acc 1
2016-11-12T19:20:56.041234: step 13403, loss 0.00212485, acc 1
2016-11-12T19:20:56.100114: step 13404, loss 0.0144853, acc 0.984375
2016-11-12T19:20:56.157908: step 13405, loss 0.00445026, acc 1
2016-11-12T19:20:56.216245: step 13406, loss 0.0118509, acc 0.984375
2016-11-12T19:20:56.274070: step 13407, loss 6.25129e-05, acc 1
2016-11-12T19:20:56.330257: step 13408, loss 0.00136168, acc 1
2016-11-12T19:20:56.389542: step 13409, loss 0.0143162, acc 1
2016-11-12T19:20:56.448465: step 13410, loss 0.0120112, acc 0.984375
2016-11-12T19:20:56.508324: step 13411, loss 0.0111659, acc 0.984375
2016-11-12T19:20:56.566789: step 13412, loss 0.129353, acc 0.96875
2016-11-12T19:20:56.626472: step 13413, loss 0.0182994, acc 0.984375
2016-11-12T19:20:56.688352: step 13414, loss 0.00128416, acc 1
2016-11-12T19:20:56.748584: step 13415, loss 0.000404123, acc 1
2016-11-12T19:20:56.809222: step 13416, loss 0.0049443, acc 1
2016-11-12T19:20:56.867347: step 13417, loss 0.00224013, acc 1
2016-11-12T19:20:56.926237: step 13418, loss 0.00630635, acc 1
2016-11-12T19:20:56.967198: step 13419, loss 0.000526253, acc 1
2016-11-12T19:20:57.028621: step 13420, loss 0.000146703, acc 1
2016-11-12T19:20:57.087104: step 13421, loss 8.18274e-06, acc 1
2016-11-12T19:20:57.145051: step 13422, loss 0.00810198, acc 1
2016-11-12T19:20:57.204200: step 13423, loss 0.00205658, acc 1
2016-11-12T19:20:57.263182: step 13424, loss 0.000524051, acc 1
2016-11-12T19:20:57.320807: step 13425, loss 0.0290691, acc 0.984375
2016-11-12T19:20:57.379694: step 13426, loss 0.132234, acc 0.96875
2016-11-12T19:20:57.436295: step 13427, loss 0.00539761, acc 1
2016-11-12T19:20:57.497068: step 13428, loss 6.18022e-05, acc 1
2016-11-12T19:20:57.557034: step 13429, loss 0.128372, acc 0.984375
2016-11-12T19:20:57.615962: step 13430, loss 0.0278306, acc 0.984375
2016-11-12T19:20:57.676797: step 13431, loss 0.000429234, acc 1
2016-11-12T19:20:57.735671: step 13432, loss 0.0103979, acc 1
2016-11-12T19:20:57.793419: step 13433, loss 0.0113514, acc 0.984375
2016-11-12T19:20:57.855976: step 13434, loss 0.0214714, acc 0.984375
2016-11-12T19:20:57.916153: step 13435, loss 0.000197833, acc 1
2016-11-12T19:20:57.972263: step 13436, loss 0.000130406, acc 1
2016-11-12T19:20:58.031628: step 13437, loss 0.0108196, acc 1
2016-11-12T19:20:58.092693: step 13438, loss 0.000194841, acc 1
2016-11-12T19:20:58.152754: step 13439, loss 0.0103259, acc 1
2016-11-12T19:20:58.211070: step 13440, loss 0.0113718, acc 0.984375
2016-11-12T19:20:58.271896: step 13441, loss 0.0020405, acc 1
2016-11-12T19:20:58.332334: step 13442, loss 0.00883912, acc 1
2016-11-12T19:20:58.391119: step 13443, loss 0.0642286, acc 0.984375
2016-11-12T19:20:58.453217: step 13444, loss 0.0061619, acc 1
2016-11-12T19:20:58.511999: step 13445, loss 0.000184866, acc 1
2016-11-12T19:20:58.568657: step 13446, loss 0.0110885, acc 1
2016-11-12T19:20:58.627444: step 13447, loss 0.0112647, acc 1
2016-11-12T19:20:58.685605: step 13448, loss 0.00322091, acc 1
2016-11-12T19:20:58.744748: step 13449, loss 0.134939, acc 0.984375
2016-11-12T19:20:58.806929: step 13450, loss 0.00896398, acc 1
2016-11-12T19:20:58.865895: step 13451, loss 0.000141267, acc 1
2016-11-12T19:20:58.923046: step 13452, loss 0.000608926, acc 1
2016-11-12T19:20:58.983097: step 13453, loss 0.0121123, acc 0.984375
2016-11-12T19:20:59.041791: step 13454, loss 0.00789709, acc 1
2016-11-12T19:20:59.100168: step 13455, loss 0.00026064, acc 1
2016-11-12T19:20:59.157414: step 13456, loss 0.0109946, acc 1
2016-11-12T19:20:59.219570: step 13457, loss 0.0210791, acc 0.984375
2016-11-12T19:20:59.280897: step 13458, loss 0.000278821, acc 1
2016-11-12T19:20:59.340005: step 13459, loss 0.0186304, acc 0.984375
2016-11-12T19:20:59.399658: step 13460, loss 0.00981417, acc 1
2016-11-12T19:20:59.457982: step 13461, loss 3.98246e-05, acc 1
2016-11-12T19:20:59.514552: step 13462, loss 0.00077634, acc 1
2016-11-12T19:20:59.572904: step 13463, loss 0.000762094, acc 1
2016-11-12T19:20:59.633638: step 13464, loss 0.000693063, acc 1
2016-11-12T19:20:59.692255: step 13465, loss 0.000264377, acc 1
2016-11-12T19:20:59.751137: step 13466, loss 0.0463724, acc 0.953125
2016-11-12T19:20:59.810066: step 13467, loss 0.0231213, acc 0.984375
2016-11-12T19:20:59.872739: step 13468, loss 8.41873e-05, acc 1
2016-11-12T19:20:59.930154: step 13469, loss 2.038e-05, acc 1
2016-11-12T19:20:59.991036: step 13470, loss 0.0387275, acc 0.984375
2016-11-12T19:21:00.051717: step 13471, loss 0.00113599, acc 1
2016-11-12T19:21:00.114337: step 13472, loss 2.98492e-05, acc 1
2016-11-12T19:21:00.172776: step 13473, loss 0.00971858, acc 1
2016-11-12T19:21:00.231381: step 13474, loss 0.0121299, acc 0.984375
2016-11-12T19:21:00.291418: step 13475, loss 0.00104751, acc 1
2016-11-12T19:21:00.349227: step 13476, loss 0.0398315, acc 0.984375
2016-11-12T19:21:00.408636: step 13477, loss 5.31946e-05, acc 1
2016-11-12T19:21:00.465290: step 13478, loss 0.000438541, acc 1
2016-11-12T19:21:00.523707: step 13479, loss 0.0133627, acc 0.984375
2016-11-12T19:21:00.582212: step 13480, loss 0.00780807, acc 1
2016-11-12T19:21:00.641856: step 13481, loss 0.0112399, acc 0.984375
2016-11-12T19:21:00.700768: step 13482, loss 0.000213879, acc 1
2016-11-12T19:21:00.758607: step 13483, loss 0.00186554, acc 1
2016-11-12T19:21:00.816712: step 13484, loss 0.00527206, acc 1
2016-11-12T19:21:00.874029: step 13485, loss 0.0234779, acc 0.984375
2016-11-12T19:21:00.937174: step 13486, loss 0.0157932, acc 0.984375
2016-11-12T19:21:00.996711: step 13487, loss 8.9325e-06, acc 1
2016-11-12T19:21:01.053067: step 13488, loss 0.00159081, acc 1
2016-11-12T19:21:01.111363: step 13489, loss 0.00261082, acc 1
2016-11-12T19:21:01.151754: step 13490, loss 5.5587e-05, acc 1
2016-11-12T19:21:01.209883: step 13491, loss 0.00935323, acc 1
2016-11-12T19:21:01.268960: step 13492, loss 0.000184726, acc 1
2016-11-12T19:21:01.326865: step 13493, loss 0.0102255, acc 1
2016-11-12T19:21:01.385307: step 13494, loss 0.00307255, acc 1
2016-11-12T19:21:01.443568: step 13495, loss 8.47605e-06, acc 1
2016-11-12T19:21:01.500032: step 13496, loss 0.0241778, acc 0.984375
2016-11-12T19:21:01.560888: step 13497, loss 3.33188e-05, acc 1
2016-11-12T19:21:01.620466: step 13498, loss 0.00133103, acc 1
2016-11-12T19:21:01.679489: step 13499, loss 0.0155655, acc 1
2016-11-12T19:21:01.741294: step 13500, loss 0.000129633, acc 1

Evaluation:
2016-11-12T19:21:01.814133: step 13500, loss 5.70682, acc 0.538

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-13500

2016-11-12T19:21:02.292596: step 13501, loss 0.011527, acc 0.984375
2016-11-12T19:21:02.354828: step 13502, loss 0.0315952, acc 0.984375
2016-11-12T19:21:02.416898: step 13503, loss 0.00328085, acc 1
2016-11-12T19:21:02.477529: step 13504, loss 0.000625209, acc 1
2016-11-12T19:21:02.536000: step 13505, loss 0.0192821, acc 1
2016-11-12T19:21:02.596653: step 13506, loss 0.00017167, acc 1
2016-11-12T19:21:02.656732: step 13507, loss 0.000128409, acc 1
2016-11-12T19:21:02.714494: step 13508, loss 0.00789289, acc 1
2016-11-12T19:21:02.775386: step 13509, loss 0.0124073, acc 1
2016-11-12T19:21:02.834764: step 13510, loss 0.0117713, acc 0.984375
2016-11-12T19:21:02.896290: step 13511, loss 0.0544636, acc 0.96875
2016-11-12T19:21:02.955431: step 13512, loss 0.000202728, acc 1
2016-11-12T19:21:03.012516: step 13513, loss 0.0100641, acc 1
2016-11-12T19:21:03.069411: step 13514, loss 0.000589784, acc 1
2016-11-12T19:21:03.129261: step 13515, loss 0.00032787, acc 1
2016-11-12T19:21:03.188326: step 13516, loss 0.00195419, acc 1
2016-11-12T19:21:03.246562: step 13517, loss 0.00128839, acc 1
2016-11-12T19:21:03.304765: step 13518, loss 0.000270126, acc 1
2016-11-12T19:21:03.362620: step 13519, loss 0.00494203, acc 1
2016-11-12T19:21:03.422581: step 13520, loss 0.000365034, acc 1
2016-11-12T19:21:03.481893: step 13521, loss 0.00231197, acc 1
2016-11-12T19:21:03.541779: step 13522, loss 0.000364763, acc 1
2016-11-12T19:21:03.601195: step 13523, loss 0.00100992, acc 1
2016-11-12T19:21:03.658173: step 13524, loss 0.00242284, acc 1
2016-11-12T19:21:03.716075: step 13525, loss 0.000247948, acc 1
2016-11-12T19:21:03.773680: step 13526, loss 0.00837627, acc 1
2016-11-12T19:21:03.832954: step 13527, loss 0.00248715, acc 1
2016-11-12T19:21:03.891652: step 13528, loss 0.00409389, acc 1
2016-11-12T19:21:03.950100: step 13529, loss 0.0115425, acc 0.984375
2016-11-12T19:21:04.008867: step 13530, loss 0.00293097, acc 1
2016-11-12T19:21:04.069900: step 13531, loss 0.0518325, acc 0.96875
2016-11-12T19:21:04.132531: step 13532, loss 0.00155938, acc 1
2016-11-12T19:21:04.192560: step 13533, loss 0.00017685, acc 1
2016-11-12T19:21:04.252984: step 13534, loss 0.000174195, acc 1
2016-11-12T19:21:04.312651: step 13535, loss 0.00179689, acc 1
2016-11-12T19:21:04.373267: step 13536, loss 0.00144356, acc 1
2016-11-12T19:21:04.432777: step 13537, loss 0.023169, acc 0.984375
2016-11-12T19:21:04.490633: step 13538, loss 0.000160346, acc 1
2016-11-12T19:21:04.547488: step 13539, loss 0.00016133, acc 1
2016-11-12T19:21:04.605340: step 13540, loss 0.000103117, acc 1
2016-11-12T19:21:04.665600: step 13541, loss 0.00321363, acc 1
2016-11-12T19:21:04.723681: step 13542, loss 0.00593464, acc 1
2016-11-12T19:21:04.782065: step 13543, loss 0.0587752, acc 0.984375
2016-11-12T19:21:04.840650: step 13544, loss 0.00817443, acc 1
2016-11-12T19:21:04.899893: step 13545, loss 0.00209745, acc 1
2016-11-12T19:21:04.957298: step 13546, loss 0.0151147, acc 0.984375
2016-11-12T19:21:05.015157: step 13547, loss 0.0030767, acc 1
2016-11-12T19:21:05.072388: step 13548, loss 0.0155109, acc 1
2016-11-12T19:21:05.132036: step 13549, loss 0.000558517, acc 1
2016-11-12T19:21:05.189823: step 13550, loss 0.000482135, acc 1
2016-11-12T19:21:05.249334: step 13551, loss 0.0132861, acc 0.984375
2016-11-12T19:21:05.308983: step 13552, loss 0.00118563, acc 1
2016-11-12T19:21:05.368831: step 13553, loss 0.000278714, acc 1
2016-11-12T19:21:05.426491: step 13554, loss 0.00987203, acc 1
2016-11-12T19:21:05.483563: step 13555, loss 1.27612e-05, acc 1
2016-11-12T19:21:05.543367: step 13556, loss 0.000633499, acc 1
2016-11-12T19:21:05.601078: step 13557, loss 0.00354056, acc 1
2016-11-12T19:21:05.659967: step 13558, loss 5.37439e-05, acc 1
2016-11-12T19:21:05.719260: step 13559, loss 3.53948e-05, acc 1
2016-11-12T19:21:05.776553: step 13560, loss 0.0266794, acc 0.96875
2016-11-12T19:21:05.815334: step 13561, loss 0.0275361, acc 1
2016-11-12T19:21:05.875705: step 13562, loss 0.0169087, acc 0.984375
2016-11-12T19:21:05.935389: step 13563, loss 0.0315378, acc 0.984375
2016-11-12T19:21:05.994810: step 13564, loss 9.51519e-05, acc 1
2016-11-12T19:21:06.052063: step 13565, loss 0.00193081, acc 1
2016-11-12T19:21:06.113327: step 13566, loss 0.000531187, acc 1
2016-11-12T19:21:06.173042: step 13567, loss 0.0168678, acc 1
2016-11-12T19:21:06.234623: step 13568, loss 0.000269618, acc 1
2016-11-12T19:21:06.296509: step 13569, loss 0.00196889, acc 1
2016-11-12T19:21:06.356522: step 13570, loss 0.00389038, acc 1
2016-11-12T19:21:06.417727: step 13571, loss 0.000199743, acc 1
2016-11-12T19:21:06.475871: step 13572, loss 0.0014942, acc 1
2016-11-12T19:21:06.536910: step 13573, loss 0.017264, acc 1
2016-11-12T19:21:06.594907: step 13574, loss 0.025905, acc 0.96875
2016-11-12T19:21:06.656238: step 13575, loss 3.31891e-05, acc 1
2016-11-12T19:21:06.712977: step 13576, loss 0.000179015, acc 1
2016-11-12T19:21:06.772680: step 13577, loss 0.000679914, acc 1
2016-11-12T19:21:06.831709: step 13578, loss 2.26609e-05, acc 1
2016-11-12T19:21:06.889144: step 13579, loss 0.0139657, acc 0.984375
2016-11-12T19:21:06.951330: step 13580, loss 0.000335365, acc 1
2016-11-12T19:21:07.008819: step 13581, loss 6.73131e-05, acc 1
2016-11-12T19:21:07.068652: step 13582, loss 0.000403635, acc 1
2016-11-12T19:21:07.126323: step 13583, loss 0.00196957, acc 1
2016-11-12T19:21:07.185077: step 13584, loss 0.000423145, acc 1
2016-11-12T19:21:07.246363: step 13585, loss 0.000981911, acc 1
2016-11-12T19:21:07.304783: step 13586, loss 0.000178435, acc 1
2016-11-12T19:21:07.363918: step 13587, loss 3.32067e-05, acc 1
2016-11-12T19:21:07.421633: step 13588, loss 0.000126762, acc 1
2016-11-12T19:21:07.480607: step 13589, loss 0.0292548, acc 0.984375
2016-11-12T19:21:07.541551: step 13590, loss 0.000254067, acc 1
2016-11-12T19:21:07.599870: step 13591, loss 0.000260803, acc 1
2016-11-12T19:21:07.658241: step 13592, loss 0.000377095, acc 1
2016-11-12T19:21:07.716056: step 13593, loss 0.0234409, acc 0.984375
2016-11-12T19:21:07.775104: step 13594, loss 7.59314e-06, acc 1
2016-11-12T19:21:07.831486: step 13595, loss 8.69034e-06, acc 1
2016-11-12T19:21:07.888808: step 13596, loss 4.02623e-05, acc 1
2016-11-12T19:21:07.946172: step 13597, loss 0.011169, acc 1
2016-11-12T19:21:08.005399: step 13598, loss 0.000346836, acc 1
2016-11-12T19:21:08.064602: step 13599, loss 0.00479037, acc 1
2016-11-12T19:21:08.122521: step 13600, loss 6.11149e-05, acc 1

Evaluation:
2016-11-12T19:21:08.194487: step 13600, loss 5.74619, acc 0.54

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-13600

2016-11-12T19:21:08.674586: step 13601, loss 0.0164341, acc 0.984375
2016-11-12T19:21:08.735593: step 13602, loss 0.000292504, acc 1
2016-11-12T19:21:08.792974: step 13603, loss 0.000160985, acc 1
2016-11-12T19:21:08.852961: step 13604, loss 0.084865, acc 0.984375
2016-11-12T19:21:08.912790: step 13605, loss 1.72851e-06, acc 1
2016-11-12T19:21:08.969983: step 13606, loss 0.0157734, acc 1
2016-11-12T19:21:09.028527: step 13607, loss 4.47596e-05, acc 1
2016-11-12T19:21:09.089555: step 13608, loss 0.0276929, acc 0.984375
2016-11-12T19:21:09.151274: step 13609, loss 0.00267494, acc 1
2016-11-12T19:21:09.212097: step 13610, loss 0.060141, acc 0.984375
2016-11-12T19:21:09.273845: step 13611, loss 0.029996, acc 0.984375
2016-11-12T19:21:09.330952: step 13612, loss 0.000172224, acc 1
2016-11-12T19:21:09.390469: step 13613, loss 0.000173372, acc 1
2016-11-12T19:21:09.448744: step 13614, loss 0.00516991, acc 1
2016-11-12T19:21:09.512188: step 13615, loss 0.00961771, acc 1
2016-11-12T19:21:09.569948: step 13616, loss 0.000275747, acc 1
2016-11-12T19:21:09.629064: step 13617, loss 0.0128444, acc 0.984375
2016-11-12T19:21:09.688750: step 13618, loss 0.000539374, acc 1
2016-11-12T19:21:09.748198: step 13619, loss 0.0783838, acc 0.96875
2016-11-12T19:21:09.808734: step 13620, loss 0.0545769, acc 0.984375
2016-11-12T19:21:09.872428: step 13621, loss 0.00856854, acc 1
2016-11-12T19:21:09.928928: step 13622, loss 7.88606e-06, acc 1
2016-11-12T19:21:09.988432: step 13623, loss 0.0013214, acc 1
2016-11-12T19:21:10.048745: step 13624, loss 0.00787648, acc 1
2016-11-12T19:21:10.109027: step 13625, loss 0.0455663, acc 0.984375
2016-11-12T19:21:10.168722: step 13626, loss 0.0213284, acc 0.984375
2016-11-12T19:21:10.230088: step 13627, loss 0.0198442, acc 0.984375
2016-11-12T19:21:10.287845: step 13628, loss 0.000997156, acc 1
2016-11-12T19:21:10.344736: step 13629, loss 0.0084148, acc 1
2016-11-12T19:21:10.402822: step 13630, loss 0.0129896, acc 0.984375
2016-11-12T19:21:10.461597: step 13631, loss 0.00221184, acc 1
2016-11-12T19:21:10.503902: step 13632, loss 0.000697722, acc 1
2016-11-12T19:21:10.563650: step 13633, loss 0.0248456, acc 0.984375
2016-11-12T19:21:10.624562: step 13634, loss 0.000366809, acc 1
2016-11-12T19:21:10.683515: step 13635, loss 0.020645, acc 0.984375
2016-11-12T19:21:10.744693: step 13636, loss 0.000354428, acc 1
2016-11-12T19:21:10.805275: step 13637, loss 0.0373482, acc 0.96875
2016-11-12T19:21:10.865581: step 13638, loss 0.000927444, acc 1
2016-11-12T19:21:10.925463: step 13639, loss 0.00485693, acc 1
2016-11-12T19:21:10.983837: step 13640, loss 0.00389064, acc 1
2016-11-12T19:21:11.043158: step 13641, loss 8.84395e-05, acc 1
2016-11-12T19:21:11.099663: step 13642, loss 7.65635e-05, acc 1
2016-11-12T19:21:11.156852: step 13643, loss 0.00010645, acc 1
2016-11-12T19:21:11.214583: step 13644, loss 0.0360003, acc 0.984375
2016-11-12T19:21:11.273120: step 13645, loss 0.000119685, acc 1
2016-11-12T19:21:11.331613: step 13646, loss 0.00983844, acc 1
2016-11-12T19:21:11.391524: step 13647, loss 0.0110506, acc 1
2016-11-12T19:21:11.449470: step 13648, loss 0.0104277, acc 1
2016-11-12T19:21:11.508884: step 13649, loss 0.000158204, acc 1
2016-11-12T19:21:11.565795: step 13650, loss 0.0120381, acc 1
2016-11-12T19:21:11.626735: step 13651, loss 0.0137829, acc 0.984375
2016-11-12T19:21:11.684248: step 13652, loss 0.0150198, acc 1
2016-11-12T19:21:11.743043: step 13653, loss 0.00926068, acc 1
2016-11-12T19:21:11.801679: step 13654, loss 0.000203292, acc 1
2016-11-12T19:21:11.858371: step 13655, loss 0.000817559, acc 1
2016-11-12T19:21:11.916791: step 13656, loss 3.28496e-05, acc 1
2016-11-12T19:21:11.975305: step 13657, loss 0.000543386, acc 1
2016-11-12T19:21:12.033100: step 13658, loss 0.00143862, acc 1
2016-11-12T19:21:12.091826: step 13659, loss 0.0161118, acc 0.984375
2016-11-12T19:21:12.152336: step 13660, loss 0.000274734, acc 1
2016-11-12T19:21:12.211019: step 13661, loss 9.78442e-05, acc 1
2016-11-12T19:21:12.267755: step 13662, loss 0.00129137, acc 1
2016-11-12T19:21:12.325285: step 13663, loss 0.0163964, acc 0.984375
2016-11-12T19:21:12.389588: step 13664, loss 0.0100558, acc 1
2016-11-12T19:21:12.449530: step 13665, loss 0.0159872, acc 1
2016-11-12T19:21:12.508553: step 13666, loss 0.0367061, acc 0.96875
2016-11-12T19:21:12.566263: step 13667, loss 8.95998e-06, acc 1
2016-11-12T19:21:12.624721: step 13668, loss 0.00339464, acc 1
2016-11-12T19:21:12.682975: step 13669, loss 5.6266e-05, acc 1
2016-11-12T19:21:12.740558: step 13670, loss 0.000354568, acc 1
2016-11-12T19:21:12.798359: step 13671, loss 0.000107973, acc 1
2016-11-12T19:21:12.856694: step 13672, loss 0.00146612, acc 1
2016-11-12T19:21:12.914983: step 13673, loss 0.00669485, acc 1
2016-11-12T19:21:12.975745: step 13674, loss 0.000299504, acc 1
2016-11-12T19:21:13.036648: step 13675, loss 3.19531e-05, acc 1
2016-11-12T19:21:13.095236: step 13676, loss 0.00188785, acc 1
2016-11-12T19:21:13.152771: step 13677, loss 0.000229993, acc 1
2016-11-12T19:21:13.211288: step 13678, loss 0.0673525, acc 0.984375
2016-11-12T19:21:13.272556: step 13679, loss 0.000703604, acc 1
2016-11-12T19:21:13.333472: step 13680, loss 0.000838775, acc 1
2016-11-12T19:21:13.392431: step 13681, loss 0.000628249, acc 1
2016-11-12T19:21:13.450715: step 13682, loss 9.03472e-06, acc 1
2016-11-12T19:21:13.507364: step 13683, loss 0.000490461, acc 1
2016-11-12T19:21:13.565195: step 13684, loss 8.5651e-05, acc 1
2016-11-12T19:21:13.624679: step 13685, loss 0.000231609, acc 1
2016-11-12T19:21:13.683989: step 13686, loss 0.000109428, acc 1
2016-11-12T19:21:13.740662: step 13687, loss 0.000485822, acc 1
2016-11-12T19:21:13.800356: step 13688, loss 7.5609e-05, acc 1
2016-11-12T19:21:13.859922: step 13689, loss 0.00252686, acc 1
2016-11-12T19:21:13.918174: step 13690, loss 0.0273844, acc 0.984375
2016-11-12T19:21:13.979386: step 13691, loss 0.000402081, acc 1
2016-11-12T19:21:14.036172: step 13692, loss 0.0177226, acc 1
2016-11-12T19:21:14.095807: step 13693, loss 0.000239013, acc 1
2016-11-12T19:21:14.153927: step 13694, loss 0.0214981, acc 0.984375
2016-11-12T19:21:14.211909: step 13695, loss 0.00226135, acc 1
2016-11-12T19:21:14.271000: step 13696, loss 0.0109845, acc 1
2016-11-12T19:21:14.329913: step 13697, loss 5.85143e-05, acc 1
2016-11-12T19:21:14.388959: step 13698, loss 0.00815342, acc 1
2016-11-12T19:21:14.448320: step 13699, loss 0.0196347, acc 0.984375
2016-11-12T19:21:14.507514: step 13700, loss 0.000258765, acc 1

Evaluation:
2016-11-12T19:21:14.580621: step 13700, loss 5.7723, acc 0.536

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-13700

2016-11-12T19:21:15.058728: step 13701, loss 3.90946e-06, acc 1
2016-11-12T19:21:15.117516: step 13702, loss 0.009754, acc 1
2016-11-12T19:21:15.159942: step 13703, loss 0.00048255, acc 1
2016-11-12T19:21:15.218662: step 13704, loss 0.00237667, acc 1
2016-11-12T19:21:15.278966: step 13705, loss 0.00128511, acc 1
2016-11-12T19:21:15.340647: step 13706, loss 1.987e-05, acc 1
2016-11-12T19:21:15.398286: step 13707, loss 0.00731708, acc 1
2016-11-12T19:21:15.456953: step 13708, loss 4.72883e-06, acc 1
2016-11-12T19:21:15.513444: step 13709, loss 0.000246079, acc 1
2016-11-12T19:21:15.573781: step 13710, loss 0.00240852, acc 1
2016-11-12T19:21:15.632673: step 13711, loss 2.91251e-05, acc 1
2016-11-12T19:21:15.689372: step 13712, loss 0.000153072, acc 1
2016-11-12T19:21:15.746718: step 13713, loss 1.53437e-05, acc 1
2016-11-12T19:21:15.805030: step 13714, loss 0.00157806, acc 1
2016-11-12T19:21:15.863425: step 13715, loss 0.0257224, acc 0.984375
2016-11-12T19:21:15.922024: step 13716, loss 0.000188282, acc 1
2016-11-12T19:21:15.981048: step 13717, loss 0.000573587, acc 1
2016-11-12T19:21:16.044368: step 13718, loss 0.000225461, acc 1
2016-11-12T19:21:16.104681: step 13719, loss 0.00407831, acc 1
2016-11-12T19:21:16.164725: step 13720, loss 0.011391, acc 1
2016-11-12T19:21:16.225522: step 13721, loss 0.000116533, acc 1
2016-11-12T19:21:16.282448: step 13722, loss 0.00130301, acc 1
2016-11-12T19:21:16.340365: step 13723, loss 0.00435664, acc 1
2016-11-12T19:21:16.400711: step 13724, loss 0.000197151, acc 1
2016-11-12T19:21:16.459980: step 13725, loss 7.71456e-06, acc 1
2016-11-12T19:21:16.519047: step 13726, loss 0.00045204, acc 1
2016-11-12T19:21:16.579751: step 13727, loss 8.64119e-05, acc 1
2016-11-12T19:21:16.641206: step 13728, loss 0.00891386, acc 1
2016-11-12T19:21:16.698343: step 13729, loss 0.0185848, acc 0.984375
2016-11-12T19:21:16.758616: step 13730, loss 1.40407e-05, acc 1
2016-11-12T19:21:16.816365: step 13731, loss 0.000282064, acc 1
2016-11-12T19:21:16.875484: step 13732, loss 0.00886438, acc 1
2016-11-12T19:21:16.936063: step 13733, loss 9.81205e-05, acc 1
2016-11-12T19:21:16.996291: step 13734, loss 0.000161959, acc 1
2016-11-12T19:21:17.056531: step 13735, loss 0.0105226, acc 1
2016-11-12T19:21:17.115337: step 13736, loss 0.000490173, acc 1
2016-11-12T19:21:17.172532: step 13737, loss 0.147669, acc 0.984375
2016-11-12T19:21:17.231062: step 13738, loss 0.0142173, acc 1
2016-11-12T19:21:17.290000: step 13739, loss 5.39661e-05, acc 1
2016-11-12T19:21:17.348718: step 13740, loss 0.00012085, acc 1
2016-11-12T19:21:17.405265: step 13741, loss 0.039468, acc 0.984375
2016-11-12T19:21:17.463522: step 13742, loss 0.0285731, acc 0.96875
2016-11-12T19:21:17.525175: step 13743, loss 6.64405e-05, acc 1
2016-11-12T19:21:17.584772: step 13744, loss 0.0129175, acc 0.984375
2016-11-12T19:21:17.644844: step 13745, loss 0.0129459, acc 0.984375
2016-11-12T19:21:17.703016: step 13746, loss 0.000764385, acc 1
2016-11-12T19:21:17.761252: step 13747, loss 0.00303231, acc 1
2016-11-12T19:21:17.822819: step 13748, loss 0.0212365, acc 0.984375
2016-11-12T19:21:17.881818: step 13749, loss 0.00546843, acc 1
2016-11-12T19:21:17.940590: step 13750, loss 0.000900501, acc 1
2016-11-12T19:21:17.998408: step 13751, loss 8.87235e-06, acc 1
2016-11-12T19:21:18.057094: step 13752, loss 3.94953e-05, acc 1
2016-11-12T19:21:18.115388: step 13753, loss 0.000672086, acc 1
2016-11-12T19:21:18.173540: step 13754, loss 0.00650123, acc 1
2016-11-12T19:21:18.232825: step 13755, loss 0.0143637, acc 0.984375
2016-11-12T19:21:18.291468: step 13756, loss 0.0162325, acc 0.984375
2016-11-12T19:21:18.353002: step 13757, loss 2.98716e-05, acc 1
2016-11-12T19:21:18.410828: step 13758, loss 0.0545177, acc 0.984375
2016-11-12T19:21:18.468397: step 13759, loss 0.0105498, acc 1
2016-11-12T19:21:18.525952: step 13760, loss 0.000415571, acc 1
2016-11-12T19:21:18.583758: step 13761, loss 5.37317e-05, acc 1
2016-11-12T19:21:18.641037: step 13762, loss 0.000848232, acc 1
2016-11-12T19:21:18.703459: step 13763, loss 4.22005e-05, acc 1
2016-11-12T19:21:18.761026: step 13764, loss 0.0200641, acc 0.984375
2016-11-12T19:21:18.821277: step 13765, loss 0.0285244, acc 0.984375
2016-11-12T19:21:18.880421: step 13766, loss 0.01885, acc 0.984375
2016-11-12T19:21:18.939508: step 13767, loss 0.0173434, acc 0.984375
2016-11-12T19:21:18.998182: step 13768, loss 6.75121e-05, acc 1
2016-11-12T19:21:19.058410: step 13769, loss 2.08093e-05, acc 1
2016-11-12T19:21:19.115976: step 13770, loss 3.33918e-05, acc 1
2016-11-12T19:21:19.175750: step 13771, loss 0.000687878, acc 1
2016-11-12T19:21:19.234591: step 13772, loss 3.0217e-05, acc 1
2016-11-12T19:21:19.292293: step 13773, loss 0.0135401, acc 0.984375
2016-11-12T19:21:19.332693: step 13774, loss 0.0961868, acc 0.95
2016-11-12T19:21:19.392277: step 13775, loss 4.44486e-05, acc 1
2016-11-12T19:21:19.450123: step 13776, loss 1.47381e-05, acc 1
2016-11-12T19:21:19.507518: step 13777, loss 0.000267016, acc 1
2016-11-12T19:21:19.565272: step 13778, loss 0.0288893, acc 0.984375
2016-11-12T19:21:19.625003: step 13779, loss 9.04204e-05, acc 1
2016-11-12T19:21:19.682969: step 13780, loss 0.000544745, acc 1
2016-11-12T19:21:19.740222: step 13781, loss 0.000191677, acc 1
2016-11-12T19:21:19.799855: step 13782, loss 0.0163376, acc 1
2016-11-12T19:21:19.860292: step 13783, loss 0.0159548, acc 0.984375
2016-11-12T19:21:19.918165: step 13784, loss 0.000328768, acc 1
2016-11-12T19:21:19.976733: step 13785, loss 0.0308477, acc 0.984375
2016-11-12T19:21:20.036505: step 13786, loss 0.0426046, acc 0.984375
2016-11-12T19:21:20.097245: step 13787, loss 4.4907e-05, acc 1
2016-11-12T19:21:20.156604: step 13788, loss 3.61888e-06, acc 1
2016-11-12T19:21:20.213384: step 13789, loss 0.000384818, acc 1
2016-11-12T19:21:20.274286: step 13790, loss 0.0136729, acc 0.984375
2016-11-12T19:21:20.331926: step 13791, loss 0.00633504, acc 1
2016-11-12T19:21:20.390686: step 13792, loss 0.0313141, acc 0.984375
2016-11-12T19:21:20.452287: step 13793, loss 0.0117568, acc 0.984375
2016-11-12T19:21:20.509917: step 13794, loss 0.00194566, acc 1
2016-11-12T19:21:20.572540: step 13795, loss 0.00112305, acc 1
2016-11-12T19:21:20.633308: step 13796, loss 6.77766e-05, acc 1
2016-11-12T19:21:20.691975: step 13797, loss 0.000670707, acc 1
2016-11-12T19:21:20.752392: step 13798, loss 0.0181374, acc 0.984375
2016-11-12T19:21:20.811782: step 13799, loss 0.000104615, acc 1
2016-11-12T19:21:20.871653: step 13800, loss 0.000275193, acc 1

Evaluation:
2016-11-12T19:21:20.943808: step 13800, loss 5.7185, acc 0.542

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-13800

2016-11-12T19:21:21.425868: step 13801, loss 0.00040057, acc 1
2016-11-12T19:21:21.483576: step 13802, loss 0.0097104, acc 1
2016-11-12T19:21:21.541394: step 13803, loss 0.0229658, acc 0.984375
2016-11-12T19:21:21.600386: step 13804, loss 0.000120254, acc 1
2016-11-12T19:21:21.659527: step 13805, loss 0.000266911, acc 1
2016-11-12T19:21:21.716834: step 13806, loss 2.85254e-05, acc 1
2016-11-12T19:21:21.775869: step 13807, loss 0.000247803, acc 1
2016-11-12T19:21:21.833652: step 13808, loss 0.000741526, acc 1
2016-11-12T19:21:21.892834: step 13809, loss 0.0189507, acc 1
2016-11-12T19:21:21.950978: step 13810, loss 2.13977e-05, acc 1
2016-11-12T19:21:22.008993: step 13811, loss 0.000849325, acc 1
2016-11-12T19:21:22.067737: step 13812, loss 2.54107e-05, acc 1
2016-11-12T19:21:22.124527: step 13813, loss 2.07771e-05, acc 1
2016-11-12T19:21:22.185220: step 13814, loss 0.0132437, acc 0.984375
2016-11-12T19:21:22.243546: step 13815, loss 0.000387957, acc 1
2016-11-12T19:21:22.302033: step 13816, loss 0.000279825, acc 1
2016-11-12T19:21:22.360900: step 13817, loss 0.000763223, acc 1
2016-11-12T19:21:22.420699: step 13818, loss 0.00982109, acc 1
2016-11-12T19:21:22.478490: step 13819, loss 0.00709899, acc 1
2016-11-12T19:21:22.538344: step 13820, loss 0.000106522, acc 1
2016-11-12T19:21:22.596842: step 13821, loss 0.0485084, acc 0.984375
2016-11-12T19:21:22.659166: step 13822, loss 0.00704414, acc 1
2016-11-12T19:21:22.716944: step 13823, loss 0.172886, acc 0.96875
2016-11-12T19:21:22.777531: step 13824, loss 2.30778e-05, acc 1
2016-11-12T19:21:22.835497: step 13825, loss 0.000113154, acc 1
2016-11-12T19:21:22.895691: step 13826, loss 0.0094765, acc 1
2016-11-12T19:21:22.958182: step 13827, loss 0.0136341, acc 0.984375
2016-11-12T19:21:23.018293: step 13828, loss 7.55475e-05, acc 1
2016-11-12T19:21:23.075812: step 13829, loss 0.0165614, acc 0.984375
2016-11-12T19:21:23.136162: step 13830, loss 0.00131982, acc 1
2016-11-12T19:21:23.196920: step 13831, loss 1.33188e-05, acc 1
2016-11-12T19:21:23.252435: step 13832, loss 0.0481428, acc 0.984375
2016-11-12T19:21:23.311319: step 13833, loss 0.0229154, acc 0.984375
2016-11-12T19:21:23.370880: step 13834, loss 0.0016834, acc 1
2016-11-12T19:21:23.430700: step 13835, loss 0.00140998, acc 1
2016-11-12T19:21:23.489178: step 13836, loss 0.00016207, acc 1
2016-11-12T19:21:23.548214: step 13837, loss 0.0148981, acc 0.984375
2016-11-12T19:21:23.606145: step 13838, loss 0.00110716, acc 1
2016-11-12T19:21:23.664760: step 13839, loss 0.00521715, acc 1
2016-11-12T19:21:23.724717: step 13840, loss 6.2647e-05, acc 1
2016-11-12T19:21:23.782429: step 13841, loss 0.000479259, acc 1
2016-11-12T19:21:23.840836: step 13842, loss 0.0187433, acc 0.984375
2016-11-12T19:21:23.900489: step 13843, loss 0.00890824, acc 1
2016-11-12T19:21:23.958297: step 13844, loss 0.000483105, acc 1
2016-11-12T19:21:23.996725: step 13845, loss 2.9682e-05, acc 1
2016-11-12T19:21:24.057790: step 13846, loss 0.00907005, acc 1
2016-11-12T19:21:24.118344: step 13847, loss 0.00367611, acc 1
2016-11-12T19:21:24.177591: step 13848, loss 0.0115736, acc 1
2016-11-12T19:21:24.236848: step 13849, loss 0.00236084, acc 1
2016-11-12T19:21:24.297551: step 13850, loss 0.000913805, acc 1
2016-11-12T19:21:24.356500: step 13851, loss 0.0151659, acc 0.984375
2016-11-12T19:21:24.417100: step 13852, loss 0.100438, acc 0.984375
2016-11-12T19:21:24.477124: step 13853, loss 0.000385952, acc 1
2016-11-12T19:21:24.534468: step 13854, loss 0.00311919, acc 1
2016-11-12T19:21:24.593886: step 13855, loss 0.0299012, acc 0.984375
2016-11-12T19:21:24.654305: step 13856, loss 2.54679e-05, acc 1
2016-11-12T19:21:24.711294: step 13857, loss 0.0290315, acc 0.984375
2016-11-12T19:21:24.770753: step 13858, loss 0.011942, acc 0.984375
2016-11-12T19:21:24.829075: step 13859, loss 0.00685182, acc 1
2016-11-12T19:21:24.887792: step 13860, loss 0.00135112, acc 1
2016-11-12T19:21:24.947246: step 13861, loss 0.00634955, acc 1
2016-11-12T19:21:25.009018: step 13862, loss 0.000984636, acc 1
2016-11-12T19:21:25.066626: step 13863, loss 0.00167484, acc 1
2016-11-12T19:21:25.125220: step 13864, loss 0.00194981, acc 1
2016-11-12T19:21:25.184543: step 13865, loss 0.000143784, acc 1
2016-11-12T19:21:25.242443: step 13866, loss 0.141612, acc 0.984375
2016-11-12T19:21:25.304062: step 13867, loss 0.190878, acc 0.96875
2016-11-12T19:21:25.364765: step 13868, loss 0.00442551, acc 1
2016-11-12T19:21:25.425175: step 13869, loss 0.000466253, acc 1
2016-11-12T19:21:25.484394: step 13870, loss 0.0511103, acc 0.984375
2016-11-12T19:21:25.543791: step 13871, loss 0.0148087, acc 0.984375
2016-11-12T19:21:25.602368: step 13872, loss 0.0126874, acc 0.984375
2016-11-12T19:21:25.661097: step 13873, loss 0.0154063, acc 1
2016-11-12T19:21:25.719309: step 13874, loss 0.000248818, acc 1
2016-11-12T19:21:25.780142: step 13875, loss 0.0184794, acc 1
2016-11-12T19:21:25.838539: step 13876, loss 0.00819674, acc 1
2016-11-12T19:21:25.897519: step 13877, loss 6.02131e-05, acc 1
2016-11-12T19:21:25.955011: step 13878, loss 0.0240629, acc 0.984375
2016-11-12T19:21:26.013998: step 13879, loss 0.0174717, acc 0.984375
2016-11-12T19:21:26.072973: step 13880, loss 0.00118454, acc 1
2016-11-12T19:21:26.132543: step 13881, loss 0.000790885, acc 1
2016-11-12T19:21:26.190433: step 13882, loss 0.00401231, acc 1
2016-11-12T19:21:26.249322: step 13883, loss 0.00451983, acc 1
2016-11-12T19:21:26.307831: step 13884, loss 6.44976e-05, acc 1
2016-11-12T19:21:26.368240: step 13885, loss 0.000910732, acc 1
2016-11-12T19:21:26.427713: step 13886, loss 0.000146282, acc 1
2016-11-12T19:21:26.487408: step 13887, loss 0.00778006, acc 1
2016-11-12T19:21:26.545540: step 13888, loss 0.0147622, acc 0.984375
2016-11-12T19:21:26.604649: step 13889, loss 0.204211, acc 0.96875
2016-11-12T19:21:26.663243: step 13890, loss 0.000377032, acc 1
2016-11-12T19:21:26.720425: step 13891, loss 0.000129049, acc 1
2016-11-12T19:21:26.778020: step 13892, loss 0.0620236, acc 0.984375
2016-11-12T19:21:26.839901: step 13893, loss 1.19381e-05, acc 1
2016-11-12T19:21:26.896373: step 13894, loss 0.0101587, acc 1
2016-11-12T19:21:26.957006: step 13895, loss 0.000680772, acc 1
2016-11-12T19:21:27.016352: step 13896, loss 1.26992e-05, acc 1
2016-11-12T19:21:27.076710: step 13897, loss 0.000310264, acc 1
2016-11-12T19:21:27.134442: step 13898, loss 0.000530925, acc 1
2016-11-12T19:21:27.193712: step 13899, loss 0.000216729, acc 1
2016-11-12T19:21:27.252625: step 13900, loss 0.000456664, acc 1

Evaluation:
2016-11-12T19:21:27.324615: step 13900, loss 5.76059, acc 0.546

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-13900

2016-11-12T19:21:27.811609: step 13901, loss 0.00027218, acc 1
2016-11-12T19:21:27.870042: step 13902, loss 0.00152749, acc 1
2016-11-12T19:21:27.928929: step 13903, loss 0.000129425, acc 1
2016-11-12T19:21:27.986092: step 13904, loss 0.000124361, acc 1
2016-11-12T19:21:28.045834: step 13905, loss 0.224836, acc 0.984375
2016-11-12T19:21:28.104175: step 13906, loss 6.52424e-05, acc 1
2016-11-12T19:21:28.161795: step 13907, loss 7.38896e-05, acc 1
2016-11-12T19:21:28.218284: step 13908, loss 2.2378e-05, acc 1
2016-11-12T19:21:28.275436: step 13909, loss 0.00276446, acc 1
2016-11-12T19:21:28.334370: step 13910, loss 4.00736e-05, acc 1
2016-11-12T19:21:28.391671: step 13911, loss 0.000628121, acc 1
2016-11-12T19:21:28.452614: step 13912, loss 0.00842039, acc 1
2016-11-12T19:21:28.510844: step 13913, loss 0.0134152, acc 0.984375
2016-11-12T19:21:28.571305: step 13914, loss 0.00157418, acc 1
2016-11-12T19:21:28.632689: step 13915, loss 4.59505e-05, acc 1
2016-11-12T19:21:28.671110: step 13916, loss 1.22893e-05, acc 1
2016-11-12T19:21:28.728150: step 13917, loss 0.0060684, acc 1
2016-11-12T19:21:28.786864: step 13918, loss 0.0262843, acc 0.984375
2016-11-12T19:21:28.845888: step 13919, loss 0.011876, acc 1
2016-11-12T19:21:28.905112: step 13920, loss 0.0105956, acc 1
2016-11-12T19:21:28.963116: step 13921, loss 0.00015312, acc 1
2016-11-12T19:21:29.021214: step 13922, loss 0.00824285, acc 1
2016-11-12T19:21:29.083476: step 13923, loss 0.00204275, acc 1
2016-11-12T19:21:29.143074: step 13924, loss 0.0012818, acc 1
2016-11-12T19:21:29.200801: step 13925, loss 0.00150131, acc 1
2016-11-12T19:21:29.260758: step 13926, loss 0.00463572, acc 1
2016-11-12T19:21:29.321610: step 13927, loss 5.70122e-06, acc 1
2016-11-12T19:21:29.380957: step 13928, loss 0.00413384, acc 1
2016-11-12T19:21:29.439076: step 13929, loss 0.0106213, acc 1
2016-11-12T19:21:29.496848: step 13930, loss 0.000316907, acc 1
2016-11-12T19:21:29.558920: step 13931, loss 7.63847e-05, acc 1
2016-11-12T19:21:29.617964: step 13932, loss 8.76863e-05, acc 1
2016-11-12T19:21:29.676770: step 13933, loss 0.0701119, acc 0.984375
2016-11-12T19:21:29.737928: step 13934, loss 0.000406633, acc 1
2016-11-12T19:21:29.796437: step 13935, loss 0.0434277, acc 0.984375
2016-11-12T19:21:29.856646: step 13936, loss 0.0175599, acc 0.984375
2016-11-12T19:21:29.916378: step 13937, loss 2.16073e-05, acc 1
2016-11-12T19:21:29.973215: step 13938, loss 0.00303268, acc 1
2016-11-12T19:21:30.031713: step 13939, loss 5.62084e-05, acc 1
2016-11-12T19:21:30.088904: step 13940, loss 0.000106601, acc 1
2016-11-12T19:21:30.147829: step 13941, loss 0.00722807, acc 1
2016-11-12T19:21:30.205973: step 13942, loss 4.97282e-06, acc 1
2016-11-12T19:21:30.263554: step 13943, loss 3.03765e-05, acc 1
2016-11-12T19:21:30.322078: step 13944, loss 0.0130428, acc 0.984375
2016-11-12T19:21:30.384745: step 13945, loss 0.00375858, acc 1
2016-11-12T19:21:30.443732: step 13946, loss 0.0214948, acc 0.984375
2016-11-12T19:21:30.504308: step 13947, loss 0.0425228, acc 0.984375
2016-11-12T19:21:30.567213: step 13948, loss 0.00110099, acc 1
2016-11-12T19:21:30.628532: step 13949, loss 0.011906, acc 0.984375
2016-11-12T19:21:30.688911: step 13950, loss 0.000395639, acc 1
2016-11-12T19:21:30.747819: step 13951, loss 0.0134466, acc 0.984375
2016-11-12T19:21:30.806409: step 13952, loss 0.0071537, acc 1
2016-11-12T19:21:30.865484: step 13953, loss 8.03548e-06, acc 1
2016-11-12T19:21:30.922288: step 13954, loss 6.40222e-05, acc 1
2016-11-12T19:21:30.980070: step 13955, loss 0.000745431, acc 1
2016-11-12T19:21:31.038656: step 13956, loss 3.72758e-05, acc 1
2016-11-12T19:21:31.097821: step 13957, loss 0.000883817, acc 1
2016-11-12T19:21:31.157047: step 13958, loss 0.00248707, acc 1
2016-11-12T19:21:31.216785: step 13959, loss 2.29161e-05, acc 1
2016-11-12T19:21:31.274569: step 13960, loss 0.0130629, acc 0.984375
2016-11-12T19:21:31.337729: step 13961, loss 0.0882291, acc 0.96875
2016-11-12T19:21:31.399654: step 13962, loss 0.000941464, acc 1
2016-11-12T19:21:31.460868: step 13963, loss 2.0714e-05, acc 1
2016-11-12T19:21:31.519607: step 13964, loss 0.000558053, acc 1
2016-11-12T19:21:31.576986: step 13965, loss 0.0137796, acc 0.984375
2016-11-12T19:21:31.636559: step 13966, loss 0.00205531, acc 1
2016-11-12T19:21:31.696120: step 13967, loss 0.0437431, acc 0.984375
2016-11-12T19:21:31.758400: step 13968, loss 0.00315936, acc 1
2016-11-12T19:21:31.816787: step 13969, loss 2.61143e-05, acc 1
2016-11-12T19:21:31.873593: step 13970, loss 9.68489e-06, acc 1
2016-11-12T19:21:31.933734: step 13971, loss 1.41561e-07, acc 1
2016-11-12T19:21:31.989842: step 13972, loss 8.99431e-05, acc 1
2016-11-12T19:21:32.047875: step 13973, loss 0.0104243, acc 1
2016-11-12T19:21:32.106884: step 13974, loss 0.0265016, acc 0.984375
2016-11-12T19:21:32.168117: step 13975, loss 0.00718297, acc 1
2016-11-12T19:21:32.228468: step 13976, loss 0.000428785, acc 1
2016-11-12T19:21:32.285956: step 13977, loss 8.7325e-05, acc 1
2016-11-12T19:21:32.344754: step 13978, loss 0.000124104, acc 1
2016-11-12T19:21:32.406733: step 13979, loss 0.0004394, acc 1
2016-11-12T19:21:32.465788: step 13980, loss 5.04016e-06, acc 1
2016-11-12T19:21:32.523128: step 13981, loss 0.0323922, acc 0.984375
2016-11-12T19:21:32.580785: step 13982, loss 0.00103349, acc 1
2016-11-12T19:21:32.640100: step 13983, loss 0.000657385, acc 1
2016-11-12T19:21:32.697658: step 13984, loss 1.12458e-05, acc 1
2016-11-12T19:21:32.753610: step 13985, loss 0.00111178, acc 1
2016-11-12T19:21:32.813557: step 13986, loss 0.000172624, acc 1
2016-11-12T19:21:32.852415: step 13987, loss 0.0178791, acc 1
2016-11-12T19:21:32.911969: step 13988, loss 2.47532e-05, acc 1
2016-11-12T19:21:32.968459: step 13989, loss 0.0104663, acc 1
2016-11-12T19:21:33.026408: step 13990, loss 0.00819663, acc 1
2016-11-12T19:21:33.086250: step 13991, loss 0.0197462, acc 0.984375
2016-11-12T19:21:33.145996: step 13992, loss 0.00374654, acc 1
2016-11-12T19:21:33.204699: step 13993, loss 0.00483114, acc 1
2016-11-12T19:21:33.264837: step 13994, loss 0.0120991, acc 1
2016-11-12T19:21:33.329463: step 13995, loss 0.000452987, acc 1
2016-11-12T19:21:33.386910: step 13996, loss 0.0784458, acc 0.984375
2016-11-12T19:21:33.447553: step 13997, loss 0.000140714, acc 1
2016-11-12T19:21:33.504910: step 13998, loss 0.0121676, acc 0.984375
2016-11-12T19:21:33.562873: step 13999, loss 0.000942295, acc 1
2016-11-12T19:21:33.620732: step 14000, loss 0.00685411, acc 1

Evaluation:
2016-11-12T19:21:33.694691: step 14000, loss 5.73718, acc 0.54

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-14000

2016-11-12T19:21:34.173393: step 14001, loss 0.00393891, acc 1
2016-11-12T19:21:34.233344: step 14002, loss 0.000551255, acc 1
2016-11-12T19:21:34.291357: step 14003, loss 0.000908178, acc 1
2016-11-12T19:21:34.352944: step 14004, loss 0.024562, acc 0.984375
2016-11-12T19:21:34.411217: step 14005, loss 3.73964e-05, acc 1
2016-11-12T19:21:34.469412: step 14006, loss 0.0114698, acc 0.984375
2016-11-12T19:21:34.526928: step 14007, loss 2.27831e-05, acc 1
2016-11-12T19:21:34.583499: step 14008, loss 5.12536e-06, acc 1
2016-11-12T19:21:34.640031: step 14009, loss 0.00740771, acc 1
2016-11-12T19:21:34.697779: step 14010, loss 0.00387161, acc 1
2016-11-12T19:21:34.754801: step 14011, loss 0.000402405, acc 1
2016-11-12T19:21:34.815591: step 14012, loss 0.000195805, acc 1
2016-11-12T19:21:34.876728: step 14013, loss 0.0571336, acc 0.96875
2016-11-12T19:21:34.935830: step 14014, loss 9.38599e-05, acc 1
2016-11-12T19:21:34.993620: step 14015, loss 4.20307e-05, acc 1
2016-11-12T19:21:35.053228: step 14016, loss 0.0157176, acc 0.984375
2016-11-12T19:21:35.111951: step 14017, loss 0.0145031, acc 0.984375
2016-11-12T19:21:35.171449: step 14018, loss 1.93228e-05, acc 1
2016-11-12T19:21:35.232524: step 14019, loss 0.000309114, acc 1
2016-11-12T19:21:35.292741: step 14020, loss 0.0661849, acc 0.984375
2016-11-12T19:21:35.355205: step 14021, loss 3.43083e-05, acc 1
2016-11-12T19:21:35.413499: step 14022, loss 0.000218161, acc 1
2016-11-12T19:21:35.473184: step 14023, loss 0.00726582, acc 1
2016-11-12T19:21:35.532729: step 14024, loss 0.020812, acc 0.984375
2016-11-12T19:21:35.592943: step 14025, loss 0.0319785, acc 0.984375
2016-11-12T19:21:35.651952: step 14026, loss 0.0320167, acc 0.96875
2016-11-12T19:21:35.711639: step 14027, loss 7.11263e-05, acc 1
2016-11-12T19:21:35.769116: step 14028, loss 4.95461e-07, acc 1
2016-11-12T19:21:35.824665: step 14029, loss 0.00013929, acc 1
2016-11-12T19:21:35.884369: step 14030, loss 0.00458286, acc 1
2016-11-12T19:21:35.943463: step 14031, loss 4.44897e-05, acc 1
2016-11-12T19:21:36.000686: step 14032, loss 1.25542e-05, acc 1
2016-11-12T19:21:36.060017: step 14033, loss 1.86448e-06, acc 1
2016-11-12T19:21:36.116207: step 14034, loss 5.18772e-05, acc 1
2016-11-12T19:21:36.174334: step 14035, loss 0.0012185, acc 1
2016-11-12T19:21:36.231771: step 14036, loss 0.000691703, acc 1
2016-11-12T19:21:36.290002: step 14037, loss 5.0134e-05, acc 1
2016-11-12T19:21:36.348659: step 14038, loss 0.00735007, acc 1
2016-11-12T19:21:36.406016: step 14039, loss 0.00386958, acc 1
2016-11-12T19:21:36.465305: step 14040, loss 0.0121604, acc 0.984375
2016-11-12T19:21:36.522535: step 14041, loss 0.0113789, acc 0.984375
2016-11-12T19:21:36.582918: step 14042, loss 0.0546919, acc 0.984375
2016-11-12T19:21:36.641607: step 14043, loss 0.00126263, acc 1
2016-11-12T19:21:36.703868: step 14044, loss 5.18608e-05, acc 1
2016-11-12T19:21:36.764622: step 14045, loss 0.0235178, acc 0.984375
2016-11-12T19:21:36.825540: step 14046, loss 2.17405e-05, acc 1
2016-11-12T19:21:36.882766: step 14047, loss 5.483e-05, acc 1
2016-11-12T19:21:36.939450: step 14048, loss 0.0130961, acc 0.984375
2016-11-12T19:21:37.001522: step 14049, loss 2.1265e-05, acc 1
2016-11-12T19:21:37.064916: step 14050, loss 0.00837618, acc 1
2016-11-12T19:21:37.123461: step 14051, loss 0.00424352, acc 1
2016-11-12T19:21:37.181236: step 14052, loss 0.00919713, acc 1
2016-11-12T19:21:37.240738: step 14053, loss 0.00713292, acc 1
2016-11-12T19:21:37.304306: step 14054, loss 0.0273523, acc 0.984375
2016-11-12T19:21:37.361868: step 14055, loss 0.00109659, acc 1
2016-11-12T19:21:37.420769: step 14056, loss 0.000120809, acc 1
2016-11-12T19:21:37.480221: step 14057, loss 3.03592e-05, acc 1
2016-11-12T19:21:37.519387: step 14058, loss 2.52718e-06, acc 1
2016-11-12T19:21:37.576117: step 14059, loss 0.0363986, acc 0.984375
2016-11-12T19:21:37.636630: step 14060, loss 0.0153405, acc 0.984375
2016-11-12T19:21:37.696869: step 14061, loss 0.0154964, acc 0.984375
2016-11-12T19:21:37.755648: step 14062, loss 0.00633739, acc 1
2016-11-12T19:21:37.813095: step 14063, loss 1.49564e-05, acc 1
2016-11-12T19:21:37.869677: step 14064, loss 0.0114593, acc 0.984375
2016-11-12T19:21:37.926106: step 14065, loss 0.039364, acc 0.96875
2016-11-12T19:21:37.985122: step 14066, loss 0.011959, acc 0.984375
2016-11-12T19:21:38.044800: step 14067, loss 0.0111033, acc 0.984375
2016-11-12T19:21:38.102438: step 14068, loss 2.06212e-05, acc 1
2016-11-12T19:21:38.159479: step 14069, loss 5.6233e-05, acc 1
2016-11-12T19:21:38.220180: step 14070, loss 5.69945e-05, acc 1
2016-11-12T19:21:38.279741: step 14071, loss 1.21411e-05, acc 1
2016-11-12T19:21:38.336042: step 14072, loss 0.00124613, acc 1
2016-11-12T19:21:38.395610: step 14073, loss 0.000153932, acc 1
2016-11-12T19:21:38.453436: step 14074, loss 0.000334679, acc 1
2016-11-12T19:21:38.513528: step 14075, loss 0.0133299, acc 0.984375
2016-11-12T19:21:38.571979: step 14076, loss 0.00851581, acc 1
2016-11-12T19:21:38.629464: step 14077, loss 0.0132904, acc 0.984375
2016-11-12T19:21:38.688130: step 14078, loss 2.54748e-05, acc 1
2016-11-12T19:21:38.745938: step 14079, loss 2.66535e-06, acc 1
2016-11-12T19:21:38.804458: step 14080, loss 0.00565727, acc 1
2016-11-12T19:21:38.864475: step 14081, loss 0.00106438, acc 1
2016-11-12T19:21:38.923274: step 14082, loss 0.00845434, acc 1
2016-11-12T19:21:38.984733: step 14083, loss 0.000932894, acc 1
2016-11-12T19:21:39.045663: step 14084, loss 0.0270917, acc 0.984375
2016-11-12T19:21:39.105880: step 14085, loss 0.00190254, acc 1
2016-11-12T19:21:39.165382: step 14086, loss 7.78285e-06, acc 1
2016-11-12T19:21:39.222870: step 14087, loss 0.0110176, acc 1
2016-11-12T19:21:39.284312: step 14088, loss 0.000328448, acc 1
2016-11-12T19:21:39.342299: step 14089, loss 0.000390796, acc 1
2016-11-12T19:21:39.400565: step 14090, loss 0.000657854, acc 1
2016-11-12T19:21:39.459608: step 14091, loss 0.0147355, acc 0.984375
2016-11-12T19:21:39.520265: step 14092, loss 2.52077e-05, acc 1
2016-11-12T19:21:39.577203: step 14093, loss 0.0151515, acc 0.984375
2016-11-12T19:21:39.636090: step 14094, loss 4.57419e-05, acc 1
2016-11-12T19:21:39.694411: step 14095, loss 0.0169811, acc 0.984375
2016-11-12T19:21:39.755759: step 14096, loss 1.07922e-05, acc 1
2016-11-12T19:21:39.812829: step 14097, loss 0.00792364, acc 1
2016-11-12T19:21:39.869435: step 14098, loss 0.00010605, acc 1
2016-11-12T19:21:39.926642: step 14099, loss 0.00139039, acc 1
2016-11-12T19:21:39.986215: step 14100, loss 3.18533e-05, acc 1

Evaluation:
2016-11-12T19:21:40.058400: step 14100, loss 5.80079, acc 0.54

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-14100

2016-11-12T19:21:40.533894: step 14101, loss 0.0233581, acc 0.984375
2016-11-12T19:21:40.592738: step 14102, loss 5.90925e-05, acc 1
2016-11-12T19:21:40.651347: step 14103, loss 1.6186e-06, acc 1
2016-11-12T19:21:40.708851: step 14104, loss 8.14826e-06, acc 1
2016-11-12T19:21:40.765357: step 14105, loss 0.00108182, acc 1
2016-11-12T19:21:40.824757: step 14106, loss 0.00451274, acc 1
2016-11-12T19:21:40.884399: step 14107, loss 0.0048915, acc 1
2016-11-12T19:21:40.942855: step 14108, loss 0.00363411, acc 1
2016-11-12T19:21:41.000970: step 14109, loss 0.0110955, acc 1
2016-11-12T19:21:41.061156: step 14110, loss 0.000201218, acc 1
2016-11-12T19:21:41.119372: step 14111, loss 0.00065863, acc 1
2016-11-12T19:21:41.176866: step 14112, loss 0.00727561, acc 1
2016-11-12T19:21:41.237731: step 14113, loss 0.000539926, acc 1
2016-11-12T19:21:41.295308: step 14114, loss 0.0105353, acc 1
2016-11-12T19:21:41.351768: step 14115, loss 0.117299, acc 0.984375
2016-11-12T19:21:41.414119: step 14116, loss 0.0038623, acc 1
2016-11-12T19:21:41.476855: step 14117, loss 3.38373e-05, acc 1
2016-11-12T19:21:41.540103: step 14118, loss 0.000149438, acc 1
2016-11-12T19:21:41.600321: step 14119, loss 1.96861e-05, acc 1
2016-11-12T19:21:41.659000: step 14120, loss 9.37767e-05, acc 1
2016-11-12T19:21:41.715945: step 14121, loss 3.76606e-06, acc 1
2016-11-12T19:21:41.773282: step 14122, loss 2.02463e-06, acc 1
2016-11-12T19:21:41.829467: step 14123, loss 0.000228119, acc 1
2016-11-12T19:21:41.888623: step 14124, loss 0.00144228, acc 1
2016-11-12T19:21:41.946807: step 14125, loss 5.69682e-05, acc 1
2016-11-12T19:21:42.005953: step 14126, loss 0.000311727, acc 1
2016-11-12T19:21:42.064981: step 14127, loss 0.00404356, acc 1
2016-11-12T19:21:42.124776: step 14128, loss 0.000411274, acc 1
2016-11-12T19:21:42.164676: step 14129, loss 0.000287692, acc 1
2016-11-12T19:21:42.224342: step 14130, loss 7.64795e-05, acc 1
2016-11-12T19:21:42.283789: step 14131, loss 1.34175e-05, acc 1
2016-11-12T19:21:42.341531: step 14132, loss 0.0019373, acc 1
2016-11-12T19:21:42.400758: step 14133, loss 0.000797451, acc 1
2016-11-12T19:21:42.458994: step 14134, loss 0.00742033, acc 1
2016-11-12T19:21:42.518163: step 14135, loss 0.00773371, acc 1
2016-11-12T19:21:42.580181: step 14136, loss 4.96214e-05, acc 1
2016-11-12T19:21:42.637985: step 14137, loss 0.00166285, acc 1
2016-11-12T19:21:42.696006: step 14138, loss 0.0191638, acc 0.984375
2016-11-12T19:21:42.756883: step 14139, loss 6.81722e-07, acc 1
2016-11-12T19:21:42.813797: step 14140, loss 0.014511, acc 0.984375
2016-11-12T19:21:42.871201: step 14141, loss 1.67543e-05, acc 1
2016-11-12T19:21:42.928868: step 14142, loss 0.0169314, acc 1
2016-11-12T19:21:42.988545: step 14143, loss 0.000370109, acc 1
2016-11-12T19:21:43.045862: step 14144, loss 0.00090983, acc 1
2016-11-12T19:21:43.103341: step 14145, loss 0.000339737, acc 1
2016-11-12T19:21:43.164986: step 14146, loss 8.01856e-06, acc 1
2016-11-12T19:21:43.221567: step 14147, loss 0.00771443, acc 1
2016-11-12T19:21:43.280719: step 14148, loss 1.03861e-05, acc 1
2016-11-12T19:21:43.340966: step 14149, loss 0.000496674, acc 1
2016-11-12T19:21:43.398472: step 14150, loss 0.0298979, acc 0.984375
2016-11-12T19:21:43.458358: step 14151, loss 0.0071322, acc 1
2016-11-12T19:21:43.516769: step 14152, loss 0.0522332, acc 0.96875
2016-11-12T19:21:43.576304: step 14153, loss 5.17373e-05, acc 1
2016-11-12T19:21:43.637178: step 14154, loss 0.000220906, acc 1
2016-11-12T19:21:43.696463: step 14155, loss 0.0167246, acc 0.984375
2016-11-12T19:21:43.757097: step 14156, loss 0.000882686, acc 1
2016-11-12T19:21:43.816387: step 14157, loss 0.00317471, acc 1
2016-11-12T19:21:43.874542: step 14158, loss 1.14234e-05, acc 1
2016-11-12T19:21:43.932294: step 14159, loss 0.003872, acc 1
2016-11-12T19:21:43.991241: step 14160, loss 0.00873275, acc 1
2016-11-12T19:21:44.049608: step 14161, loss 0.0272467, acc 0.984375
2016-11-12T19:21:44.113281: step 14162, loss 0.034115, acc 0.984375
2016-11-12T19:21:44.173537: step 14163, loss 0.0114181, acc 0.984375
2016-11-12T19:21:44.232704: step 14164, loss 1.0696e-05, acc 1
2016-11-12T19:21:44.288940: step 14165, loss 0.0145704, acc 0.984375
2016-11-12T19:21:44.348360: step 14166, loss 0.00514972, acc 1
2016-11-12T19:21:44.409734: step 14167, loss 0.000398715, acc 1
2016-11-12T19:21:44.466856: step 14168, loss 1.06119e-05, acc 1
2016-11-12T19:21:44.524324: step 14169, loss 0.0105348, acc 1
2016-11-12T19:21:44.581901: step 14170, loss 0.000174309, acc 1
2016-11-12T19:21:44.640142: step 14171, loss 0.000103445, acc 1
2016-11-12T19:21:44.697401: step 14172, loss 0.0080618, acc 1
2016-11-12T19:21:44.754108: step 14173, loss 2.29283e-06, acc 1
2016-11-12T19:21:44.810586: step 14174, loss 1.17366e-05, acc 1
2016-11-12T19:21:44.868828: step 14175, loss 0.0148617, acc 0.984375
2016-11-12T19:21:44.925540: step 14176, loss 0.018885, acc 0.984375
2016-11-12T19:21:44.984784: step 14177, loss 5.44407e-05, acc 1
2016-11-12T19:21:45.044758: step 14178, loss 0.0133119, acc 0.984375
2016-11-12T19:21:45.102477: step 14179, loss 8.19101e-05, acc 1
2016-11-12T19:21:45.161299: step 14180, loss 0.00180384, acc 1
2016-11-12T19:21:45.221019: step 14181, loss 0.000136112, acc 1
2016-11-12T19:21:45.281635: step 14182, loss 0.00190678, acc 1
2016-11-12T19:21:45.340764: step 14183, loss 0.000164441, acc 1
2016-11-12T19:21:45.398148: step 14184, loss 0.0122355, acc 0.984375
2016-11-12T19:21:45.459964: step 14185, loss 0.00557211, acc 1
2016-11-12T19:21:45.518744: step 14186, loss 9.45959e-05, acc 1
2016-11-12T19:21:45.576485: step 14187, loss 1.17344e-06, acc 1
2016-11-12T19:21:45.637976: step 14188, loss 3.39716e-06, acc 1
2016-11-12T19:21:45.694585: step 14189, loss 0.0147601, acc 0.984375
2016-11-12T19:21:45.752807: step 14190, loss 0.000866314, acc 1
2016-11-12T19:21:45.811778: step 14191, loss 0.00934225, acc 1
2016-11-12T19:21:45.870172: step 14192, loss 0.00026557, acc 1
2016-11-12T19:21:45.929423: step 14193, loss 0.0116197, acc 1
2016-11-12T19:21:45.988579: step 14194, loss 0.000380025, acc 1
2016-11-12T19:21:46.047528: step 14195, loss 0.00226777, acc 1
2016-11-12T19:21:46.105869: step 14196, loss 6.848e-06, acc 1
2016-11-12T19:21:46.164879: step 14197, loss 0.000240045, acc 1
2016-11-12T19:21:46.224799: step 14198, loss 4.17499e-05, acc 1
2016-11-12T19:21:46.282369: step 14199, loss 0.0161523, acc 0.984375
2016-11-12T19:21:46.321698: step 14200, loss 7.15255e-08, acc 1

Evaluation:
2016-11-12T19:21:46.391356: step 14200, loss 5.78906, acc 0.546

Saved model checkpoint to /home/markyan7020/ADL2016/hw2/cnn-text-classification-tf/runs/1478948814/checkpoints/model-14200

